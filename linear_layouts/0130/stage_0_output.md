### Ground Truth Glossary

| Term | Hardware_Definition (Official Manuals) | Hard_Constraints (Alignment/Size/Types) | Paper_1_Coverage (Linear Layouts) | Paper_2_Coverage (ISL) | Paper_3_Coverage (Categorical) |
|---|---|---|---|---|---|
| `CUtensorMap` / TMA (Tensor Memory Access) | CUDA Driver API: `CUtensorMap` is a tensor-map descriptor created by `cuTensorMapEncodeTiled` / `cuTensorMapEncodeIm2col*`, i.e., a packed/opaque descriptor describing a tiled (or im2col) tensor region in global memory for TMA-style multidimensional transfers; “opaque value” intended to be accessed only via CUDA APIs and PTX. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>PTX ISA: a tensor-map is a 1024-bit object; PTX provides `tensormap.replace.*.b1024` to mutate specific fields (e.g., global address / dims / strides / swizzle). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | Descriptor bounds (CUDA Driver API):<br>• `tensorMap` storage address: 64B aligned. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• Rank: `tensorRank ∈ [1,5]`; if `interleave != NONE`, then `tensorRank ≥ 3`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalAddress`: 16B aligned; 32B aligned if `interleave == 32B` and/or packed types `16U6_ALIGN16B` / `16U4_ALIGN16B`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalDim[i]`: non-zero, ≤ 2^32; packed-type extra rules (e.g., `globalDim[0]` multiple of 128 for `16U6_ALIGN16B` / `16U4_ALIGN16B`; multiple of 2 for `16U4_ALIGN8B`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalStrides[i]` (bytes, for lower `tensorRank-1` dims): multiple of 16 and < 2^40; becomes multiple of 32 under `interleave == 32B` and/or packed `16U6_ALIGN16B` / `16U4_ALIGN16B`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `boxDim[i]`: non-zero, ≤ 256; when `interleave == NONE`, `boxDim[0] * elementSize(tensorDataType)` must be a multiple of 16B; packed types may force `boxDim[0] == 128`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `elementStrides[i]`: non-zero, ≤ 8; importantly, when `interleave == NONE`, `elementStrides[0]` is ignored (“TMA doesn’t support the stride for dimension zero”). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>Swizzle modes (CUDA Driver API): includes `NONE`, `32B`, `64B`, `128B`, plus atomicity/atom variants (e.g., `128B_ATOM_32B`, `128B_ATOM_32B_FLIP_8B`, `128B_ATOM_64B`) and cross-constraints with `interleave` / packed types. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>Swizzle inner-dimension constraints (CUDA Driver API & CUDA PG): bounding-box inner dimension must be ≤ swizzle span (e.g., `SWIZZLE_64B` ⇒ ≤ 64B; `SWIZZLE_128B*` ⇒ ≤ 128B). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>Additional “programming guide” constraints for TMA swizzle: notes a requirement that global memory be 128B-aligned when applying a TMA swizzle pattern. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)) | Mentions the existence of “tensor descriptors that rely on TMA engines” and uses this as a motivating hardware feature boundary in evaluation, but does not enumerate the driver/PTX descriptor legality constraints (rank ≤ 5, stride multiples, boxDim caps, swizzle enum restrictions, etc.). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Defines CuTe layouts primarily as shape/stride mappings from N-D coordinates to 1-D indices (an abstract layout model); no explicit modeling of `CUtensorMap` fields, rank cap 5, stride-multiple rules, or swizzle enum legality is present in the shown definitions. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Treats “CuTe layouts” and their algebra abstractly (layout as coordinate→offset mapping; categorical structure), not as a hardware tensor-map descriptor with bounded encodings / alignment legality. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| TMEM (Blackwell “Tensor Memory”) | PTX ISA (Blackwell-family targets): Tensor Memory is a per-CTA memory region accessed via `tcgen05.*` (“Tensorcore 5th Generation”) instructions; supports dynamic allocation via `tcgen05.alloc`, which allocates Tensor Memory “columns” and writes a Tensor Memory address (`taddr`) into shared memory. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>Context anchor: NVIDIA CUTLASS documents SM100 “Blackwell” support and calls out “Blackwell’s new tensor memory … as `tmem`”. ([docs.nvidia.com](https://docs.nvidia.com/cutlass/latest/CHANGELOG.html)) | Allocation & lifetime rules (PTX):<br>• `tcgen05.alloc` can block if insufficient Tensor Memory; writes `taddr` to `.shared::cta` at `dst`. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Every allocation must be explicitly deallocated via `tcgen05.dealloc` before kernel exit. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• `tcgen05.dealloc`: `taddr` must point to a prior allocation; `cta_group::2` requires synchronization with peer-CTA warp and dealloc may block. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• After any thread executes `tcgen05.relinquish_alloc_permit`, further `tcgen05.alloc` by that CTA is illegal. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>Access restrictions (PTX):<br>• Tensor Memory of a CTA is partitioned: each warp of a warpgroup can access only a lane range (warp0: 0–31, warp1: 32–63, warp2: 64–95, warp3: 96–127). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• `tcgen05.ld` / `tcgen05.st` are warp-collective; all threads in the warp must specify the same `taddr` base address or behavior is undefined. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• All `tcgen05` instructions in a kernel must use the same `.cta_group` value. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Example of instruction-specific alignment: `tcgen05.shift` requires `taddr` lane alignment to 32. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>Data packing/layout constraints (PTX): packing formats for certain kinds (e.g., `mxf4` in Tensor Memory; `mxf8f6f4` in shared memory) are specified. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | Mentions “Tensor Memory on Blackwell” as a special memory unit requiring special layouts, but (in the shown sections) does not enumerate TMEM lane-partition rules, alloc/dealloc legality, or tcgen05 instruction constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Models CuTe layouts / swizzles abstractly; no explicit model of Tensor Memory as a partitioned per-CTA store with warp-lane access restrictions and dynamic allocation semantics. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Category-theoretic framing of CuTe layouts; “Tensor Memory” appears only as an external reference in bibliography, not as a formally constrained memory system (no lane partitions / alloc/dealloc model). ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `cp.async.bulk.tensor` | PTX ISA: `cp.async.bulk.tensor` is a non-blocking instruction that initiates an asynchronous copy of tensor data between `.global` and `.shared::{cta,cluster}` using `[tensorMap, tensorCoords]`, with completion via `mbarrier::complete_tx::bytes` (for gmem→smem) or via bulk async-groups (`bulk_group` + `cp.async.bulk.commit_group`/`wait_group`) for smem→gmem variants. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | PTX tensor-copy legality constraints (selected):<br>• Instruction encodes: `dim ∈ {1d..5d}`, dst/src spaces, `load_mode ∈ {tile, tile::gather4, im2col, im2col::w, im2col::w::128}`, completion mechanism, etc. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• “Tensor copy restrictions” exist for sub-byte element types (e.g., `.b4x16*`, `.b6*`), including mandatory `Box-Size[0]` and alignment/stride rules (e.g., global address and tensor strides 32B-aligned for certain packed sub-byte types; tensorCoords[0] multiple of 128; and swizzle restrictions). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Direction `.global.shared::cta`: starting coordinates must be non-negative; bounding-box constraints must stay within tensor boundaries. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Bulk async-group semantics: `cp.async.bulk.commit_group` batches prior uncommitted bulk ops; `cp.async.bulk.wait_group` waits for groups; no memory ordering guarantee between two bulk ops within the same bulk async-group. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>CUDA C++ Programming Guide (cc 9.0, multi-dim bulk tensor async copy): global addr 16B-aligned; global strides multiples of 16B; shared destination 128B-aligned; barrier 8B-aligned; transfer size multiple of 16B; also describes OOB fill behavior (zero-fill for out-of-bounds on g→s). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html))<br><br>Feature gating example: certain qualifiers (`tile::gather4`, `im2col::w`) require SM100-class targets depending on dst space. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | Discusses layout conversions, shared-memory load/store, swizzling, and notes kernels “use tensor descriptors that rely on TMA engines,” but does not (in the shown sections) formalize `cp.async.bulk.tensor` instruction-level constraints (completion mechanisms, alignment tables, tensorCoords legality, load_mode gating). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Focus is on expressing layout abstractions (CuTe + linear layouts) as integer set relations; does not define `cp.async.bulk.tensor` as a constrained asynchronous instruction with completion mechanism requirements and address/stride alignment legality. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Focus is categorical semantics of CuTe layouts/operations; does not treat asynchronous bulk tensor copies, barrier-completion mechanisms, or alignment/boxing legality as first-class constraints. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `wgmma` / `mma` (operand layout requirements) | PTX ISA: `wgmma.mma_async.sync.aligned.*` encodes shapes and types; variants take matrix descriptors (e.g., `a-desc`, `b-desc`) and/or register fragments, depending on data type/shape. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>PTX ISA: matrix descriptors are encoded structures with fields including leading dimension and an explicit swizzling mode selection (with enumerated swizzle modes and invalid values). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>PTX ISA: `mma.sync.aligned.*.row.col.*` (and other layout qualifiers) encode the operand layout interpretation directly in the instruction form. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>Blackwell path: PTX ISA specifies TCGen05 (`tcgen05.mma*`) with Tensor Memory datapath “layout organization” tables and lane-alignment requirements. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | `wgmma` descriptor/layout constraints (PTX evidence):<br>• Uses descriptor operands (`a-desc`, `b-desc`) in instruction forms; examples show `descA/descB` held in `.b64` registers. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Matrix descriptor includes a swizzling-mode field with enumerated values: 0 = no swizzle; 1 = 128B with 32B atomic swizzle; 2 = 128B; 4 = 64B; 6 = 32B; (3/5/7 invalid). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>• Sparse `wgmma.mma_async.sp` defines packed storage/fragment layout constraints (thread-to-fragment mapping is specified in PTX). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>`mma.sync` constraints: instruction form constrains supported shapes (`m*n*k`), data types, and matrix layout qualifiers (e.g., `.row.col`) as part of the opcode; operand fragment sizes/types are fixed by the chosen opcode. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br><br>Blackwell TCGen05 constraints: Tensor Memory datapath layouts enforce required “Tensor Memory Datapath Lane Alignment” values per MMA variant; some layouts require consistent lane alignment across matrices. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | Explicitly discusses `mma` and `wgmma` as hardware primitives with special layout requirements; models and constructs “MMA swizzling” as a layout map to mitigate bank conflicts; also discusses shared-memory `ldmatrix/stmatrix` requirements (contiguous bytes per thread / group collaboration). ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br><br>Does not (in the shown segments) reproduce PTX matrix-descriptor bitfield legality tables or enumerate allowed swizzle modes as an ISA constraint; instead treats swizzling/layout as compiler-chosen transformations. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Defines CuTe layouts as abstract coordinate→index maps with shape/stride and includes a CuTe swizzle background section, but does not tie the layout formalism to PTX-level operand descriptor encodings (matrix descriptor bitfields, allowed swizzle IDs) or opcode-selected fragment layouts. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Develops a categorical semantics of CuTe layout operations; uses standard row/column-major as illustrative cases and discusses composition/product at the level of abstract layout morphisms, not PTX operand encodings/fragments. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `mbarrier` / Async Transaction Barriers | PTX ISA: `mbarrier` is an opaque barrier object in shared memory supporting (i) synchronizing subsets of threads, (ii) one-way cross-CTA arrive within a cluster (with limitations), and (iii) waiting for completion of asynchronous memory operations and making them visible; supports init/inval and arrive/test_wait/try_wait plus tx-count tracking (`expect_tx`/`complete_tx`). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | Object representation & alignment (PTX): `mbarrier` object type is `.b64`, 8-byte aligned, in `.shared`. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>Validity constraints (PTX): performing any operation except `mbarrier.init` on an uninitialized mbarrier is UB; likewise non-mbarrier ops on an initialized mbarrier are UB. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>Counts/ranges (PTX): expected arrival count range `[1, 2^20−1]`; pending `[0, 2^20−1]`; tx-count range `[−(2^20−1), 2^20−1]`. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>Phase completion (PTX): current phase completes when pending arrivals count is zero AND tx-count is zero. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>Hopper+ tx-count (PTX): tx-count support described as “starting with Hopper architecture (`sm_9x`)”. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))<br>Coupling to async copies (PTX): `cp.async.bulk.tensor` with `.mbarrier::complete_tx::bytes` performs `complete-tx` on the mbarrier with byte count on completion. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)) | No explicit `mbarrier` formalization observed in the shown portions; the paper’s focus is layouts and layout-driven codegen (swizzling, bank conflicts, shared-memory primitives), not the precise semantics/representation constraints of async-transaction barrier objects. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Abstract layout modeling (shape/stride and relations) does not model mbarrier’s phase/tx-count semantics, `.b64` object representation, or completion conditions that gate correctness of asynchronous copies. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Category-theoretic layout semantics does not incorporate barrier objects with tx-count phase semantics; asynchronous completion is outside the layout algebra presented in the cited sections. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| Bank Conflicts (shared memory bank rules) | CUDA C++ Programming Guide: shared memory has 32 banks; successive 32-bit words map to successive banks; bank conflicts arise if the same bank is used multiple times within a transaction, reducing bandwidth (serialization into multiple transactions). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/cuda-c-programming-guide/index.html))<br><br>CUDA C++ Programming Guide (TMA context): TMA supports “swizzle patterns” to alter the shared-memory layout to reduce/eliminate bank conflicts. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/cuda-c-programming-guide/index.html)) | Hardware bank model: 32 banks, 32-bit word/bank mapping; conflict degree depends on per-warp access pattern; padding can eliminate conflicts in common patterns (e.g., 32×32 transpose becomes conflict-free with +1 padding). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html))<br><br>TMA swizzle modes (CUDA PG, illustrative): swizzle width options include none/32B/64B/128B and are motivated by bank-conflict avoidance. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/cuda-c-programming-guide/index.html)) | Explicitly models shared memory layouts and bank conflicts, and presents an “optimal swizzling” algorithm aiming to maximize vectorization and minimize bank conflicts; also defines “mma swizzling” as a formal mapping to avoid bank conflicts for MMA-family layouts. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Mentions “complex stride configurations and swizzle patterns” and defines CuTe layout mapping via shape/stride; does not explicitly instantiate the 32-bank hardware model or bank-conflict cost semantics in the shown definitions. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Discusses layouts (row/col major) and their composition in a categorical setting; bank-conflict hardware cost model (32-bank mapping, conflict degrees) is not part of the categorical semantics in the cited sections. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |

---

### Constraint Cliffs (max 5)

1. **“Arbitrary rank/strides” vs tensor-map legality:** all three papers treat layouts as general coordinate→offset/index mappings (shape/stride, relations, morphisms), but real `CUtensorMap`/TMA descriptors have strict bounds: rank ≤ 5, `globalStrides` multiples of 16B (often 32B), and per-dimension caps like `boxDim[i] ≤ 256`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))

2. **“Swizzle is any invertible permutation” vs enumerated swizzle modes + inner-dimension caps:** hardware constrains swizzle to enumerated modes (driver enum and PTX descriptor fields) and requires the bounding-box “inner dimension” to be ≤ swizzle span (32/64/128), with additional cross-constraints (interleave ↔ swizzle; packed types ↔ allowed swizzles). Most paper formalisms treat swizzle as a general algebraic transformation without encoding-level admissibility checks. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))

3. **“Copy is a pure function” vs `cp.async.bulk.tensor` alignment + boxing + completion mechanism:** PTX makes `cp.async.bulk.tensor` a constrained async instruction (dims 1–5, load_mode gating by SM target, tensorCoords non-negativity, and sub-byte box/stride/alignment rules), and correctness requires using either an mbarrier completion mechanism or bulk async-group sequencing semantics. Paper-level abstractions typically do not model these instruction/ABI constraints. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))

4. **“Special memory is just another index space” vs TMEM lane partition + warp-collective addressing:** Blackwell Tensor Memory is partitioned by warp lanes (each warp accesses only a lane range), and `tcgen05.ld/st` require all threads in a warp to provide the same `taddr` base or behavior is undefined—constraints absent from the papers’ abstract layout models. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))

5. **“Barrier = synchronization” vs mbarrier phase/tx-count contract:** hardware `mbarrier` is a `.b64` object with strict alignment and UB rules, and phase completion requires both pending-arrival count and tx-count to reach zero; ignoring the tx-count/complete-tx mechanism (common in abstract treatments) is a correctness bug for async-copy pipelines. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html))

---
Learn more:
1. [https://docs.nvidia.com/cuda/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
2. [https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html](https://docs.nvidia.cn/cuda/parallel-thread-execution/index.html)
3. [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
4. [https://arxiv.org/html/2505.23819v3](https://arxiv.org/html/2505.23819v3)
5. [https://arxiv.org/html/2511.10374v1](https://arxiv.org/html/2511.10374v1)
6. [https://arxiv.org/pdf/2601.05972v1](https://arxiv.org/pdf/2601.05972v1)
7. [https://docs.nvidia.com/cutlass/latest/CHANGELOG.html](https://docs.nvidia.com/cutlass/latest/CHANGELOG.html)
8. [https://docs.nvidia.com/cuda/archive/12.9.1/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/archive/12.9.1/cuda-c-programming-guide/index.html)
9. [https://docs.nvidia.cn/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html](https://docs.nvidia.cn/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html)
