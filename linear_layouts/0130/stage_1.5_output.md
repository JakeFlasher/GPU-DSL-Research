# Stage‑1 Ranked Bottlenecks
**Source:** “Elephant‑in‑the‑room diagnosis (ranked)” from `stage_1_output.md` (Items 1–5).

## 1. The Toolbox: Ranked Bottlenecks & Hybrid Solutions

| Bottleneck | Theory A (Why it fits) | Theory B (Why it fits) | Prototype Mechanism (Hybrid) | Key Metrics | Main Risk |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1) Latency hiding / async orchestration correctness**<br>*(Async copy + barriers + phase/tx contracts)* | **Why (Math):** *Typestate / effect system* over a finite‑state machine for **mbarrier phase + tx‑count**, making “deadlock‑free pipeline” a type property.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Add a linear token type like `!gpu.async_contract<mbarrier, phase>` that must be consumed by `wait`/`advance` ops.<br>• **Alg:** SSA token flow + typestate checking to ensure `expect_tx` and completion are consistent.<br>• **HW hook:** PTX requires phase completion only when **pending arrivals == 0 and tx‑count == 0**, and `cp.async.bulk.tensor` can complete via `mbarrier::complete_tx::bytes`. ([docs.nvidia.com])<br><br>**Risk:** False positives if we don’t model both completion modes (`.mbarrier` vs `.bulk_group`) or multi‑producer patterns precisely. ([docs.nvidia.com])<br><br>**Metric plan:**<br>• **Compile-time:** % kernels rejected / auto‑repaired.<br>• **Runtime:** Nsight Compute **SchedulerStats** (“no eligible warps”, skipped issue slots) + **warp stall sampling**; track achieved occupancy vs issued work. ([docs.nvidia.com]) | **Why (Math):** *Petri nets / synchronous dataflow (SDF)* model pipelines as token flow with bounded buffers (stages), enabling deadlock‑freedom + throughput reasoning.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Represent outstanding async ops as `!async.token` / `!gpu.async.token` dependencies.<br>• **Alg:** Modulo scheduling / ILP to choose stage depth + order.<br>• **HW hook:** PTX exposes distinct completion mechanisms for `cp.async.bulk.tensor` (`.mbarrier::complete_tx::bytes` vs `.bulk_group`) that must be scheduled differently. ([mlir.llvm.org])<br><br>**Risk:** Schedule “looks legal” but underperforms because model ignores contention, bank conflicts, or cluster effects.<br><br>**Metric plan:** Nsight **SchedulerStats** (eligible/issued warps) and “skipped issue slots” as primary signal of bubble reduction. ([docs.nvidia.com]) | **Why (Compose A+B):** Use **B** to *construct* a pipeline schedule, then **A** to *prove/check* it satisfies barrier/tx semantics (and to compute the exact tx expectations).<br><br>**Mechanism (MLIR/Triton/CuTe):**<br>• **Step 1:** Schedule candidate pipeline (stage count, commit/wait placement).<br>• **Step 2:** Typestate checker inserts/validates `expect_tx` and ensures phase discipline.<br>• **Step 3:** Lower through NVVM/NVGPU mbarrier ops. ([mlir.llvm.org])<br><br>**Risk:** Feedback loop (scheduler ↔ checker) can oscillate without a monotone refinement rule.<br><br>**Metric plan:** Iterations-to-fix + compile time; runtime: reduction in “no eligible warps” cycles. ([docs.nvidia.com]) | • Nsight **SchedulerStats / WarpStateStats** (eligible warps, skipped issue slots). ([docs.nvidia.com])<br>• **Achieved occupancy** (`sm__warps_active.avg.pct_of_peak_sustained_active`). ([docs.nvidia.com])<br>• Barrier correctness counters (pass/fail) + compile‑time “contract check” latency.<br>• Code size deltas (extra barriers/ops). | Modeling **barrier phases + tx‑count** precisely enough to avoid both false positives and missed deadlocks is nontrivial, especially when mixing completion modes. ([docs.nvidia.com]) |
| **2) Descriptor legality cliffs**<br>*(TMA / tensor-map / swizzle / alignment / rank caps)* | **Why (Math):** *Refinement types + Presburger/SMT constraints* match the nature of tensor‑map legality: “layout expression” + “descriptor fields” + “integer/alignment constraints.”<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** `!gpu.tma_desc<rank, swizzle, interleave, …>` with refinements.<br>• **Alg:** Constraint propagation + SMT/ILP to infer padding/box sizes/swizzle.<br>• **HW hook:** `cuTensorMapEncode*` requires e.g. **tensorMap addr 64B aligned**, **rank 3–5**, `boxDim[i] <= 256`, `elementStrides[i] <= 8` (and `elementStrides[0]` ignored when interleave NONE), plus enumerated swizzle restrictions/inner-dim caps; PTX also exposes `tensormap.replace … .b1024` (tensor-map is 1024‑bit) for field patching. ([docs.nvidia.com])<br><br>**Risk:** Solver time blow‑ups when shapes become symbolic (dynamic) or when search includes many discrete enum choices (swizzle/interleave/atom modes).<br><br>**Metric plan:** Solver time + % of kernels that become TMA‑legal (vs fallback) + code size from specialization. ([docs.nvidia.com]) | **Why (Math):** *Equality saturation (e‑graphs)* is good at exploring many layout rewrites while quotienting by equivalence—ideal for “repairing” a near‑legal layout (pad/split/reorder) into a legal one.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Layouts as terms.<br>• **Alg:** Equality saturation with e‑class analyses tracking alignment/inner‑dim and legality; extract cheapest legal representative.<br>• **HW hook:** Swizzle patterns have explicit **alignment and inner‑dimension requirements** and are enumerated, not arbitrary permutations. ([arxiv.org])<br><br>**Risk:** E‑graph growth (compile time/memory) unless rewrites are aggressively bounded by legality/cost heuristics.<br><br>**Metric plan:** E‑graph size, rewrite count, compile time; legality hit rate. ([arxiv.org]) | **Why (Compose A+B):** Use e‑graphs for **structural** exploration, SMT/refinement for **field‑level** synthesis/proofs (enum/stride/alignment).<br><br>**Mechanism (MLIR/Triton/CuTe):** Run legality‑aware rewrite saturation to generate a small candidate set; then invoke constraint solver to (i) prove legality and (ii) emit concrete `CUtensorMap` fields / patch plan (`tensormap.replace`) before lowering to `cp.async.bulk.tensor`. ([arxiv.org])<br><br>**Risk:** “Two‑stage” pipeline can miss solutions if rewrite system lacks a needed transformation (e.g., padding dimension selection).<br><br>**Metric plan:** Compile time split (rewrite vs solve) and final TMA eligibility rate. | • % kernels that emit TMA fast path vs fallback.<br>• Compile time: rewrite+solve wall time; peak memory.<br>• Code size (# specialized variants).<br>• Swizzle legality failures (counts). | The legal subset is **discrete and brittle** (enum coupling, alignment tables); overfitting to one CUDA/PTX version is likely if the constraint model isn’t versioned. ([docs.nvidia.com]) |
| **3) Memory bandwidth wall / arithmetic intensity + on‑chip tiering**<br>*(Reg vs SMEM vs TMEM; feed tensor cores)* | **Why (Math):** *Roofline / operational intensity model* gives a principled “should we invest in reuse/tiering?” decision before expensive search.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Attach per‑tile summaries (bytes moved per tier, FLOPs) and derive operational intensity.<br>• **Alg:** Static estimator + profiler‑calibrated correction.<br>• **HW hook:** Nsight Compute provides **Roofline Charts**, MemoryWorkloadAnalysis, and scheduler stats to classify bandwidth vs compute vs latency hiding. ([docs.nvidia.com])<br><br>**Risk:** Roofline can underpredict benefits of latency hiding and can miss bank‑conflict serialization (shared) as a first‑order limiter. ([docs.nvidia.com])<br><br>**Metric plan:** Achieved bandwidth/throughput metrics + roofline position + shared‑memory “bank conflicts/wavefronts” tables. ([docs.nvidia.com]) | **Why (Math):** *Memory‑tier type system* (a resource calculus) makes “where do accumulators live?” explicit and checkable—critical for Blackwell’s TMEM semantics.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Tiered address spaces + `tier(Reg/SMEM/TMEM)` in layout types.<br>• **Alg:** Placement + spill/tiling solver under capacity/collective constraints.<br>• **HW hook:** PTX TMEM allocation is **column‑based (unit 32 cols, power‑of‑two, allocates all 128 lanes)**; TMEM is lane‑partitioned per warp in a warpgroup; and `tcgen05.ld` is warp‑collective requiring the **same `taddr` across the warp**. ([docs.nvidia.com])<br>• **Prototype path:** CuTe already exposes `tmem` as a first‑class locale and provides tcgen05 building blocks. ([docs.nvidia.com])<br><br>**Metric plan:** Register pressure → achieved occupancy; TMEM traffic visibility in tooling; SM throughput. ([developer.nvidia.com]) | **Why (Compose A+B):** Roofline decides *which axis dominates*; tier typing ensures the chosen “reuse plan” is **legal** (especially for TMEM collectives) and can be lowered reliably.<br><br>**Mechanism (MLIR/Triton/CuTe):** Run roofline classifier → pick candidate reuse strategy (more stages, SMEM double‑buffer, TMEM accumulators) → enforce legality with tier types and tcgen05/TMA constraint checks; integrate into autotune space pruning. ([docs.nvidia.com])<br><br>**Risk:** Cross‑arch portability: TMEM is SM100+ specific; AMD wave64 implies different collaboration grain (may need a “grain type” in IR). ([rocm.docs.amd.com])<br><br>**Metric plan:** Roofline uplift + reduced register spills + fewer shared bank conflicts. ([docs.nvidia.com]) | • `sm__throughput.*.pct_of_peak_sustained_active` (compute saturation proxy). ([docs.nvidia.com])<br>• Shared memory **Bank Conflicts / Wavefronts**. ([docs.nvidia.com])<br>• Achieved occupancy proxy metric. ([docs.nvidia.com])<br>• Roofline chart movement. ([docs.nvidia.com])<br>• TMEM traffic visibility (Blackwell tooling). ([developer.nvidia.com]) | Risk of “beautiful tiering IR” that becomes a moving target as PTX/TMEM semantics evolve, plus significant engineering surface area (alloc/dealloc, collective rules). ([docs.nvidia.com]) |
| **4) Compilation/search cost**<br>*(Combinatorial space: tile × swizzle × stages × grain)* | **Why (Math):** *Hierarchical search + learned cost model* is designed for huge discrete optimization spaces under constraints (exactly our case).<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** Parameterized schedule space.<br>• **Alg:** Hierarchical sampling + evolutionary refinement + learned cost model; prune with legality constraints from bottleneck #2.<br>• **Prototype path:** Integrate into Triton-style tile compiler (explicit tiles) with fast compile+measure loop. ([arxiv.org])<br><br>**Risk:** Model drift across GPUs and toolkits; may need per‑family retraining.<br><br>**Metric plan:** Tuning wall time, # variants compiled/measured, win rate vs baseline. ([arxiv.org]) | **Why (Math):** *Staged rewriting / transformation strategy languages* let us treat compilation as a program, enabling memoization and incremental variant construction instead of full recompilation.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** MLIR **Transform dialect** to encode variant generation and control flow.<br>• **Alg:** Staged application + caching of intermediate IR.<br>• **Prototype path:** Define tiling/swizzle/pipeline transforms with **PDL** patterns and apply them to produce variants quickly. ([mlir.llvm.org])<br><br>**Risk:** Transformation infrastructure can become the bottleneck unless invariants (legality, dominance, memory spaces) are enforced robustly.<br><br>**Metric plan:** Compile time per variant; cache hit rate; transform interpreter time. ([mlir.llvm.org]) | **Why (Compose A+B):** Use **B** to make variant generation cheap (staged + cached), and **A** to choose which few variants to actually compile/benchmark next.<br><br>**Mechanism (MLIR/Triton/CuTe):** Transform/PDL generates normalized candidate IR → legality/type filters (from #1/#2/#3) → learned cost model proposes next samples → optional on‑device measurement; results fed back into model. ([mlir.llvm.org])<br><br>**Risk:** Multi‑system complexity (compiler + runtime measurement + model management).<br><br>**Metric plan:** End‑to‑end autotune cost (seconds), peak RSS, artifact cache size. | • End‑to‑end autotune time (seconds) + # variants compiled.<br>• Compile time per variant (seconds) + peak memory.<br>• Code size growth (SASS / binary).<br>• Cache hit rate for intermediate IR. | The search system can silently optimize for “what the model can predict” rather than “what the hardware needs,” especially when legality cliffs create sparse feasible regions. ([arxiv.org]) |
| **5) Dynamic memory management**<br>*(KV cache / fragmentation; system-level)* | **Why (Math):** *Linear/affine types + region reasoning* make buffer/page ownership explicit (who can reuse/free), enabling safer pooling and less fragmentation pressure.<br><br>**Mechanism (IR+Alg+HW):**<br>• **IR:** A `kv.page` resource type with affine “consume/reuse” semantics.<br>• **Alg:** Lifetime + escape analysis to preplan pool sizes.<br>• **Prototype path:** MLIR runtime interface + host allocator that exports page handles to Triton/CuTe kernels for attention. (This is a compiler+runtime contract, not a PTX instruction feature.)<br><br>**Risk:** Integration boundary: most KV cache logic lives in serving runtime, not in kernel compiler.<br><br>**Metric plan:** Fragmentation %, peak memory, allocator overhead, tail latency (p99). | **Why (Math):** *Paging / virtual-memory model* converts variable-length KV allocations into fixed-size blocks to approach near‑zero waste from fragmentation.<br><br>**Mechanism (IR+Alg+HW):**<br>• **Alg:** Page table + block allocator.<br>• **Compiler artifact:** Attention kernels lowered to paged addressing.<br>• **Prototype path:** Adopt PagedAttention/vLLM approach and optionally specialize kernels per page size. ([arxiv.org])<br><br>**Risk:** Address indirection and less coalescing can reduce raw throughput if kernels aren’t specialized carefully.<br><br>**Metric plan:** KV waste, throughput-at-fixed-latency, and CPU overhead for paging. ([arxiv.org]) | **Why (Compose A+B):** Compiler provides **lifetime + access-shape hints**, runtime provides **paging**, and kernels use those hints to keep loads coalesced where possible.<br><br>**Mechanism (MLIR/Triton/CuTe):**<br>• (i) Compile-time lifetime analysis picks page size + reuse windows.<br>• (ii) Runtime page allocator exposes stable page handles.<br>• (iii) Kernel codegen uses page-aware layout rules and can opportunistically use bulk transfers when pages are contiguous/eligible.<br><br>**Risk:** Multiple “almost compatible” page sizes/layouts across models makes caching and reuse harder.<br><br>**Metric plan:** Memory saved vs baseline, allocator time, and code size of specialized kernels. | • Fragmentation / KV waste (%).<br>• Peak GPU memory (GB) at target batch/seq.<br>• Allocator latency (CPU) and p99 end-to-end latency.<br>• Code size (# kernel variants). | Biggest risk is **scope creep**: memory management spans scheduler, runtime, and kernels; benefits can be wiped out by one layer not participating. ([arxiv.org]) |

---

## 2. Literature Scan (2019–2026)
*Primary sources (10 items)*

1.  **NVIDIA PTX ISA (CUDA docs):** Defines `cp.async.bulk.tensor` completion modes, `mbarrier` tx‑count/phase completion, and Blackwell `tcgen05`/TMEM semantics—this is the authoritative contract you must compile against. ([docs.nvidia.com])
2.  **CUDA Driver API – Tensor Memory / `CUtensorMap`:** Encodes the *hard legality* constraints (alignment/boxDim/rank/swizzle enums) that create real “TMA or fallback” cliffs. ([docs.nvidia.com])
3.  **CUDA C++ Programming Guide (swizzle + async barriers):** Documents swizzle alignment/inner-dimension requirements and barrier-phase tracking concepts used to reason about correctness and avoid bank conflicts. ([docs.nvidia.com])
4.  **MLIR NVVM + NVGPU dialect docs:** Exposes compiler IR ops for `mbarrier.arrive.expect_tx`, etc., making barrier/async contracts representable and checkable in the compiler pipeline. ([mlir.llvm.org])
5.  **Nsight Compute Profiling Guide (2025.3):** Provides roofline charts, scheduler/warp-state stats, and metric structuring needed to turn these theories into measurable evaluation plans. ([docs.nvidia.com])
6.  **CUTLASS/CuTe changelog (Blackwell enablement):** Shows `tmem` as a first-class locale and tcgen05 support in real kernel-building infrastructure—concrete prototype path for tiering theories. ([docs.nvidia.com])
7.  **egg (equality saturation):** Modern, fast e‑graph framework with e‑class analyses—good backbone for legality‑aware layout rewrite search. ([arxiv.org])
8.  **Ansor (OSDI 2020):** Exemplar of hierarchical search + learned cost model for tensor program optimization under large combinatorial spaces. ([arxiv.org])
9.  **Triton (MAPL/PLDI 2019):** Demonstrates the “tile-first IR” approach and how compiler passes can reliably generate high‑performance kernels from parametric tile programs—key prototyping substrate. ([research.ibm.com])
10. **PagedAttention / vLLM (2023):** Primary reference for paging-based KV cache management to reduce fragmentation in LLM serving. ([arxiv.org])

---

*If you want, I can turn **one row** (your choice) into a 2–3 page “implementation charter”: exact MLIR types/ops, pass ordering, legality checks, and a minimal Nsight/rocprof metric collection script plan.*