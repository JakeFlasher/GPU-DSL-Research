\documentclass[conference]{IEEEtran}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{fancyvrb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}

% --------------------------
% Listings setup (compact, IEEE-friendly)
% --------------------------
\lstset{
  basicstyle=\footnotesize\ttfamily,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em
}

% --------------------------
% Local macros (no $...$ math anywhere; use \( \) or \[ \])
% --------------------------
\newcommand{\TMA}{\textsc{TMA}}
\newcommand{\PTX}{\textsc{PTX}}
\newcommand{\SM}{\textsc{SM}}
\newcommand{\IR}{\textsc{IR}}
\newcommand{\MLIR}{\textsc{MLIR}}
\newcommand{\TTIR}{\textsc{TTIR}}
\newcommand{\TTGIR}{\textsc{TTGIR}}
\newcommand{\DSMEM}{\textsc{DSMEM}}
\newcommand{\LDS}{\textsc{LDS}}
\newcommand{\CTA}{\textsc{CTA}}
\newcommand{\VTM}{\textsc{VTM}}
\newcommand{\WRsms}{\textsc{WR--SMS}}

\begin{document}

\title{TITAN: Typestate-Integrated Tokenized Asynchrony for TMA Pipelines in Triton}
\author{\IEEEauthorblockN{Proposed by Principal Systems \& Compiler Architect (ASPLOS/ISCA/MICRO Focus)}}
\maketitle

\begin{abstract}
Modern GPU performance is increasingly governed by \emph{software-managed pipelining}: overlapping global\(\rightarrow\)shared transport with tensor-core compute, often under warp specialization. Yet today, compilers largely optimize \emph{spatial} layout and indexing while leaving \emph{temporal} transport correctness and overlap to brittle hand-written pipelines and conservative synchronization.
We frame this as a \textbf{Hardware--Math Gap}: the mismatch between a compiler's mathematical pipeline schedule and the hardware-visible ISA contracts required for correctness on real GPUs.

TITAN bridges this gap by making transport and synchronization \emph{first-class and verifiable} at the \IR{} level.
First, TITAN introduces a protocol-shaped pipeline \IR{} with explicit \texttt{acquire/produce/wait/release} structure over a ring buffer and a \textbf{two-semaphore (Ready/Free) handshake} that provides runtime backpressure and eliminates wrap-around ambiguity via epoch indexing.
Second, TITAN provides an \textbf{augmented affine, time-indexed typestate} that proves safety in the presence of multi-warp readers, predication/tails (no ``wait that never arrives'' hangs), and decoupled tensor-core consumption via explicit compute-consumption tokens.
Third, TITAN models \textbf{proxy consistency} explicitly (generic/async/tensormap) and synthesizes only the proxy fences justified by the effect graph, while exploiting implicit fences on completion where the ISA provides them.
Finally, TITAN is backend-parameterized: on NVIDIA, readiness uses barrier phase/state tokens (\texttt{mbarrier} semantics) and \TMA{} legality is verified; on AMD, TITAN uses \textbf{Virtual Token Mapping} to faithfully lower token consumption to counter-based \texttt{s\_waitcnt} without pretending per-transfer handle waits exist.
\end{abstract}

% ============================================================
\section{Problem Formulation \& Motivation (Hardware--Math Gap, made precise)}
% ============================================================

\subsection{Software-Managed Pipelining is Now the Dominant Compiler Problem}
Modern GPU kernels increasingly rely on \textbf{software-managed pipelining}: overlapping global\(\rightarrow\)shared transfers with tensor-core compute, frequently with \textbf{warp specialization} (dedicated ``producer'' warps issuing transfers, ``consumer'' warps computing).
The core compiler problem is no longer just finding a good initiation interval---it is enforcing \textbf{hardware-faithful correctness} while preserving overlap.

We therefore frame TITAN as addressing a specific \textbf{Hardware--Math Gap}: the semantic gap between (i) the \emph{mathematical} pipeline schedule a compiler wants to express and verify, and (ii) the \emph{actual ISA contracts} that make that schedule correct on real GPUs.

\subsection{Context: Spatial Layout Algebra is Necessary but Insufficient}
The seed system (Linear Layouts) elevates tensor layouts to first-class backend objects by representing layout conversions and swizzles as linear maps over \(\mathbb{F}_2\) (bit-matrix algebra).
This is a \emph{spatial} algebra: it answers \emph{where} each logical element resides (registers, threads/warps, shared-memory offsets) and synthesizes conversions (e.g., warp-shuffle gather) accordingly.

Modern Hopper/Blackwell performance, however, is increasingly dominated by a different axis: \emph{temporal} overlap and correctness of asynchronous transport:
\begin{itemize}
  \item \TMA{} bulk tensor transfers (global \(\leftrightarrow\) shared, and cluster shared \(\leftrightarrow\) shared via \DSMEM{}) provide high-throughput movement while avoiding register staging.
  \item Completion is synchronized via barrier protocols (e.g., \texttt{mbarrier} arrive/wait) and requires subtle proxy-ordering fences to prevent visibility bugs or unintended serialization.
  \item Warp specialization partitions a thread block into producer warps (copy/issue) and consumer warps (compute), increasing the importance of compiler-controlled scheduling and liveness constraints.
\end{itemize}

\textbf{Core mismatch:} a layout engine that only optimizes \emph{index maps} cannot express or optimize the partial order of \emph{events} required to exploit \TMA{} and warp specialization.
In practice, this forces:
\begin{enumerate}
  \item \textbf{Hand-coded pipelines} (or narrow template heuristics) that are brittle across shapes and architectures.
  \item \textbf{Over-synchronization} (extra waits/fences) to ensure correctness, which collapses overlap and turns copy engines into idle hardware.
  \item \textbf{Uncontrolled register pressure} (over-buffering / over-unrolling) that kills occupancy and negates gains, especially when pipelines are deepened without resource awareness.
\end{enumerate}

\subsection{Two Dominant Failure Modes of the Hardware--Math Gap}
\subsubsection{(G1) Proxy Consistency Gap (memory visibility across proxy domains)}
On NVIDIA Hopper-class GPUs, key asynchronous mechanisms execute in distinct proxy domains (at minimum: generic, async, and tensormap).
Correctness requires not only ordering operations, but also establishing cross-proxy visibility with the right proxy fences, and exploiting implicit fences on completion to avoid over-fencing \cite{ptx_isa, cuda_prog_guide}.

This is not a ``performance detail'': missing a required proxy fence can produce undefined behavior, while inserting fences conservatively can destroy the very overlap the pipeline was meant to achieve.

\textbf{TITAN's stance:} Unlike pipeline IRs that treat ``fence'' as a monolithic primitive, TITAN models proxy consistency explicitly: proxy-domain effects are part of the IR contract, and the compiler proves that every cross-proxy read has a justified ordering path.

\subsubsection{(G2) ISA Mismatch Gap (handle/phase tokens vs counter completion)}
Pipeline IRs often assume per-transfer capability tokens: ``wait on exactly this copy.''
That assumption matches some hardware mechanisms (e.g., phase/state-based completion), but not others.

\begin{itemize}
  \item On NVIDIA, completion is naturally expressed using phase/state associated with barrier arrival (state tokens / parity discipline) \cite{ptx_isa}.
  \item On AMD, completion is fundamentally counter-based (\texttt{s\_waitcnt}), not handle-based. A naive \texttt{wait(token)} abstraction is not faithfully representable without either weakening semantics or serializing the schedule \cite{llvm_waitcnt}.
\end{itemize}

\textbf{TITAN's stance:} Portability requires acknowledging the mismatch.
TITAN provides a portability layer that makes token semantics explicit and parameterized: exact (phase/state) tokens where hardware supports them, and \textbf{Virtual Token Mapping} (prefix/stream ordinals) where hardware provides only counters.

\subsection{What Must Be Proven (Safety Properties that Fail in Real Kernels)}
A correct pipeline system must rule out the reviewer-grade failures that occur in real kernels:
\begin{enumerate}
  \item \textbf{Ring-buffer overwrite or deadlock under imbalance} (consumer slower than producer).
  \item \textbf{WAR/UAF hazards} from decoupled tensor-core consumption (issue \(\neq\) done reading).
  \item \textbf{Multi-warp readers} (warp specialization) where ``done reading'' is collective, not scalar.
  \item \textbf{Tail / predication hangs} (``wait on a token that never arrives'') from short loops and partial tiles.
\end{enumerate}

\subsection{Target Constraints and Non-Negotiables (Preserved)}
We constrain ourselves to a real-metal, software-only agenda:
\begin{itemize}
  \item \textbf{Evaluation on real GPUs only:} NVIDIA H100 (SM90) and AMD MI300 (CDNA3) using Triton kernels, TritonBench-like harnesses, Nsight Compute (NVIDIA) / rocprof (AMD), and end-to-end latency.
  \item \textbf{No new hardware:} exploit existing \TMA{}, barriers, thread block clusters/\DSMEM{}, warp specialization, Tensor Cores, shared-memory swizzles, and ISA-visible synchronization.
  \item \textbf{Compiler artifact:} implementable in Triton/\MLIR{} backend and/or PyTorch Inductor integration. No simulator-driven claims.
\end{itemize}

\subsection{Problem Statement (Revised)}
Given a loop with per-iteration tiles, a target depth \(D\), and a target backend (NVIDIA or AMD), generate a pipelined schedule and \IR{} such that:
\begin{enumerate}
  \item \textbf{Safety holds for all executions} (including imbalance and tails): no use-before-ready, no overwrite-before-consumed, no hangs from predication.
  \item The \IR{} is \textbf{lowerable to the target ISA without semantic gaps}: proxy fences match proxy domains; completion matches phase/state tokens on NVIDIA and counter completion on AMD.
  \item \textbf{Performance does not regress} due to over-fencing, over-synchronization, or register-pressure blowups; TITAN must disable or degrade optimizations (e.g., warp specialization, depth \(D\)) when the cost model predicts cliffs.
\end{enumerate}

We call this system \textbf{TITAN}: \underline{T}ypestate-\underline{I}ntegrated \underline{T}okenized \underline{A}sync on \underline{N}VIDIA (and portable lowering hooks for AMD).

% ============================================================
\section{Methodology: Protocol-Shaped IR with Verified, Backend-Parameterized Tokens}
% ============================================================

\subsection{Single Principle: Correctness Must Be Established at the IR Level}
TITAN is built around a single principle:
\begin{quote}
\emph{Correctness must be established at the IR level using a protocol that is (a) runtime-realizable and (b) hardware-faithful at lowering time.}
\end{quote}

This is implemented via:
\begin{enumerate}
  \item a \textbf{protocol-shaped IR} (stages, epochs, acquire/produce/wait/release, guarded prologue/epilogue);
  \item an \textbf{augmented typestate} that verifies the protocol (wrap-around, multi-reader, async compute consumption, predication);
  \item a \textbf{portability layer} that defines what a ``token'' means per backend (NVIDIA phase/state tokens; AMD Virtual Token Mapping to \texttt{s\_waitcnt}).
\end{enumerate}

\subsection{Patch Kit Summary (Implementation Commitments)}
TITAN's methodology explicitly incorporates the following patch-level commitments (the items correspond to the concrete mechanisms described in the subsequent subsections).

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{p{0.21\textwidth} p{0.44\textwidth} p{0.12\textwidth} p{0.19\textwidth}}
\hline
\textbf{Vulnerability} & \textbf{Proposed Fix (Theory/Algo)} & \textbf{Impl. Cost} & \textbf{Grounding} \\
\hline
S1: ring-buffer backpressure underspecified \(\Rightarrow\) overwrite/deadlock/ABA &
Two-semaphore (Ready/Free) handshake per stage + time-indexed (epoch) protocol; \texttt{stage\_acquire} blocks on \(\textsf{Free}(s,e)\); invariant \(\textsf{produced}-\textsf{consumed} \le D\). &
Medium &
Bounded ring-buffer protocols; credit-based flow control. \\
\hline
S2: typestate too weak (multi-warp readers, async consumption, predication) &
Affine/time-indexed typestate; reader multiplicity (fractional or collective release); compute-consumption token (\texttt{compute.issue}/\texttt{compute.wait}); predication-safe ``0-byte arrive'' or poison semantics. &
High &
Typestate + affine/linear types; fractional permissions; separation-logic style reasoning. \\
\hline
Tail/prologue/epilogue hangs &
Unified time-loop with guarded produce/consume; guarantee every consumer wait is paired with a dominating producer arrive even under predication; fallback to non-pipelined tail when needed. &
Medium &
Classic software pipelining fill/drain. \\
\hline
Reg pressure hand-wavy \(\Rightarrow\) occupancy collapse/spills &
Reg-aware scheduling objective; iterative deepening on \(D\)/unroll; live-range splitting and rematerialization rules; optional regalloc feedback loop. &
High &
Register-pressure-aware scheduling and modulo scheduling literature. \\
\hline
Warp specialization cliffs &
Disable/degenerate heuristic; only enable specialization when predicted overlap benefit exceeds lost compute throughput + sync overhead + occupancy drop; restrict search neighborhood. &
Medium &
Roofline/occupancy models; GPU producer/consumer heuristics. \\
\hline
IR mismatch to NVIDIA (phase tokens, proxies, async-group completion) &
Hardware-faithful IR: barrier arrival state token, explicit proxy-domain effects (generic/async/tensormap), bulk async-group commit/wait ops, TMA legality verifier (alignment/stride/swizzle). &
High &
\PTX{} barrier/proxy primitives and CUDA async-copy guidance \cite{ptx_isa,cuda_prog_guide}. \\
\hline
AMD: \texttt{s\_waitcnt} is counter-based (no per-transfer tokens) &
Virtual Token Mapping: tokens are stream/prefix ordinals with monotone waits; stage-level grouped issuance; forbid out-of-order token waits. &
Medium &
AMDGPU waitcnt semantics \cite{llvm_waitcnt}. \\
\hline
Compile-time: ``small search'' hides hard scheduling &
\WRsms{} + pruning and caching; early reject on legality/reg budget; deterministic fallback (e.g., \(D=2\), no specialization). &
Medium &
Swing Modulo Scheduling; bounded search with caching. \\
\hline
\end{tabular}
\caption{TITAN patch kit commitments: each vulnerability is closed by a named mechanism that becomes part of the \IR{} contract and/or verifier.}
\end{table*}

\subsection{Transport as a Tokenized Event Structure (Retained, with Backend-Parameterized Semantics)}
We model a kernel as an event structure rather than an index-rewrite problem.

\paragraph{Places.}
Let \(\mathcal{P} = \{\textsf{Global}, \textsf{Shared}_{\CTA}, \textsf{Shared}_{\textsf{Cluster}}, \textsf{Reg}\}\) be memory \emph{places}.
A tensor fragment is always associated with a place \(p \in \mathcal{P}\) and a spatial layout \(L\) (from the seed layout algebra).

\paragraph{Transport primitive (parameterized).}
A transport operation produces a backend-parameterized token:
\[
\textsf{produce} : (p_s, p_d, L, R, s, e, \kappa) \rightarrow t
\]
where:
\begin{itemize}
  \item \(p_s, p_d \in \mathcal{P}\) are source/destination places,
  \item \(L\) is the spatial layout defining the address mapping for region \(R\),
  \item \(R\) is a rectangular (tiled) region of elements,
  \item \(s \in \{0,\dots,D-1\}\) is a stage slot and \(e \in \mathbb{Z}_{\ge 0}\) is an epoch (wrap-around index),
  \item \(\kappa\) is a backend capability describing token semantics:
    \begin{itemize}
      \item NVIDIA: phase/state token for \texttt{mbarrier} completion \cite{ptx_isa},
      \item AMD: virtual ordinal token that maps to a monotone \texttt{s\_waitcnt} frontier \cite{llvm_waitcnt}.
    \end{itemize}
  \item \(t\) is a linear/affine token representing the eventual availability of \(R\) in \(p_d\) under the backend's semantics.
\end{itemize}

\paragraph{Event graph semantics.}
Let \(E\) be a set of events and \(\prec\) a strict partial order. TITAN generates events:
\[
E = E_{\textsf{acquire}} \cup E_{\textsf{produce}} \cup E_{\textsf{ready}} \cup E_{\textsf{wait}} \cup E_{\textsf{compute}} \cup E_{\textsf{release}} \cup E_{\textsf{fence}}
\]
and constructs a dependency relation \(\prec\) such that:
\begin{itemize}
  \item Any consumer wait for \((s,e)\) must be ordered after the corresponding producer produce for \((s,e)\).
  \item Any compute reading from stage \((s,e)\) must be ordered after the corresponding wait-ready for \((s,e)\).
  \item Any producer acquire for \((s,e{+}1)\) must be ordered after the consumer release for \((s,e)\) (runtime backpressure; see below).
\end{itemize}

This explicitly represents \emph{when/how} transport happens and exposes the graph to scheduling and verification.

\subsection{Patch 1: Two-Semaphore Handshake and Epoch-Indexed Backpressure (S1)}
We replace the ``compile-time partial order'' story with an explicit runtime protocol that is provably safe under imbalance (consumer slower than producer) and that eliminates parity/ABA ambiguity by construction.

\paragraph{Time-indexed stage resources.}
Let \(D\) be ring depth. Each logical iteration \(t\) maps to:
\begin{itemize}
  \item stage index \(s(t) = t \bmod D\),
  \item epoch \(e(t) = \lfloor t / D \rfloor\).
\end{itemize}
TITAN's \IR{} and typestate never speak about ``stage \(s\)'' without also tracking epoch \(e\).
This eliminates wrap-around ambiguity by construction.

\paragraph{Two-semaphore handshake per \((s,e)\).}
For each stage slot \(s\) and epoch \(e\), correctness is governed by:
\begin{itemize}
  \item \(\textsf{Free}(s,e)\): permission to overwrite stage \(s\) for epoch \(e\).
  \item \(\textsf{Ready}(s,e)\): evidence that produced data for \((s,e)\) is complete and visible to consumers.
\end{itemize}

\paragraph{Blocking acquire (explicit backpressure).}
Define \texttt{stage.acquire(s,e)} as \emph{blocking} until \(\textsf{Free}(s,e)\) is available, then granting an exclusive write capability \(\textsf{Slot}(s,e)\).
When the consumer is slower, the producer blocks on \(\textsf{Free}\); this is the required runtime backpressure mechanism.

\paragraph{Ring-buffer boundedness invariant (no lapping).}
The protocol enforces:
\[
\forall t:\quad \textsf{produced}(t) - \textsf{consumed}(t) \le D.
\]
This is the precise condition that prevents wrap-around aliasing (ABA). Operationally, it holds because \texttt{stage.acquire} blocks until the slot is truly free.

\paragraph{IR contract sketch (protocol-shaped).}
TITAN shapes candidate regions into:
\begin{itemize}
  \item \textbf{Acquire:} obtain \(\textsf{Slot}(s,e)\) by blocking on \(\textsf{Free}(s,e)\).
  \item \textbf{Produce:} launch async data movement into the slot; obtain a readiness token \(\textsf{ReadyTok}(s,e)\).
  \item \textbf{Consume:} wait-ready, then compute.
  \item \textbf{Release:} signal \(\textsf{Free}(s,e{+}1)\).
\end{itemize}

\subsection{Patch 2: Augmented Affine, Time-Indexed Typestate (S2)}
TITAN attaches an augmented typestate to each \((s,e)\) to cover real hazards: multi-warp readers, decoupled tensor-core consumption, and predication.

\paragraph{Affine/linear resources (per \((s,e)\)).}
We define the following resources:
\begin{itemize}
  \item \(\textsf{Free}(s,e)\): permission to acquire \(\textsf{Slot}(s,e)\).
  \item \(\textsf{Slot}(s,e)\): exclusive write capability to stage buffer for \((s,e)\) (affine; cannot be duplicated).
  \item \(\textsf{ReadyTok}(s,e)\): evidence produced data is complete and visible to consumers.
  \item \(\textsf{ReadPerm}(s,e)\): read permission for consumers; splittable across consumer warps.
  \item \(\textsf{ComputeTok}(s,e)\): evidence compute consuming \((s,e)\) may still be reading shared asynchronously.
  \item \(\textsf{ConsumedTok}(s,e)\): evidence all consumers (including async compute pipelines) are done reading stage memory.
\end{itemize}

\paragraph{State machine (per \((s,e)\)).}
For each stage \(s\) and epoch \(e\):
\[
\textsf{Free}(s,e)
\rightarrow \textsf{InFlight}(s,e)
\rightarrow \textsf{Ready}(s,e)
\rightarrow \textsf{Reading}(s,e)
\rightarrow \textsf{Consumed}(s,e)
\rightarrow \textsf{Free}(s,e{+}1).
\]

\paragraph{Multi-warp readers (collective correctness).}
Warp specialization makes ``the consumer'' a set of warps, not a scalar thread.
TITAN supports either:
\begin{itemize}
  \item \textbf{Fractional permissions (paper-clean):} \(\textsf{ReadPerm}(s,e)\) splits into \(k\) pieces (one per consumer warp), and \(\textsf{ConsumedTok}(s,e)\) is derivable only after rejoining all pieces.
  \item \textbf{Collective release (implementation-friendly):} \texttt{stage.release} is specified as a uniform collective across consumer warps (or across a consumer-only barrier), and the verifier enforces non-divergence in the release path.
\end{itemize}
Either way, TITAN proves:
\begin{quote}
A stage cannot become \(\textsf{Free}(s,e{+}1)\) until \textbf{all} consumer warps have completed their reads for \((s,e)\).
\end{quote}

\paragraph{Async compute consumption token (WAR/UAF closure).}
On modern tensor-core pipelines, ``issue compute'' does not imply ``done reading shared.''
TITAN therefore introduces an explicit compute-consumption token:
\begin{itemize}
  \item \texttt{compute.issue(...)} yields \(\textsf{ComputeTok}(s,e)\),
  \item \texttt{compute.wait(ComputeTok)} is required before \texttt{stage.release}.
\end{itemize}
This closes the classic WAR/UAF hole: overwriting shared memory is illegal until both (i) all consumer warps have logically finished and (ii) the hardware compute pipeline has completed deferred reads.

\paragraph{Predication and partial tiles (no ``wait never arrives'').}
TITAN requires one explicit semantic choice; both are verifiable:
\begin{enumerate}
  \item \textbf{0-byte arrive semantics (default):} the producer always signals readiness (produces a \(\textsf{ReadyTok}(s,e)\)) even if \texttt{bytes=0}. Consumers never hang; correctness is maintained via masked compute or defined zero-fill behavior.
  \item \textbf{Poison semantics:} producer yields \(\textsf{ReadyPoison}(s,e)\); consumers must branch to a safe path; the verifier enforces poison handling.
\end{enumerate}
If neither is feasible (e.g., complex ragged epilogues), TITAN uses a non-pipelined tail fallback rather than risking hangs.

\subsection{Patch 3: Prologue/Epilogue and Unified Time Loop (No Tail Hangs)}
Modulo-scheduled pipelines fail in practice on tails unless prologue/epilogue are explicit.
TITAN generates a single unified time loop that makes production and consumption guards structurally consistent.

Let \(N\) be the number of logical tiles and \(D\) be depth. Define \(P = D-1\).
TITAN generates:
\begin{itemize}
  \item time loop \(t = 0 \ldots N+P-1\),
  \item produce tile index \(p = t\),
  \item consume tile index \(c = t-P\).
\end{itemize}
Producer executes only if \(p < N\). Consumer executes only if \(0 \le c < N\).
Since \(p = c+P\), every consumer wait is paired with a dominating producer produce, by construction.

\paragraph{Warp specialization: identical loop bounds.}
Producer and consumer roles must share the same outer loop bounds and must reach \texttt{pipeline.finalize}.
TITAN forbids ``producer exits early'' patterns that strand consumers on synchronization objects.

\subsection{Patch 6: Proxy Domains and Effects (G1) + Hardware Legality (TMA/Barriers)}
\paragraph{Proxy domains as effects (generic / async / tensormap).}
Every TITAN memory operation is annotated with a proxy domain; TITAN inserts only fences justified by the effect graph:
\begin{itemize}
  \item generic \(\rightarrow\) async: requires \texttt{fence.proxy.async.\{space\}::\{scope\}} when a location written by generic is consumed by async proxy operations \cite{ptx_isa,cuda_prog_guide},
  \item generic \(\rightarrow\) tensormap: requires \texttt{fence.proxy.tensormap::generic} (or fused tensormap fence-proxy) when tensor-map descriptors are updated by generic instructions and then read by tensormap proxy operations \cite{ptx_isa,cuda_tensor_map},
  \item completion is modeled to include implicit proxy fences when the ISA provides them \cite{ptx_isa}, avoiding conservative global fences that serialize the pipeline.
\end{itemize}

\paragraph{Hardware legality: TMA constraints affect ring-buffer layout.}
When lowering staged copies to NVIDIA \TMA{} bulk tensor operations, TITAN verifies that:
\begin{itemize}
  \item shared-memory destination base addresses meet required alignment constraints (e.g., 128-byte alignment for multidimensional bulk tensor copies) \cite{cuda_prog_guide},
  \item global addresses and strides meet alignment and stride-multiple requirements (baseline strengthened under swizzle modes) \cite{cuda_prog_guide},
  \item swizzle repeat boundaries and per-stage padding preserve legality across ring-buffer stages.
\end{itemize}
If illegal, TITAN either pads the stage stride to restore legality or falls back to a non-\TMA{} path.

\paragraph{Barrier object constraints.}
Barrier objects and their placement (CTA shared vs cluster shared) impose additional constraints on where per-stage synchronization state can live and which operations are legal \cite{ptx_isa}.
TITAN checks legality before committing a schedule.

\subsection{Patch 7: Backend-Parameterized Tokens and AMD Virtual Token Mapping (G2)}
\subsubsection{NVIDIA: phase/state-based barrier completion}
On NVIDIA, waiting is not ``wait(barrier)''; it is waiting on completion of a particular phase/state.
TITAN models this explicitly by representing an arrival state token returned by arrive operations and requiring it for waits \cite{ptx_isa}.

\subsubsection{AMD: Virtual Token Mapping to counter-based completion}
On AMD, \texttt{s\_waitcnt} is counter-based, so per-transfer capability tokens are not faithfully representable in general \cite{llvm_waitcnt}.
TITAN therefore uses \textbf{Virtual Token Mapping}:
\begin{itemize}
  \item A token is a stream ordinal in a per-class issue stream: \(\textsf{VTok}(\textsf{class}, n)\).
  \item The verifier enforces \textbf{monotone waits} (no out-of-order waiting) within a class.
  \item Transfers belonging to a single stage are issued as a contiguous group, and the consumer waits the group token (stage-level granularity).
\end{itemize}
If a candidate schedule would require selective waiting (e.g., waiting on ``the middle'' op while leaving older ones outstanding), it is rejected or rewritten (grouping, smaller \(D\), or disabling specialization).
This makes AMD portability explicit, verifiable, and non-illusory.

\subsection{Patch 4/5/8: Resource-Constrained Scheduling with Reg Budget and Specialization Heuristics}
TITAN schedules producer and consumer roles under hard resource constraints and an explicit register budget.

\paragraph{Reg-aware acceptance criterion.}
A candidate schedule is feasible only if predicted peak live weight satisfies a reg budget (and occupancy floor), and shared-memory footprint satisfies:
\[
D \cdot \textsf{StageBytes} \le \textsf{SmemBudget}(\textsf{SM}, \textsf{ClusterSize}).
\]
When estimates are insufficient, TITAN optionally performs a compile-and-measure regalloc feedback loop and retries with smaller \(D\) or reduced unroll.

\paragraph{Warp specialization disable heuristic.}
Warp specialization is enabled only when predicted overlap benefit exceeds lost compute throughput plus synchronization overhead and the resulting occupancy remains above a floor.
Otherwise, TITAN degenerates to a non-specialized pipeline (or synchronous baseline) to avoid sharp cliffs on small tiles and low-\(K\) kernels.

\paragraph{\WRsms{} scheduling and compile-time budget.}
TITAN uses a bounded search over small \(D\) and warp partitions, scheduling each steady state via a swing-modulo-scheduling variant that is role-aware and reg-budget-aware.
Candidates are pruned early on legality/reg/smem failures, schedules are cached by (arch, tile shape, dtype, fusion pattern), and a deterministic fallback is always available.

\subsection{How TITAN Subsumes the Seed's Layout Algebra (Preserved, with Temporal Contract)}
Linear Layouts answers: \emph{given an op graph, choose layouts and conversions that make memory movement cheap}.
TITAN adds: \emph{given chosen layouts, schedule the movement to occur asynchronously and correctly}.

Formally, we treat the seed layout \(L\) as a parameter to transport:
\[
\textsf{produce}(p_s,p_d,L,R,s,e,\kappa) \text{ is legal iff } \textsf{LegalTransport}(L,R,p_s,p_d,\kappa).
\]
Thus, TITAN composes with the seed system:
\begin{enumerate}
  \item Seed selects \(L\) and inserts conversions/swizzles (spatial).
  \item TITAN decides which movements become staged async transfers, where barriers and proxy fences go, how to overlap them with compute, and how to make the schedule correct under tails and imbalance (temporal).
\end{enumerate}

% ============================================================
\section{System Architecture (The Compiler) --- Robust Version}
% ============================================================

\subsection{Architecture Overview (Four Enforceable Stages)}
TITAN compiles pipelined GPU kernels through four enforceable stages:
\begin{enumerate}
  \item \textbf{Pipeline Construction (IR shaping):} rewrite candidate regions into a staged ring-buffer form with explicit acquire/produce/wait/release structure and guarded prologue/epilogue.
  \item \textbf{\WRsms{} Scheduling:} schedule producer/consumer roles with resource constraints and a register budget objective; reject or degrade configurations that violate budgets.
  \item \textbf{Verification:} run (i) augmented typestate (protocol correctness) and (ii) proxy/effects verification (visibility correctness), plus hardware legality checks (e.g., \TMA{} constraints).
  \item \textbf{Lowering (backend-specific):} map the verified \IR{} to NVIDIA or AMD primitives, using backend-parameterized token semantics.
\end{enumerate}
If any step fails, TITAN falls back to a deterministic baseline (e.g., \(D=2\), no warp specialization), rather than emitting ``probably-correct'' code.

\subsection{Compiler Overview (Flow Diagram, Updated)}
TITAN is implemented as a backend transformation in Triton's NVIDIA/AMD backends, positioned after layout propagation (so it sees final shared-memory shapes/swizzles) and before low-level LLVM/\PTX{} emission (so it can control barrier placement, unrolling, warp roles, and proxy fences).

\begin{figure}[t]
\begin{Verbatim}[fontsize=\footnotesize, commandchars=\\\{\}]
+--------------------------------------------------------------+
|  PyTorch Inductor / Triton Frontend                           |
|   - tiling choices, fusion, epilogue structure                |
+------------------------------+-------------------------------+
                               |
                               v
+--------------------------------------------------------------+
|  TTIR  ->  TTGIR                                             |
|   - tensor ops + memory ops                                  |
+------------------------------+-------------------------------+
                               |
                               v
+--------------------------------------------------------------+
|  Seed Pass: Linear Layouts (spatial)                          |
|   - choose per-op layout L                                    |
|   - synthesize swizzles / convert_layout                       |
+------------------------------+-------------------------------+
                               |
                               v
+--------------------------------------------------------------+
|  TITAN Pass: Protocol-Shaped Pipeline + Verified Scheduling   |
|   (1) Pipeline construction (stages+epochs, pro/epi guards)   |
|   (2) Two-semaphore handshake (Ready/Free) + stage acquire    |
|   (3) Warp roles: producers vs consumers                      |
|   (4) WR--SMS schedule under reg/smem constraints              |
|   (5) Typestate verify (multi-reader, compute-tokens, tails)  |
|   (6) Proxy/effects verify + fence synthesis (3 proxies)      |
|   (7) Hardware legality verify (TMA align/stride/swizzle)     |
+------------------------------+-------------------------------+
                               |
                               v
+--------------------------------------------------------------+
|  LLVM IR / Target Lowering                                   |
|   NVIDIA: TMA + mbarrier state tokens + proxy fences + groups |
|   AMD: pipelined global->LDS + s_waitcnt via Virtual Tokens   |
+------------------------------+-------------------------------+
                               |
                               v
+--------------------------------------------------------------+
|  GPU Binary (SASS / GCN)                                      |
+--------------------------------------------------------------+
\end{Verbatim}
\caption{TITAN compilation flow: spatial layout selection remains intact; TITAN makes transport scheduling first-class, hardware-faithful, and verifiable.}
\end{figure}

\subsection{IR Extensions: Protocol Ops, Tokens, and Proxy Effects}
TITAN introduces (or reuses) a minimal set of \MLIR{}-like ops in the Triton GPU \IR{}.
The critical change from earlier drafts is that the op set is explicitly \emph{isomorphic} to real completion/fence mechanisms, rather than a ``nice-looking'' token API.

\paragraph{Core protocol ops (backend-neutral).}
\begin{itemize}
  \item \texttt{ttg.pipe.init(D, roles)}: construct a pipeline object and bind warp roles.
  \item \texttt{ttg.stage.acquire(pipe, s, e)} \(\rightarrow\) yields \texttt{!ttg.slot}: blocks on \(\textsf{Free}(s,e)\).
  \item \texttt{ttg.stage.produce(slot, \dots)} \(\rightarrow\) yields \texttt{!ttg.ready}: launches async transport and produces \(\textsf{ReadyTok}(s,e)\) under backend semantics.
  \item \texttt{ttg.stage.wait\_ready(ready)}: establishes \(\textsf{Ready}(s,e)\) before reads.
  \item \texttt{ttg.compute.issue(\dots)} \(\rightarrow\) yields \texttt{!ttg.compute\_tok}: models decoupled compute consumption.
  \item \texttt{ttg.compute.wait(compute\_tok)}: required before release.
  \item \texttt{ttg.stage.release(slot, consumed)}: signals \(\textsf{Free}(s,e{+}1)\).
  \item \texttt{ttg.pipe.finalize(pipe)}: collective finalization (required for warp specialization).
\end{itemize}

\paragraph{Proxy/effects ops (backend-neutral interface, backend-specific lowering).}
\begin{itemize}
  \item \texttt{ttg.proxy.fence(from, to, scope, space)}: inserted only when crossing proxy domains.
  \item Memory ops carry proxy-domain annotations: \textsf{generic}, \textsf{async}, \textsf{tensormap}.
\end{itemize}

\paragraph{NVIDIA-specific lowering traits (phase tokens and bulk groups).}
\begin{itemize}
  \item \texttt{ttg.nv.mbarrier.arrive\_expect\_tx(bar, tx)} \(\rightarrow\) yields \texttt{!nv.mbarrier.state} \cite{ptx_isa}.
  \item \texttt{ttg.nv.mbarrier.wait(bar, state)}: phase/state-based completion \cite{ptx_isa}.
  \item \texttt{ttg.nv.tma.async\_bulk\_tensor(\dots)}: \TMA{} issue ops.
  \item \texttt{ttg.nv.bulk\_group.commit} / \texttt{ttg.nv.bulk\_group.wait(n)}: async-group completion for store-direction mechanisms where required \cite{ptx_isa}.
\end{itemize}

\paragraph{AMD-specific lowering traits (virtual tokens).}
\begin{itemize}
  \item \texttt{ttg.amd.issue\_mem(class)} \(\rightarrow\) yields \texttt{!ttg.vtok}: increments a per-class stream ordinal.
  \item \texttt{ttg.amd.wait(vtok)}: lowers to monotone \texttt{s\_waitcnt} threshold updates \cite{llvm_waitcnt}.
\end{itemize}

\noindent A sketch of the intended \IR{} is shown below (notation is illustrative; exact op names follow Triton backend conventions):

\begin{lstlisting}[language=C,caption={Protocol-shaped TITAN IR sketch (TTGIR/MLIR-like).}]
%pipe  = ttg.pipe.init %D, %roles : (...) -> !ttg.pipe

// Unified time-loop induces (s,e) = (t % D, t / D)
%slot  = ttg.stage.acquire %pipe, %s, %e : (...) -> !ttg.slot

// Producer: launch async transport into slot; produce backend token.
%rdy   = ttg.stage.produce %slot, %src_desc, %bytes, %proxy
         : (!ttg.slot, !ttg.src, i32, !ttg.proxy) -> !ttg.ready

// Consumer: establish readiness, then issue compute reading from the stage.
ttg.stage.wait_ready %rdy : !ttg.ready -> ()
%ctok  = ttg.compute.issue %slot, %acc : (...) -> !ttg.compute_tok
ttg.compute.wait %ctok : !ttg.compute_tok -> ()

// Collective release after all readers + compute completion.
%cons  = ttg.stage.consumed %slot : !ttg.slot -> !ttg.consumed
ttg.stage.release %slot, %cons : (!ttg.slot, !ttg.consumed) -> ()

ttg.pipe.finalize %pipe : !ttg.pipe -> ()
\end{lstlisting}

\subsection{Warp Specialization and Ring-Buffer Placement (Retained, with Protocol Semantics)}
TITAN explicitly partitions warps in a thread block into producer and consumer roles.
Producers issue async produces and signal readiness; consumers wait-ready and execute tensor-core compute, then collectively release.

\begin{figure}[t]
\begin{Verbatim}[fontsize=\footnotesize, commandchars=\\\{\}]
+--------------------------------------------------------------+
|  Thread Block (CTA) with Warp Specialization                  |
|                                                              |
|  Warps:  W0 W1 | W2 W3 W4 W5 W6 W7                            |
|         [PROD] | [       CONSUME / WGMMA       ]              |
|                                                              |
|  Shared Memory Ring Buffer (depth D)                          |
|   stage 0: [A_tile][B_tile]  (Ready/Free state, epoch-tagged) |
|   stage 1: [A_tile][B_tile]  (Ready/Free state, epoch-tagged) |
|   ...                                                        |
|   stage D-1:             (Ready/Free state, epoch-tagged)     |
|                                                              |
|  Unified time loop t = 0 .. N + (D-1) - 1                     |
|   Producer: if t < N: acquire_free(s,e) -> produce -> ready   |
|   Consumer: if 0 <= t-(D-1) < N: wait_ready -> compute -> rel |
+--------------------------------------------------------------+
\end{Verbatim}
\caption{Warp specialization and pipelined shared-memory staging. TITAN allocates ring buffers and verifies epoch-indexed slot ownership with explicit backpressure.}
\end{figure}

\subsection{TITAN Pass Pipeline (From Construction to Verified Lowering)}
\paragraph{(1) Candidate region identification.}
We match producer-consumer motifs where shared-memory staging is beneficial:
\begin{itemize}
  \item Matmul-like kernels (GEMM, grouped GEMM, MoE MLPs),
  \item Attention tiles (FlashAttention-style), and
  \item Structured epilogues with predictable reuse.
\end{itemize}
We reuse the seed's anchor ops (e.g., MMA/WGMMA) as scheduling anchors.

\paragraph{(2) Pipeline construction (IR shaping).}
Rewrite eligible loads/stores into explicit staged operations with:
\begin{itemize}
  \item ring-buffer stages \(s \in \{0,\dots,D-1\}\),
  \item epochs \(e\) (wrap-around index),
  \item unified time loop with guarded produce/consume.
\end{itemize}

\paragraph{(3) Two-semaphore handshake materialization.}
Materialize \(\textsf{Free}(s,e)\) and \(\textsf{Ready}(s,e)\) via backend-specific mechanisms (barrier phase tokens on NVIDIA; counter-frontier tokens on AMD), but keep a backend-neutral verification interface.

\paragraph{(4) \WRsms{} scheduling under resource constraints.}
Choose:
\begin{itemize}
  \item pipeline depth \(D \in \{2,3,4,5\}\) (bounded discrete search),
  \item producer warp count \(W_P\) and consumer warp count \(W_C\),
  \item unroll factor \(u\),
\end{itemize}
subject to shared-memory constraints, \TMA{} legality constraints (NVIDIA), and reg budget constraints. TITAN disables specialization when the heuristic predicts cliffs.

\paragraph{(5) Verification (typestate + proxy/effects + legality).}
Run:
\begin{itemize}
  \item augmented typestate checking (epochs, multi-reader, compute tokens, predication/tails),
  \item proxy-domain effect checking and fence synthesis (generic/async/tensormap),
  \item hardware legality checks (TMA alignment/stride/swizzle; barrier placement rules).
\end{itemize}

\paragraph{(6) Lowering.}
Lower protocol ops to target ISA:
\begin{itemize}
  \item NVIDIA: \TMA{} bulk tensor ops + \texttt{mbarrier} phase/state tokens + proxy fences + async-group completion where required \cite{ptx_isa,cuda_prog_guide}.
  \item AMD: global\(\rightarrow\)\LDS{} software pipelining + \texttt{s\_waitcnt} via \VTM{} + workgroup barriers as required \cite{llvm_waitcnt,amdgpu_usage}.
\end{itemize}

\subsection{Algorithms (Updated to Match the Verified Protocol)}

\begin{algorithm}[t]
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{\TTGIR{} region \(R\) with spatial layouts \(L\), hardware model \(H\)}
\Output{Region \(R'\) with protocol-shaped pipeline, verified schedule, and backend-parameterized tokens}
\BlankLine
Extract candidate producer-consumer subgraph \(G \subset R\) anchored at MMA/WGMMA ops\;
Rewrite eligible loads/stores into staged ring-buffer protocol with epochs and a unified time loop\;
\ForEach{pipeline configuration \((D, W_P, W_C, u)\) in bounded search space}{
  Check shared-memory budget and (NVIDIA) \TMA{} legality constraints; reject if violated\;
  Materialize two-semaphore handshake (\(\textsf{Free}/\textsf{Ready}\)) at \IR{} level\;
  Synthesize warp roles (producer vs consumer) and build \WRsms{} modulo schedule \(\tau\)\;
  Estimate register pressure; apply mitigation (reduce \(u\), reduce \(D\), rematerialize, split) if needed\;
  Run verification: typestate (epochs, multi-reader, compute tokens, tails) + proxy/effects + legality\;
  Score configuration by predicted initiation interval + occupancy penalty + fence/sync overhead\;
}
Select best feasible configuration; commit rewriting into \(R'\)\;
Lower protocol ops to target ISA (NVIDIA: \TMA{}+\texttt{mbarrier} state tokens+\texttt{fence.proxy}; AMD: \VTM{}+\texttt{s\_waitcnt})\;
\caption{TITAN compilation: protocol-shaped construction + \WRsms{} scheduling + verification + backend-parameterized lowering.}
\end{algorithm}

\begin{algorithm}[t]
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Protocol-shaped pipeline region with epochs, role partitions, and proxy annotations}
\Output{Accept (safe) or Reject (protocol/effects violation) + diagnostic}
\BlankLine
Verify unified-loop guards: every consumer \texttt{wait\_ready(s,e)} is dominated by a matching producer \texttt{produce(s,e)} or 0-byte/poison-ready\;
Verify typestate over \((s,e)\): acquire blocks on \(\textsf{Free}\); produce yields \(\textsf{ReadyTok}\); release yields \(\textsf{Free}(s,e{+}1)\)\;
Verify multi-reader discipline: collective release or permission split/join across consumer warps\;
Verify compute-consumption tokens: \texttt{compute.wait} dominates \texttt{stage.release}\;
Verify proxy/effect obligations: insert minimal \texttt{proxy.fence} when crossing proxy domains; account for implicit fences on completion\;
Verify hardware legality: (NVIDIA) \TMA{} alignment/stride/swizzle, barrier placement constraints\;
\If{any check fails}{
  Reject configuration; emit diagnostic; optionally trigger safe fallback\;
}
Accept\;
\caption{Verification: typestate + tail guards + multi-reader + compute tokens + proxy effects + legality.}
\end{algorithm}

\subsection{Lowering to Real Hardware (Revised for Hardware Fidelity)}
\paragraph{NVIDIA H100 (SM90) path.}
When descriptor and layout constraints are met, \texttt{stage.produce} lowers to \TMA{} bulk tensor operations:
\begin{itemize}
  \item Use a tensor-map descriptor for 2D--5D tiling \cite{ptx_isa,cuda_tensor_map}.
  \item Enforce \TMA{} legality constraints (alignment/stride/swizzle) at compile time; pad stages or fall back when needed \cite{cuda_prog_guide}.
  \item Use barrier completion with phase/state tokens: waiting is expressed as \texttt{mbarrier.wait(bar, state)} where \texttt{state} comes from arrive/expect-tx semantics \cite{ptx_isa}.
  \item Insert proxy fences only when justified by the proxy/effects graph (generic/async/tensormap), exploiting implicit fences on completion to avoid over-fencing \cite{ptx_isa,cuda_prog_guide}.
  \item Model and emit async-group commit/wait completion mechanisms where required (notably for store-direction bulk operations) \cite{ptx_isa}.
\end{itemize}

\paragraph{AMD MI300 (CDNA3) path.}
MI300 lacks a direct \TMA{} analogue; nevertheless, TITAN's protocol-shaped \IR{} remains useful:
\begin{itemize}
  \item \texttt{stage.produce} lowers to a software-pipelined global\(\rightarrow\)\LDS{} transfer, where tokens correspond to the \VTM{} stream ordinals rather than per-transfer handles.
  \item Token consumption is enforced via monotone \texttt{s\_waitcnt} thresholds and workgroup/wave-compatible barriers; the same typestate rules apply to prevent stage hazards, but token granularity is explicitly stage/prefix-level \cite{llvm_waitcnt,amdgpu_usage}.
\end{itemize}
This provides a portability story: TITAN is a transport scheduling and correctness abstraction, with NVIDIA benefiting from \TMA{} and AMD benefiting from disciplined pipelining and verifiable safety under counter-based completion.

% ============================================================
\section{Evaluation Plan (New Evaluation Plan)}
% ============================================================

\subsection{Overview: What the Evaluation Must Prove}
TITAN's evaluation must prove three things simultaneously:
\begin{enumerate}
  \item \textbf{Correctness under adversarial kernel behaviors} (imbalance, predication, multi-warp consumption).
  \item \textbf{Performance stability} (no cliffs from warp specialization; no over-fencing; controlled register pressure).
  \item \textbf{Portability without semantic cheating} (AMD counter model acknowledged; performance impact quantified).
\end{enumerate}

\subsection{Platforms and Tooling (Preserved and Extended)}
\paragraph{Hardware.}
\begin{itemize}
  \item NVIDIA H100 (SM90): validate \TMA{}, \texttt{mbarrier} phase/state, warp specialization, and cluster/\DSMEM{} (where available).
  \item AMD MI300: validate portability of protocol-shaped scheduling and typestate safety under wave64 and \LDS{} banking constraints.
\end{itemize}

\paragraph{Profilers and measurement.}
\begin{itemize}
  \item NVIDIA: Nsight Compute for memory throughput, barrier stall reasons, tensor core utilization, and occupancy.
  \item AMD: rocprof (or equivalent) for memory stalls, \LDS{} conflicts, wave occupancy.
  \item End-to-end: wall-clock latency for representative inference operators (decode attention, MoE MLP).
\end{itemize}

\subsection{A) Correctness Validation (Stress Tests + Negative Tests)}
\subsubsection{A1. Stress test: ring buffer saturation (imbalance)}
\textbf{Goal:} demonstrate that explicit backpressure prevents overwrite/ABA and avoids deadlock when consumers lag.
\begin{itemize}
  \item Construct kernels where consumer throughput is artificially reduced: heavy epilogue, forced \LDS{}/shared bank conflicts, controlled instruction stalls, reduced occupancy via register inflation.
  \item Sweep \(D \in \{2,3,4,5\}\) and warp partitions.
  \item Metrics: no hangs; correct outputs; producer stall time at \texttt{stage.acquire} increases smoothly under imbalance (no corruption).
\end{itemize}
\textbf{Ablation:} remove blocking acquire (or replace with non-blocking) to demonstrate the exact failure mode TITAN prevents.

\subsubsection{A2. ABA / wrap-around adversary}
\textbf{Goal:} demonstrate epoch-indexed safety.
Use very small \(D\) (e.g., 2) and many iterations with randomized per-iteration stalls; validate consumers never observe a reused parity as false-ready.

\subsubsection{A3. Multi-warp reader correctness}
\textbf{Goal:} demonstrate that a stage is not released until all consumer warps are done.
Construct consumer regions where warps have unequal work (predicated compute, divergent epilogue); validate releases occur only after all warps complete; verify intentionally broken early-release variants are rejected.

\subsubsection{A4. Async compute hazard test (WAR/UAF closure)}
\textbf{Goal:} show compute-consumption tokens are necessary and effective.
Compare variants (i) with \texttt{compute.wait} enforced before \texttt{stage.release} vs (ii) without it; validate (ii) can fail under stress while (i) does not.

\subsubsection{A5. Prologue/epilogue and tail correctness}
\textbf{Goal:} no ``wait never arrives'' hangs.
Sweep \(N\) relative to \(D\): \(N < D\), \(N = D\), \(N \gg D\). Test ragged tiles and predication-heavy shapes.
Compare unified-loop pipelined path vs fallback tail path; ensure both are correct and no-hang.

\subsubsection{A6. Proxy fence litmus suite (NVIDIA)}
\textbf{Goal:} make proxy consistency a measurable strength.
Construct litmus patterns (generic writes \(\rightarrow\) async reads; generic writes \(\rightarrow\) tensormap reads; completion \(\rightarrow\) generic consumption).
Compare TITAN minimal fences vs conservative always-fence vs intentionally under-fenced.
Metrics: correctness, fence count, overlap/performance impact.

\subsubsection{A7. TMA legality tests (alignment/swizzle)}
\textbf{Goal:} demonstrate legality verifier and padding behavior.
Generate stage layouts spanning alignments/strides/swizzle modes; validate illegal cases are rejected or padded; padded cases remain correct; measure padding overhead.

\subsection{B) Performance Evaluation (End-to-End + Targeted Ablations)}
\subsubsection{B1. End-to-end benchmarks}
Evaluate workloads that exercise staged async transfers and tensor-core compute:
\begin{itemize}
  \item GEMM families across shapes (including small tiles and low-\(K\)),
  \item attention-style blocks (short-\(K\), heavy epilogue),
  \item fused epilogues (activation, reduction, stores),
  \item memory-bound kernels to test ``don't over-specialize'' behavior.
\end{itemize}

\subsubsection{B2. Ablation: warp specialization heuristic (avoid cliffs)}
Compare always-on specialization vs TITAN conditional heuristic vs always-off.
Focus on small tiles, short \(K\), and reg-tight configurations.

\subsubsection{B3. Ablation: register-aware scheduling}
Compare naive modulo scheduling (no reg budget) vs TITAN \WRsms{} with reg budget + mitigation vs optional regalloc feedback variant.
Report occupancy stability and spill counts.

\subsubsection{B4. Ablation: proxy modeling}
Compare minimal effect-driven fences vs conservative fences.
Report overlap efficiency and throughput differences.

\subsection{C) Portability Evaluation (AMD Reality Acknowledged)}
\subsubsection{C1. Ablation: AMD Virtual Token Mapping overhead}
On AMD, compare:
\begin{itemize}
  \item TITAN \VTM{} (monotone waits, grouped per stage),
  \item conservative \texttt{s\_waitcnt(0)}-style full waits,
  \item serialized per-transfer emulation baseline (expected slow).
\end{itemize}
Measure overlap (in-flight operations), stall cycles at waits, end-to-end performance.

\subsubsection{C2. Semantic coverage + fallback rate}
Report fraction of configurations rejected due to non-representable token patterns under \VTM{} constraints.
Report which fallbacks trigger (smaller \(D\), grouped transfers, no warp specialization).

\subsection{D) Compile-Time Budget (to preempt ``this is autotuning'')}
TITAN's search is bounded and cached. We report:
\begin{itemize}
  \item compile time distribution (median / p95) across benchmark suite,
  \item number of candidate schedules explored per kernel,
  \item cache hit rates by (tile shape, dtype, arch, fusion pattern),
  \item fallback frequency.
\end{itemize}

\subsection{Baselines (Preserved and Aligned)}
\begin{itemize}
  \item \textbf{Seed baseline:} Linear Layouts-style spatial layout propagation and conversions without protocol-shaped transport scheduling (status quo backend).
  \item \textbf{Triton baseline:} existing Triton implementations (including any available \TMA{} tutorials/heuristics) treated as black-box.
  \item \textbf{Vendor baseline:} cuBLASLt / CUTLASS (NVIDIA) and rocBLAS / CK-like kernels (AMD) where applicable for GEMM-like workloads.
  \item \textbf{Ablations:}
    \begin{itemize}
      \item TITAN without typestate verification (to quantify safety vs performance),
      \item TITAN with fixed \(D=2\) vs TITAN searched \(D\),
      \item TITAN without warp specialization (single-role warps),
      \item TITAN without proxy/effects modeling (conservative always-fence) vs effect-driven minimal fences.
    \end{itemize}
\end{itemize}

\subsection{Metrics (Preserved)}
We report:
\begin{itemize}
  \item \textbf{SOL\% (Speed-of-Light)} for memory and compute:
    achieved bandwidth / peak HBM bandwidth, and achieved MMA throughput / peak.
  \item \textbf{Overlap efficiency:} fraction of time copy engine is busy while tensor core pipe is busy.
  \item \textbf{Barrier overhead:} stall cycles attributed to waits/barriers.
  \item \textbf{Register pressure and occupancy:} registers per thread, achieved occupancy, and sensitivity to pipeline depth.
  \item \textbf{Instruction mix:} changes in control/synchronization instruction count.
  \item \textbf{Fence count and proxy transitions:} number and kinds of proxy fences inserted, and measured impact on overlap.
\end{itemize}

% ============================================================
\section{Conclusion}
% ============================================================
We propose \textbf{TITAN}, a compiler backend framework that makes asynchronous transport a first-class, verifiable \IR{} concept in Triton. TITAN addresses the seed's core limitation---optimizing \emph{where} data lives without modeling \emph{when/how} it moves---by introducing:
\begin{enumerate}
  \item a \textbf{protocol-shaped transport calculus} (acquire/produce/wait/release over stages and epochs),
  \item an \textbf{augmented typestate protocol} that enforces correct pipelined use of shared-memory stages under imbalance, multi-warp consumption, predication/tails, and decoupled compute consumption, and
  \item a \textbf{resource-aware scheduler} that searches pipeline depth and warp specialization under explicit register and shared-memory constraints with deterministic fallback.
\end{enumerate}
TITAN is designed for real-metal impact on NVIDIA H100 via \TMA{}+\texttt{mbarrier} phase/state tokens and explicit proxy-domain effects, and offers a portability path to AMD MI300 through Virtual Token Mapping to counter-based completion. In doing so, TITAN turns what is currently a hand-optimized kernel trick into a systematic compiler optimization that composes with (rather than replaces) the seed's spatial layout algebra.

% ============================================================
\begin{thebibliography}{20}

\bibitem{seed_linear_layouts}
``Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using \(\mathbb{F}_2\),'' arXiv:2505.23819 (HTML edition).

\bibitem{hopper_tuning}
NVIDIA, ``Hopper Tuning Guide,'' CUDA Toolkit Documentation.

\bibitem{ptx_isa}
NVIDIA, ``Parallel Thread Execution (\PTX{}) ISA,'' CUDA Toolkit Documentation.

\bibitem{cuda_prog_guide}
NVIDIA, ``CUDA C++ Programming Guide,'' CUDA Toolkit Documentation.

\bibitem{cuda_tensor_map}
NVIDIA, ``CUDA Driver API: Tensor Memory / Tensor Map Encoding,'' CUDA Toolkit Documentation.

\bibitem{llvm_waitcnt}
LLVM Project, ``AMDGPU \texttt{s\_waitcnt} Semantics,'' LLVM Documentation.

\bibitem{amdgpu_usage}
LLVM Project, ``AMDGPU Backend Usage and Barriers,'' LLVM Documentation.

\bibitem{triton}
``Triton Language and Compiler,'' Triton project documentation.

\bibitem{warp_specialization}
``Warp Specialization in PyTorch/Triton,'' PyTorch engineering blog.

\bibitem{cypress}
``Task-Based Tensor Computations on Modern GPUs,'' PLDI 2025 (NVIDIA Research).

\bibitem{tawa}
``Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References,'' arXiv preprint (2025).

\bibitem{mlir}
LLVM Project, ``\MLIR{}: Multi-Level Intermediate Representation,'' documentation.

\bibitem{dialegg}
``DialEgg: Dialect-Agnostic \MLIR{} Optimizer using Equality Saturation with Egglog,'' CGO 2025.

\bibitem{amd_mi300}
AMD ROCm documentation and optimization notes on \LDS{} banking and wave64 execution.

\end{thebibliography}

\end{document}
