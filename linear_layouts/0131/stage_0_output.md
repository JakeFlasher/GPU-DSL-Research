## Ground Truth Glossary

| Term | Hardware_Definition (Manuals) | Hard_Constraints (Alignment/Size) | Governing_Math_Domain (precise) | Paper_1_Coverage (Linear Layouts) | Paper_2_Coverage (ISL) | Paper_3_Coverage (Categorical) |
|---|---|---|---|---|---|---|
| `CuTensorMap` / TMA | **TMA (Tensor Memory Accelerator)** is Hopper’s “more sophisticated” async copy engine for moving **1D up to 5D** tensors **global ↔ shared**, plus **shared↔shared across CTAs in a cluster** (distributed shared memory). Hopper guide also notes reduction capabilities for writes (e.g., elementwise reductions). ([docs.nvidia.com](https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html))<br><br>A **tensor map object** (`CUtensorMap`) is an **opaque** descriptor created by CUDA driver APIs and intended to be accessed only via CUDA APIs and PTX. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>In PTX, a **tensor-map object is 1024 bits** (`.b1024`), and `tensormap.replace` is specified over that full object. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Descriptor / object constraints (encode-time):**<br>• `tensorMap` address **64B-aligned**. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Rank:** tiled encode allows `tensorRank ∈ {1..5}` (and if `interleave != NONE`, additionally `tensorRank ≥ 3`); im2col encode requires `tensorRank ∈ {3,4,5}`. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalAddress` **16B-aligned**; tightened to **32B** for `interleave=32B` and for certain packed datatypes. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalDim[i]` **non-zero** and `≤ 2^32`; packed types impose extra congruences (e.g., `globalDim[0]` multiple-of-128 for some). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalStrides[i]` (bytes) **multiple-of-16** and `< 2^40`; tightened to **multiple-of-32** for `interleave=32B` and some packed types. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `boxDim[i]` **non-zero** and `≤ 256`; for `interleave=NONE`, require `(boxDim[0] * elementSizeBytes)` **multiple-of-16B**; some packed types force `boxDim[0]=128`. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `elementStrides[i]` **non-zero** and `≤ 8`; also: for `interleave=NONE`, **dimension-0 stride is not supported** (first element ignored). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>**Swizzle constraints:**<br>• Supported driver-level swizzles include `NONE`, `32B`, `64B`, `128B`, `128B_ATOM_32B` (and some modes restricted by datatype). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• If `swizzle != NONE` and `interleave=NONE`, the “inner dimension” (bytes) must be `≤ swizzleSize` (e.g., `≤32/64/128`). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>**Mutation / memory-model note:** `tensormap.replace` is specified as a weak memory op over the full 1024-bit object. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Two coupled domains (not “just linear algebra”):**<br>1) **Presburger-style integer constraints** over $$\mathbb{Z}$$ (boxes, ranks, bounds, byte strides) **plus congruences** like “multiple of 16/32/128” (i.e., constraints in $$\mathbb{Z}$$ modulo $$2^k$$ ideals). Grounded by explicit “multiple-of” constraints in the driver API. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>2) **Bit-level / bank swizzle**: swizzle described as **reordering fixed-size chunks** (16B chunks within a 32/64/128B span), i.e. a **finite permutation action**; implementable as affine maps on bit-vectors (GF(2)-linear pieces) when expressed via XOR/AND/shift. Driver API calls it a “shared memory bank swizzling pattern.” ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) | **Partially related math (GF(2) layouts, bank conflicts), but not TMA descriptors.** The paper models layouts as binary-matrix maps on bits and discusses swizzling/bank-conflict minimization, but does not specify CUDA tensor-map descriptor bounds/alignment rules (rank limits, 64B descriptor alignment, stride multiples, etc.). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | **Models swizzle + GF(2) layouts, not TMA descriptors.** ISL paper defines CuTe swizzles using XOR/AND/shift and models Triton linear layouts as transformations between binary vector spaces, but does not discuss `CUtensorMap`/TMA encode-time constraints. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | **Abstract algebra only.** Categorical paper focuses on CuTe layout algebra (composition/product/division) and “alignment with CUTLASS behavior” in the sense of semantic agreement, not hardware descriptor constraints like 16B/32B alignment or rank≤5. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `TMEM` (Blackwell Tensor Memory) | PTX ISA defines **Tensor Memory** as a **dedicated on-chip memory** for **5th generation TensorCore** operations, organized as a 2D array of **lanes (rows)** and **columns**. On **`sm_100a/sm_100f`**, it is **512 columns × 128 rows per CTA**, each cell **32 bits**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>Blackwell tuning guide states **B200 is compute capability 10.0** (context for “Blackwell-era” features). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/blackwell-tuning-guide/index.html)) | **Structural / addressing constraints:**<br>• **Shape/capacity:** 512 columns × 128 rows per CTA, 32-bit cells. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• **Address is 32-bit**, composed of **lane index** and **column index** (bitfield split shown in PTX). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Allocation constraints (dynamic):**<br>• Tensor Memory must be **allocated by a single warp** in a CTA using `tcgen05.alloc`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Allocation/deallocation is in **columns**; **unit = 32 columns**; requested column-count must be a **power of 2**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• All allocated TMEM must be explicitly **deallocated before kernel exit**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Access restrictions (warpgroup partition):** lanes are partitioned by warp-within-warpgroup: warp 0→lanes 0–31, warp 1→32–63, warp 2→64–95, warp 3→96–127 (for `tcgen05.ld/st` access restrictions). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Collective access constraint:** `tcgen05.ld` requires **all threads in the warp use the same `taddr`**; otherwise **undefined behavior**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Alignment example constraint:** `tcgen05.shift` requires the **lane of `taddr` aligned to 32**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Finite-lattice + bitfield arithmetic (not continuous):**<br>• Address space is essentially a **finite grid** $$\mathbb{Z}_{128} \times \mathbb{Z}_{512}$$ (lane × column), encoded via **bitfield extraction** (binary representation / masking/shifting). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Allocation is over **dyadic blocks**: column counts are $$32\cdot 2^k$$, i.e. constraints in the multiplicative monoid generated by 2 (2-adic / dyadic structure in practice). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Warpgroup access partition is a **set partition / modular range constraint** on lane indices. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | Mentions “Tensor Memory on Blackwell” as a special memory unit required by some instructions/layouts, but does not provide the PTX-defined 512×128 structure, allocation unit (32 cols), or warp lane-access constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | ISL paper is about CuTe/Triton layout abstractions via integer set relations, not Blackwell TMEM allocation/addressing constraints. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Categorical paper is about CuTe layout algebra; it does not model Blackwell TMEM’s dynamic allocation, bitfield addressing, or warp-lane access restrictions. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `cp.async.bulk.tensor` | PTX defines `cp.async.bulk.tensor` as a **non-blocking** instruction initiating an **asynchronous tensor copy** between state spaces, parameterized by `.dim` (1D–5D), destination/source spaces (e.g., `.shared::cta`/`.shared::cluster` and `.global`), and (for global→shared) an **mbarrier completion mechanism** (`.mbarrier::complete_tx::bytes`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Instruction-form constraints:**<br>• `.dim ∈ {1d,2d,3d,4d,5d}`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• For `.dst=.shared::cta`, `dstMem` must be in the executing CTA’s shared memory (else **UB**). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Target ISA: requires `sm_90` or higher; some qualifiers (e.g., `.tile::gather4`, `.im2col::w`) require Blackwell-era targets (`sm_100*` etc) depending on dst state space. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Discrete/byte-quantized restrictions (examples for sub-byte/packed types):**<br>• Box-Size[0] forced to exact byte sizes (e.g., **64B** or **96B** depending on type). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Tensor-Size[0] must be a multiple of type-specific byte quanta (e.g., 48B/64B/96B). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• First tensor coordinate must satisfy a **multiple-of** constraint (e.g., multiple of 64 or 128, type/arch dependent). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Global address + per-dimension strides may require **16B/32B alignment** depending on type/arch. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Supported swizzle modes are **finite and restricted** for those packed types. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Bounds constraints (global→shared::cta direction):** starting coordinates non-negative; bounding box must lie within tensor boundaries. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Presburger + congruences + finite mode sets:**<br>• Bounding boxes/coords are integer **box constraints** in $$\mathbb{Z}^d$$ (inequalities). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Alignment and “multiple-of” conditions are **congruence constraints** mod $$2^k$$ (e.g., 16B/32B/64/128). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Swizzle is a **finite-choice parameter** with bit-level meaning (XOR/AND/shift expressibility). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Completion is asynchronous and mediated by `mbarrier`, i.e. not purely a functional/linear mapping. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | Paper is largely about representing layouts/swizzles and generating data movement code, but does not formalize PTX-level tensor-copy instruction admissibility constraints (Box-Size byte equalities, per-type coordinate congruences, arch-gated qualifiers). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | ISL paper models layout mappings/swizzles as relations; it does not treat PTX tensor-copy instruction forms (`cp.async.bulk.tensor`) or their per-type byte-quantization constraints. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Categorical paper does not address PTX tensor-copy instructions or their legality constraints. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `wgmma` / `mma` (layout requirements) | **`wgmma.mma_async` (Hopper)**: PTX specifies `.aligned` as mandatory: all threads in the **warpgroup** must execute the same `wgmma.mma_async`; conditional divergence yields **UB**. Target requirement: **`sm_90a`**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>PTX also specifies ordering hazards: `wgmma.fence` must be issued by all warps of the warpgroup at specific points (e.g., before first `wgmma.mma_async`), else **UB**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**`mma.sync.aligned` (warp-level MMA)**: PTX shows fixed tile shapes (e.g., `.m8n8k4`, `.m16n8k8`, etc.) and explicit major-ness qualifiers (e.g., `.row.col`), with feature/type availability gated by target ISA (e.g., `.f64` variants require `sm_90+`; some newer kinds require `sm_120a`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **`wgmma` constraints (control + ordering):**<br>• Warpgroup-uniform execution required (`.aligned`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Requires `sm_90a`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Requires explicit ordering via `wgmma.fence` at specified points; else UB. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**`mma` constraints (shape/layout/type):**<br>• Shapes are **finite, discrete** and encoded in the opcode suffix `.m…n…k…`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Major-ness qualifiers like `.row.col` appear in the instruction form (layout is not freely chosen at runtime). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Type/feature subsets are target-gated (e.g., some `.f64` mma shapes require `sm_90+`; some “kind/block_scale” require `sm_120a`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Finite combinatorics + GF(2) swizzles (two layers):**<br>• **Fragment distribution** is a combinatorial mapping over finite thread ID sets (warp = 32, warpgroup = 128), i.e. actions of finite sets/groups on tile fragments (not a continuous linear operator). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• **Swizzle for MMA-family memory layouts** is bit-level and well-modeled as GF(2)-linear/affine transforms on address bits; Paper 1 explicitly defines “mma swizzling” and connects it to bank conflicts. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | **Strong partial coverage.** Paper 1 explicitly references `mma` and `wgmma` as hardware primitives needing special layouts, and defines “mma swizzling” to mitigate bank conflicts; it does not encode PTX legality constraints like warpgroup-uniform `.aligned` execution or `wgmma.fence` ordering as part of the formal model. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | **Layout-math yes; instruction legality no.** Paper 2 formalizes CuTe swizzles and Triton GF(2) layouts in ISL, but does not incorporate warp/warpgroup control-flow uniformity, fence requirements, or target-specific availability conditions for `mma`/`wgmma`. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | **Abstract layout algebra.** Paper 3 provides categorical constructions for CuTe layout operations; it does not discuss instruction-level shape/type gating or synchronization rules for `mma`/`wgmma`. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `mbarrier` / async transaction barriers | PTX defines `mbarrier.*` operations on barrier objects in shared memory; they are used for **thread synchronization** and for **asynchronous operation completion waiting** (examples show pairing with async copies). Misuse is explicitly UB. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Object validity / placement:**<br>• Any `mbarrier` op except `mbarrier.init` on a location that does not contain a valid mbarrier object is **undefined behavior**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• If no state space specified, generic addressing; if `addr` is not within the required `.shared::cta` window (for those forms), behavior is **undefined**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**State token constraints:**<br>• `mbarrier.arrive` on `.shared::cta` returns a 64-bit opaque “state”; on `.shared::cluster` (but not `.shared::cta`), it **cannot return** a value (sink `_` is mandatory). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Some wait forms require that the `state` operand be produced by a prior arrive variant; otherwise **UB**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br><br>**Target constraints:**<br>• Introduced PTX ISA 7.0; base requirement `sm_80+`; `try_wait` and `.cluster` scope require `sm_90+`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | **Concurrency + discrete counters (not linear):**<br>• Semantics live in **weak memory / synchronization** space: partial orders (“happens-before”), release/acquire-like effects, and phase/state tokens. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))<br>• Operationally, this is well-modeled as a **finite-state machine with counters** (natural-number counts + parity/phase bits, i.e. $$\mathbb{N} \times \mathbb{Z}_2$$ structures). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)) | Layout-focused; does not model barrier object invariants, state tokens, or weak-memory ordering constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Layout-focused; does not model barrier semantics or instruction-level UB conditions around barrier misuse. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Category-theory layout algebra; does not model barriers or memory consistency. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| Bank Conflicts (Shared Memory / L1 rules) | CUDA programming docs describe shared memory as banked; bank conflicts arise when multiple addresses map to the same bank, causing serialization; broadcasts/multicasts are special cases. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/11.4.3/cuda-c-programming-guide/index.html)) | **Canonical bank model (modernized):**<br>• Shared memory has **32 banks**; successive **32-bit words map to successive banks** (banked address mapping). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/))<br>• Bank conflicts are defined by collisions in that mapping; hardware splits into multiple requests; broadcast/multicast exceptions exist. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/))<br><br>**Explicit conflict condition (stride model, from programming guide):**<br>For 32-bit strided access `shared[Base + s*tid]`, threads `tid` and `tid+n` hit the same bank whenever `s*n` is a multiple of 32; equivalently when `n` is a multiple of `32/d` where `d=gcd(32,s)`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/))<br><br>**Practical mitigation is modular:** padding changes stride from 32 to 33, and because bank index is computed modulo the number of banks, 33 ≡ 1 (mod 32), eliminating conflicts (best-practices example). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-c-best-practices-guide/index.html)) | **Modular arithmetic (primary):**<br>• Bank index is a function of address modulo 32 ⇒ arithmetic in $$\mathbb{Z}/32\mathbb{Z}$$. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/))<br>• Conflict degree is a **multiplicity / partition-count** statistic of a mapping $$\{0..31\}\to\{0..31\}$$ (threads→banks). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-c-best-practices-guide/index.html))<br>• The stride condition uses **gcd** and divisibility, i.e. number theory over $$\mathbb{Z}$$ with reduction mod 32. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/)) | **Explicitly modeled.** Paper 1 gives a linear-algebraic treatment of bank conflicts, including vectorization sets and criteria for conflict-free access, and discusses transaction splitting (e.g., >128B splits). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | **Swizzle modeled; bank-conflict hardware mostly not.** Paper 2 defines swizzle mappings and relates them to memory access optimization, but does not provide the explicit bank-index modulo/gcd constraints as a hardware law in the excerpted definitions. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Does not address bank conflict mechanics; focuses on categorical layout operations. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |

---

## Critical Mismatches (Constraint Cliffs)

1) **“Arbitrary swizzle in GF(2)” vs “finite vendor swizzle modes + byte alignment.”**  
   Paper 2 defines swizzles as bit-level maps using XOR/AND/shift (a large mathematical space). ([arxiv.org](https://arxiv.org/html/2511.10374v1))  
   Hardware-facing APIs expose only a **small enumerated set** of swizzles (e.g., NONE/32B/64B/128B/128B_ATOM_32B) with **hard inner-dimension byte limits** and other datatype restrictions. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))

2) **Layout algebra over unbounded integers vs descriptor legality as mixed inequalities + congruences.**  
   Tensor-map encoding imposes “multiple-of-16/32,” “≤ 2^32,” and “boxDim ≤ 256” constraints that are fundamentally **modular/congruence** plus **bounded integer** constraints. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))  
   The seed papers focus on representing/composing mappings, not on satisfiability of these concrete mixed constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

3) **“Tensor copy” as a conceptual operation vs PTX legality constraints with exact byte equalities.**  
   `cp.async.bulk.tensor` has per-type rules like **Box-Size[0] must be exactly 64B/96B**, and coordinate/alignment congruences for packed sub-byte formats. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))  
   None of the three papers encodes these exact byte-equality admissibility constraints as part of the theory. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

4) **Functional layout mapping vs warpgroup-wide control-flow constraints (`wgmma`).**  
   PTX requires all threads in a warpgroup to execute the same `wgmma.mma_async` (`.aligned`), otherwise **undefined behavior**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))  
   This is a **global predicate / uniformity constraint** on control flow, not a property of a coordinate→index mapping; papers focusing on layout morphisms do not model this legality rule. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

5) **Purely algebraic composition vs explicit ordering/fence requirements (`wgmma.fence`).**  
   PTX specifies that missing `wgmma.fence` at required points yields undefined behavior. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))  
   Seed papers treat layout conversion correctness as algebraic/relational correctness, not as a weak-memory ordering problem. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1))

6) **Static memory-as-a-set vs TMEM as a dynamically allocated, blocking, power-of-two resource.**  
   Tensor Memory allocation is dynamic; unit is **32 columns**, requested amount must be a **power of two**, and allocation may block. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))  
   Layout algebras (all three papers) treat “memory layouts” as mappings, not as a resource-allocation protocol with blocking behavior. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

7) **Layouts ignore object-invariant UB conditions vs `mbarrier` object validity as a first-class constraint.**  
   PTX makes `mbarrier` misuse explicitly UB (e.g., operating on an uninitialized location). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html))  
   The seed papers do not treat synchronization-object invariants or state-token provenance constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

8) **“Bank conflicts = linear/bilinear objective” vs bank rules containing special-case semantics (broadcast/multicast) and modulo/gcd structure.**  
   Paper 1 models bank conflicts and does include a strong linear-algebraic treatment. ([arxiv.org](https://arxiv.org/html/2505.23819v3))  
   CUDA docs define bank conflicts with explicit modulo/gcd conditions and broadcast/multicast exceptions, which are **piecewise** and **case-dependent** rather than a single linear objective. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/))

---
Learn more:
1. [https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html](https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html)
2. [https://docs.nvidia.cn/cuda/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.cn/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
3. [https://docs.nvidia.com/cuda/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)
4. [https://arxiv.org/html/2505.23819v3](https://arxiv.org/html/2505.23819v3)
5. [https://arxiv.org/html/2511.10374v1](https://arxiv.org/html/2511.10374v1)
6. [https://arxiv.org/pdf/2601.05972v1](https://arxiv.org/pdf/2601.05972v1)
7. [https://docs.nvidia.com/cuda/archive/12.8.0/blackwell-tuning-guide/index.html](https://docs.nvidia.com/cuda/archive/12.8.0/blackwell-tuning-guide/index.html)
8. [https://docs.nvidia.com/cuda/archive/11.4.3/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/archive/11.4.3/cuda-c-programming-guide/index.html)
9. [https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/](https://docs.nvidia.com/cuda/archive/8.0/cuda-c-programming-guide/)
10. [https://docs.nvidia.com/cuda/archive/12.4.0/cuda-c-best-practices-guide/index.html](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-c-best-practices-guide/index.html)
