context_pack_version: "S1.v2"
generated_at_utc: "2026-02-05T18:12:08Z"
project_profile:
  objective: "Develop a theory-first formal model to evaluate and solve Blackwell GPU architecture problems under CUDA>13 and PTX>9; output a 9-part academic LaTeX proposal."
  hard_constraints:
    architecture: "NVIDIA Blackwell (Grace-Blackwell allowed when relevant)"
    cuda: "> 13.0 (prefer 13.1+)"
    ptx: "> 9.0 (strict)"
    required_sources_each_run: [ARCH_BW, OPT_PIPE, NV_BLOG_TILE, NV_workloads, SEED_1, SEED_2, SEED_3]
  writing_constraints:
    latex_style: "academic, math-first"
    output_split: ">= 9 LaTeX parts; each LaTeX run emits exactly 1 part"
    citation_policy: "no uncited non-trivial claims; mark UNVERIFIED if not in sources"
  scope_boundaries:
    include:
      - "formal semantics/models"
      - "layout algebra"
      - "integer relations"
      - "constraint optimization (SMT/ILP)"
      - "cache/traffic modeling"
      - "TMEM/TMA/tile IR as modeled objects"
      - "scheduling (SWP/WS/CTA)"
    exclude:
      - "marketing-only claims without evidence"
      - "non-Blackwell architectures except as explicit comparison or modeling baseline"
golden_sources:
  ARCH_BW: {url: "https://arxiv.org/html/2512.02189v1", tier: "tier_1_insight"}
  OPT_PIPE: {url: "https://arxiv.org/html/2512.18134v1", tier: "tier_1_insight"}
  NV_BLOG_TILE:
    url: "https://developer.nvidia.com/blog/advancing-gpu-programming-with-the-cuda-tile-ir-backend-for-openai-triton/"
    tier: "tier_1_insight"
  NV_workloads: {url: "https://arxiv.org/html/2502.13113", tier: "tier_2_modeling_context"}
  SEED_1: {url: "https://arxiv.org/html/2505.23819v3", tier: "tier_4_context"}
  SEED_2: {url: "https://arxiv.org/html/2511.10374v1", tier: "tier_4_context"}
  SEED_3: {url: "https://arxiv.org/html/2601.16032v2", tier: "tier_4_context"}
source_audit:
  ARCH_BW:
    used_for: "Empirical Blackwell (B200) microarchitectural facts and measurement methodology for TMEM, DE, tcgen05 Tensor Core pipeline; also software-ecosystem constraints (CUDA/TMEM/CTA, FP6 tooling)."
    anchors:
      - "III Blackwell Architecture (III-A Blackwell Architecture)"
      - "IV PTX-Microbenchmark Methodology (IV-A Novel Benchmark Design for Blackwell-Specific Features)"
      - "V Memory Subsystem (V-A Tensor Memory (TMEM); V-B Decompression Engine (DE))"
      - "VI GPU Cores Microarchitecture (VI-A Fifth-Generation Tensor Cores)"
      - "Table V (wgmma vs tcgen05.mma latency)"
      - "VII Performance Analysis & Case Studies (VII-B1 Precision Mode Impact; VII-C2 Sustained Memory Bandwidth; VII-C3 Sparse Operations)"
      - "VIII Discussion (Software Ecosystem)"
      - "IX Conclusion"
  OPT_PIPE:
    used_for: "Formal constraint system for joint software pipelining (modulo scheduling) + warp specialization, including memory/register/liveness constraints and handling unsatisfiability; Blackwell-specific scheduling observations for FMHA; toolchain note (CUDA 13.0)."
    anchors:
      - "3.1 Modulo Scheduling (dependence graph; initiation interval; modulo schedule)"
      - "4 Joint Optimization Problem"
      - "Figure 4 (Constraints enforcing a valid modulo schedule)"
      - "Figure 5 (Memory Allocation Constraints; SSA + liveness)"
      - "Figure 6 (Warp Assignment Constraints: variable latency, register limit, cross-warp spills, concurrency)"
      - "Algorithm 1 (Twill’s Search Procedure)"
      - "5.2 Cost Normalization"
      - "5.3 Variable Latency Optimizations (streaming operations)"
      - "5.4 Limitations and Future Work"
      - "6.1 Methodology (Evaluation Platforms: H100, B200; CUDA 13.0)"
      - "6.2.2 Blackwell (FMHA forward)"
      - "Figure 9 (Blackwell WS strategy visualization)"
  NV_BLOG_TILE:
    used_for: "CUDA Tile IR backend constraints and operational semantics at the API/IR boundary: prerequisites (CUDA 13.1+, Blackwell GPUs), compilation mode, known limitations (unsupported ops; tensor-of-pointer degradation), and the descriptor/TMA data-movement pattern."
    anchors:
      - "Jan 30, 2026 post header"
      - "How to use Triton-to-TileIR (Prerequisites: CUDA 13.1+; Blackwell GPUs)"
      - "Verify Tile IR compilation (.tileIR cache artifacts; ENABLE_TILE=1)"
      - "Limitations of Triton-to-TileIR"
      - "Unsupported operations"
      - "Tensor-of-pointer degradation suboptimal performance (CUDA 13.1)"
      - "TMA load/store API example (tl.make_tensor_descriptor; desc.load/store)"
  NV_workloads:
    used_for: "Workload-structure and mapping/modeling context: taxonomy for heterogeneous/hierarchical processors, reuse-based partitioning, Timeloop-based evaluation pipeline, and sensitivity to bandwidth partitioning (useful for Blackwell workload models as context, not as Blackwell-specific facts)."
    anchors:
      - "II-A Mapping"
      - "II-B Mixed-reuse Workloads"
      - "IV Harp Taxonomy (Classification of HHP’s; IV-B Describing Existing Works; Table I)"
      - "VI-A Evaluation Framework: Timeloop for Harp (Fig. 5)"
      - "VII Results (VII-A Performance; VII-F Summary of Key Trends)"
      - "IX Conclusion"
  SEED_1:
    used_for: "Primary layout algebra formalism (linear layouts over F2) including closure/completeness under shape ops, layout conversions, and bank-conflict modeling + optimal swizzling; provides math objects/operators for modeling tile IR, TMA descriptors, and Blackwell TMEM/SMEM layouts."
    anchors:
      - "Definition 4.1 (Linear Layouts)"
      - "Definitions 4.2–4.5 (Composition; Product; Left Division; Right Inverse)"
      - "Theorem 4.9 (Every distributed layout is a linear layout)"
      - "Definition 4.10 (Distributed Layout)"
      - "Definition 4.11 (mma swizzling) + Proposition 4.12"
      - "Theorem 4.13 + Definition 4.14 (Memory Layout)"
      - "4.4 Closure Under Triton Operations (Triton’s Layout Engine; Propagation Through Shape Operations)"
      - "Theorem 5.1 (SIMD primitive applicability via left division)"
      - "Appendix: Theorem 9.3 (closure + minimality), Lemma 9.4 (bank-conflict wavefronts), 9.2 Optimal Swizzling"
      - "Limitations paragraph (power-of-two shapes; affine layouts extension)"
  SEED_2:
    used_for: "Alternative unifying formalism: modeling CuTe layouts, swizzles, and Triton linear layouts as ISL integer set relations (incl. quasi-affine constructs); provides algorithmic reconstruction procedures and explicit open problems for layout inference/verification."
    anchors:
      - "2.1 CuTe Layout (integral/natural coordinate spaces; coordinate mapping; index mapping; layout mapping)"
      - "2.1.1 Hierarchical Layouts (shape compatibility)"
      - "2.1.2 Layout Operations (composition, inverse, complement; divisions/products)"
      - "2.2 CuTe Swizzle (bit ops; shift/XOR/AND)"
      - "2.3 Linear Layout (binary vector spaces; basis vectors; power-of-two note)"
      - "2.4 Polyhedral Model & ISL (Integer Set Relations; ISL Operations; Quasi-Affine Extensions)"
      - "3.1 Coordinate and Index Spaces (integer sets/polyhedra)"
      - "Algorithm: get_layout_strictly_affine (construct layout from index mapping + shape) + correctness proof"
      - "Complement-construction discussion (lexicographically smallest gap; Algorithm layout-complement)"
      - "Open question: infer both shape and strides from layout mapping alone"
      - "Complexity subsection"
  SEED_3:
    used_for: "Cache/traffic modeling and scheduling-level locality optimization on Grace-Blackwell (GB10): explicit L2 sector-access model variables/formula, persistent vs non-persistent CTA scheduling analysis, and Sawtooth traversal algorithm; includes CuTile validation + limitations."
    anchors:
      - "2.1 GPU Memory Hierarchy (L1Tex partitioning; shared-memory emphasis)"
      - "NVIDIA GB10 (Grace Blackwell chip description)"
      - "CTA Scheduling and Work Distribution (persistent CTA pattern; round-robin assignment)"
      - "Algorithm 3 (Non-Persistent Scheduling)"
      - "3.2 Modeling L2 Sector Access (variables; approximation formula)"
      - "Algorithm 4 (Sawtooth KV Access Pattern)"
      - "4.2 CUDA Results (Figures 7–8)"
      - "4.3 Validation on CuTile (Results; Figures 9–12; Limitations)"
formalism_index:
  ARCH_BW:
    definitions:
      - "Blackwell B200 GPU characterization target (dual-die, unified device view) as the modeled hardware instance."
      - "PTX microbenchmark: PTX kernels used as controlled probes of microarchitectural behaviors (PTX→SASS validated)."
      - "Pointer-chase benchmark: dependent memory access pattern to isolate latency of a memory tier."
      - "TMEM: dedicated on-chip tensor memory tier per SM for Tensor Core operations (with explicit addressing/structure as described)."
      - "DE: on-GPU hardware decompression engine; characterized by throughput/latency and pipeline depth (concurrency)."
      - "tcgen05.* family: new PTX instruction family for TMEM data movement and tcgen05.mma Tensor Core ops."
      - "Pipeline depth / saturation: concurrency level at which DE throughput reaches an efficiency threshold (reported via tables)."
    operators_or_constructions:
      - "Isolation construction: enforce dependency chains to prevent ILP overlap when measuring latency (pointer-chase)."
      - "Comparative construction: measure TMEM vs shared/global memory baselines under controlled access patterns."
      - "Sweep construction: vary operand sizes/strides to observe bandwidth saturation and per-access latency regimes."
      - "Translation audit: document PTX-to-SASS lowering to ensure the intended hardware path is exercised."
    theorems_or_propositions: []
    proof_techniques:
      - "Empirical validation via controlled microbenchmarks + reproducible methodology (warmup/iterations)."
      - "Cross-architecture comparison (B200 vs H200/Hopper) under matched workloads/metrics."
      - "Use of tail-latency statistics (P95/P99) and measured bandwidth/throughput to triangulate bottlenecks."
    algorithmic_artifacts:
      - "Microbenchmark suite covering TMEM latency/bandwidth and tcgen05 instruction behavior."
      - "DE benchmark suite across multiple compression formats and chunk sizes; pipeline-depth study."
      - "Workload case-study benchmarks (LLM inference; scientific kernels; mixed-precision training)."
    modeling_assumptions:
      - "Microbenchmarks assume the PTX-to-SASS mapping used during measurement is stable; compiler changes may alter instruction selection (UNVERIFIED across CUDA>13.0/PTX>9.0)."
      - "Pointer-chase assumes dependence prevents hiding latency by overlap; measures end-to-end access in cache-miss regimes."
      - "Workload decomposition attributes speedups to specific features (TMEM, CTA pairing, Tensor Cores); attribution depends on the profiling methodology described."
  OPT_PIPE:
    definitions:
      - "Loop dependence graph: G=(V,E) with V as loop instructions/ops and E as dependence edges between ops; edges carry cycle-delay and iteration-delay components."
      - "Initiation interval (II): throughput-controlling period for overlapped loop iterations in modulo scheduling."
      - "Modulo schedule: mapping sigma: V -> time slots (mod II) for issue times per iteration."
      - "Straight-line unrolled schedule Q (prologue + one steady-state + epilogue) used as the constraint-program substrate."
      - "Boolean schedule tensor op[op,iter,time] indicating whether a specific op instance is scheduled at a given time in Q."
      - "SSA liveness boolean live[val_instance,time] encoding whether a produced value is live at time in Q."
      - "Warp assignment boolean opw[op,warp] encoding which warp executes each op; induces warp assignment W."
      - "Blocking edges / synchronization edges: designated dependences requiring blocking synchronization, used in concurrency constraints."
      - "Cross-warp spill cost: annotated communication cost when consumer warp differs from producer warp (via shared-memory transfer)."
    operators_or_constructions:
      - "Modulo scheduling construction to seed II and an initial schedule; re-expressed as SMT constraints to allow schedule modification while preserving II/length."
      - "Constraint-system conjunction: (schedule feasibility) ∧ (memory capacity via liveness) ∧ (warp-assignment feasibility) ∧ (spill/sync concurrency)."
      - "Monotone search over II and total schedule length L to handle SMT unsatisfiability while preserving optimality guarantee."
      - "Cost normalization operator: replace raw cycle-count list c by normalized integer costs c' that preserve ratios approximately under an LP objective."
      - "Streaming-operation abstraction: treat variable-latency ops with no incoming deps as zero-latency in the static model; expose pipeline depth as tunable parameter."
    theorems_or_propositions:
      - "Claimed optimality (for the supported program class) of joint SWP+WS schedules w.r.t. the expressed constraint system (requires satisfiable constraints and monotone II search)."
    proof_techniques:
      - "Reduction to LP (initial modulo schedule) + SMT solving in QF linear integer arithmetic (QFLIA)."
      - "Correctness via invariants: uniqueness/consistency/completion/dependence/capacity constraints induce valid modulo schedule."
      - "Classic backwards dataflow reasoning encoded as constraints for liveness (propagation of live/dead)."
      - "Unsatisfiability-driven refinement: increase II/L until satisfiable, yielding maximal-throughput feasible schedule."
    algorithmic_artifacts:
      - "Algorithm 1: Twill’s search procedure over II and schedule length L with LP+SMT loop."
      - "Constraint templates: Figure 4 (schedule), Figure 5 (memory/liveness), Figure 6 (warp assignment + spills + concurrency)."
      - "Implementation pipeline: extract dependence graphs from TTGIR (tile-based SSA IR), solve LP with CBC, SMT with Yices2, normalization LP with SCIP."
      - "Variable-latency optimization: identify streaming operations and externalize pipeline depth as tuner parameter."
    modeling_assumptions:
      - "Program class restriction: singly-nested loops without additional control flow."
      - "Dependence graph extraction from TTGIR is assumed accurate; TTGIR is tile-based SSA IR with explicit data movement."
      - "Machine model provides functional-unit capacities, memory footprints/capacities, and blocking/synchronization annotations."
      - "Cross-warp spill costs are provided as node annotations; not derived automatically."
      - "Optimality is only with respect to the specified constraints and the chosen tile size (tile size not auto-selected)."
  NV_BLOG_TILE:
    definitions:
      - "Triton-to-TileIR backend: source-based compilation pipeline producing Tile IR artifacts for Triton kernels when enabled."
      - "CUDA Tile IR backend: compiler backend targeting NVIDIA CUDA Tile IR virtual instruction set (as described)."
      - "Tile IR cache artifact: compiled kernel cached with .tileIR extension (distinct from .cubin in SIMT backend)."
      - "Tensor-of-pointer pattern: Triton pattern where a tensor is materialized as a tensor of elementwise pointers to describe memory accesses."
      - "Tensor descriptor: (base pointer, shape, strides, block_shape) object created by tl.make_tensor_descriptor for TMA-backed tile loads/stores."
      - "TMA load/store API: descriptor-based tile transfer interface used to avoid in-kernel pointer materialization."
    operators_or_constructions:
      - "Backend-selection operator: ENABLE_TILE=1 switches compilation to Tile IR backend and changes cache artifact type."
      - "Descriptor construction: tl.make_tensor_descriptor(base, shape, strides, block_shape) to represent a tiled layout."
      - "Descriptor tile load/store: desc.load(offsets), desc.store(offsets, value) to express tile movement."
      - "Mitigation rewrite: replace tensor-of-pointers computation with descriptor + TMA load/store using layout parameters."
    theorems_or_propositions: []
    proof_techniques:
      - "N/A (engineering blog); relies on stated prerequisites, limitations, and illustrative code patterns."
    algorithmic_artifacts:
      - "Build-from-source procedure for Triton-to-TileIR (no prebuilt binaries)."
      - "Verification workflow: run tutorial with Tile IR enabled; check cache for .tileIR artifacts."
      - "Rewrite pattern example for migrating from tensor-of-pointers to TMA-backed descriptors."
    modeling_assumptions:
      - "Backend is in early development with incomplete op coverage; performance/compatibility expected to evolve with CUDA releases."
      - "Descriptor/TMA performance benefit assumes tiles are contiguous with well-defined shape/strides/block_shape."
      - "Claims are specific to CUDA 13.1 context where tensor-of-pointer degradation is reported."
  NV_workloads:
    definitions:
      - "Mapping: a set of loop transformations (permutation, parallelization, tiling) scheduling a workload on a spatial accelerator."
      - "Mixed-reuse workloads: cascades containing both high arithmetic-intensity and low arithmetic-intensity ops with dependencies limiting overlap."
      - "Hierarchical/heterogeneous hierarchical processors (HHPs): architectures with compute positioned at multiple memory levels and/or heterogeneous sub-accelerators."
      - "Harp taxonomy: classification system for HHPs along axes of compute placement in memory hierarchy and heterogeneity location (intra-node, cross-node, cross-depth, compound)."
      - "Reuse-based operation allocation: assign ops to sub-accelerators based on reuse class; combine op stats to evaluate cascade."
      - "Resource partitioning: split shared resources (e.g., bandwidth, on-chip buffers like LLB) across sub-accelerators by policy."
      - "Timeloop-based evaluation framework: modified Timeloop + wrapper to model per-op mapping and aggregate cascade metrics."
    operators_or_constructions:
      - "Taxonomy encoding -> architecture-file generation for each sub-accelerator."
      - "Operation-to-sub-accelerator allocation based on reuse; mapping constraints passed to Timeloop mapper."
      - "Aggregation operator: wrapper combines per-op sub-accelerator stats into cascade-level performance/energy metrics."
      - "Sensitivity operator: vary bandwidth partitioning to study trend shifts across workloads."
    theorems_or_propositions: []
    proof_techniques:
      - "Empirical/simulation-based evaluation using Timeloop mappings across architecture design points."
      - "Trend analysis across workload classes (encoder-only vs decoder-only transformers) and resource-partitioning regimes."
    algorithmic_artifacts:
      - "Modified Timeloop flow enabling blackbox mapping on hierarchical/heterogeneous sub-accelerators."
      - "Wrapper tool to compose cascade results from per-op Timeloop outputs."
      - "Taxonomy-driven generation of architecture configuration files with mapping constraints."
    modeling_assumptions:
      - "Results depend on chosen hardware parameterization and mapping constraints in Timeloop v0.4."
      - "Reuse-based partitioning approximates workload structure; overlap potential constrained by dependency graph."
      - "Area-normalized comparisons assume fixed total resources; partitioning changes per-sub-accelerator roofs."
  SEED_1:
    definitions:
      - "Binary field F2 and vector spaces over F2; linear algebra operations interpreted as XOR (addition) and AND (multiplication) at the bit level."
      - "Labeled vector spaces: input/output bit dimensions are labeled by hardware-resource axes (register/thread/warp) and logical tensor axes."
      - "Linear layout: a linear map between labeled vector spaces over F2 (Definition 4.1)."
      - "Distributed layout: a surjective linear layout from (registers, threads, warps) into a logical tensor with permutation-matrix structure + optional zero columns (Definition 4.10)."
      - "Memory layout: an invertible linear layout mapping offsets to logical-tensor coordinates with restricted column structure (Definition 4.14)."
      - "mma swizzling: parameterized mapping from element coordinate to offset (Definition 4.11) used to mitigate bank conflicts for MMA-family layouts."
      - "Tile (instruction tile): layout fragment required by a SIMD primitive; compatibility test via left division."
      - "Shared-memory bank-conflict model: linear map decomposing shared memory into (Vec, Bank, Idx) components; wavefront count derived from subspace intersections (Lemma 9.4)."
    operators_or_constructions:
      - "Composition of linear layouts via matrix multiplication over F2 (Definition 4.2)."
      - "Product/direct-sum of layouts via block-diagonal matrix construction (Definition 4.3)."
      - "Left division operator (matrix decomposition) used to check if a layout can be decomposed into an instruction tile + remainder (Definition 4.4)."
      - "Right inverse construction for surjective layouts via Gaussian elimination over F2 (Definition 4.5)."
      - "Layout propagation through Triton shape ops (tt.trans/reshape/join/split/expand_dims/broadcast) via closure property (Theorem 9.3)."
      - "Optimal swizzling construction to maximize vectorization while minimizing bank conflicts (Appendix 9.2)."
    theorems_or_propositions:
      - "Theorem 4.9: Every distributed layout is a linear layout."
      - "Proposition 4.12: MMA swizzled layouts are linear layouts."
      - "Theorem 4.13: Every memory layout is a linear layout."
      - "Theorem 5.1: A SIMD instruction with tile T can lower a layout L if left-division condition exists."
      - "Theorem 9.3: Distributed-layout family is forward/backward closed under Triton shape ops and is minimal."
      - "Lemma 9.4: Bank-conflict wavefront lower bound/exactness in terms of layout subspace intersections."
    proof_techniques:
      - "Linearity arguments over F2: show constructions are linear by closure of linear maps under composition/product."
      - "Constructive proofs: explicitly construct layout matrices for blocked, mma, wgmma layouts (Appendix 9.1)."
      - "Algebraic solvability: compute right inverses via Gaussian elimination over F2."
      - "Minimality proof for closure family via generating all {0,1}-column layouts through shape ops + reductions (Theorem 9.3)."
      - "Subspace-intersection reasoning for bank-conflict lower bounds and vectorization regimes (Lemma 9.4)."
    algorithmic_artifacts:
      - "Triton layout engine procedure: forward pass propagation + heuristic conflict resolution + inserted conversions; backward pass rematerialization to reduce conversions."
      - "Bank-conflict optimal swizzle algorithm (Appendix 9.2) producing swizzled layout matrices."
      - "Layout conversion codegen: compute mapping between layouts via composition and right-inverse selection (Section 5.4)."
      - "SIMD applicability checks using left division for vectorized ld/st and ldmatrix/stmatrix tiles (Section 5.3)."
    modeling_assumptions:
      - "Primary limitation: linear layouts restrict to power-of-two shapes; mitigated by masking/padding (stated limitation)."
      - "Flipping/slicing not expressible as linear layouts; extension to affine layouts proposed (stated limitation/future direction)."
      - "Bank-conflict model assumes a shared-memory bank decomposition and vectorization parameterization; specific numeric bank counts are treated as 'on modern GPUs' (needs chip-specific calibration)."
  SEED_2:
    definitions:
      - "Integer set relation: mapping between multi-dimensional integer spaces specified by conjunctions of affine/quasi-affine constraints."
      - "CuTe layout H = (shape s : strides d): maps n-D coordinate space to 1-D index space; decomposed into coordinate mapping + index mapping + composition to layout mapping."
      - "Integral coordinate space (1-D) and natural coordinate space (n-D) associated with shape; coordinate mapping defined by (co)lex ordering."
      - "Hierarchical layouts: nested shapes/strides; define multiple compatible natural coordinate spaces for same overall mapping."
      - "CuTe swizzle: bijective bit-manipulation mapping on an interval parameterized by (bits, base, shift)."
      - "Triton linear layout: mapping between binary coordinate/index vector spaces over F2 specified via basis-vector images and XOR/AND operations."
      - "ISL operations: union/intersection/difference for sets; composition/inverse/domain/range for relations."
      - "Quasi-affine constraints: constraints involving floor/ceiling/modulo enabling representation of tiling and other non-strictly-affine maps."
      - "Layout size |H| as domain size; co-size ||H|| as lexicographic max of range plus 1 (as defined in text)."
    operators_or_constructions:
      - "Represent coordinate mappings (mod/floor) as quasi-affine integer set relations."
      - "Represent layout mapping as relation composition: M_L = M_I ∘ M_C^{-1} (and with swizzle mapping when present)."
      - "Express CuTe operations (composition/inverse/complement) using ISL relation operations."
      - "Swizzle mapping construction: derive a binary swizzle mapping relation implementing XOR/shift/mask formula; show involution property for swizzle mapping."
      - "Layout reconstruction from relations: Algorithm get_layout_strictly_affine (index mapping + shape) and get_layout_from_strides (layout mapping + strides)."
      - "Complement layout construction: iteratively fill lexicographically smallest gaps in range interval via relation difference + lexicographic minimum."
    theorems_or_propositions:
      - "Swizzle mapping and composed layout mapping are involutions for the described CuTe swizzle construction (stated in text)."
      - "Algorithm-correctness proofs provided for swizzle-mapping algorithm and for reconstruction algorithms (as paragraphs following algorithms)."
    proof_techniques:
      - "Polyhedral/ISL reasoning: correctness by structural decomposition into coordinate/swizzle/index relations and their compositions."
      - "Bit-level reasoning in binary space: masks isolate XORed bits; modulo-2 arithmetic implements XOR."
      - "Algorithmic correctness via extracting equality constraints from strictly affine relations (stride recovery)."
    algorithmic_artifacts:
      - "Algorithm: Construct Layout from Strictly Affine Index Mapping and Shape (get_layout_strictly_affine)."
      - "Algorithm: Construct Layout from Layout Mapping and Strides (get_layout_from_strides)."
      - "Algorithm 1: derive binary swizzle mapping for CuTe swizzle (accounts for positive/negative shift)."
      - "Algorithm: layout complement construction (layout-complement) described via gap-filling procedure."
    modeling_assumptions:
      - "Linear-layout binary-space representation assumes power-of-two dimension sizes (explicitly stated)."
      - "Translation to integer relations is compile-time (not runtime); uses ISL/ISLpy as implementation substrate."
      - "Coordinate mapping depends on lexicographic vs colexicographic conventions (explicitly noted when comparing CuTe vs linear layouts)."
      - "Worst-case exponential complexity of relation operations is acknowledged but argued to be practical in ISL tooling."
  SEED_3:
    definitions:
      - "GB10 as experimental platform: Grace-Blackwell chip (Blackwell GPU + ARM CPU) used for cache/scheduling experiments."
      - "Memory hierarchy model: global memory → L2 cache → per-SM L1Tex (partitionable into L1 cache + shared memory)."
      - "Flash Attention tiling kernel structure: resident Q tile in shared memory; stream K/V tiles; compute via WMMA; online softmax updates."
      - "CTA scheduling modes: persistent CTA (round-robin tile assignment) vs non-persistent (classic grid launch) scheduling."
      - "L2 sector access model: variables (sequence length, sector size, element size, tile size, head dimension, number of sectors) and an approximate formula for total sector accesses."
      - "Non-compulsory L2 miss concept: misses induced by reuse distance/eviction rather than first-touch (as used in analysis)."
      - "Sawtooth wavefront reordering: alternating forward/backward KV traversal order per query-tile parity to reduce reuse distance."
    operators_or_constructions:
      - "Metric construction: use Nsight Compute counters (L1/L2 sectors, L1 hit count) to determine filtering behavior."
      - "Model construction: approximate L2 sector accesses by counting per-tile sector counts under simplifying assumptions (ignoring trailing effects)."
      - "Traversal-order transformation: replace cyclic KV traversal with sawtooth alternating scan pattern (Algorithm 4)."
      - "Validation transfer: port optimization to CuTile; compare static vs tile-based implementations under same tile/shape parameters."
    theorems_or_propositions: []
    proof_techniques:
      - "Model validation against hardware counters: compare predicted sector-access patterns with measured L1/L2 sector counts."
      - "Ablation across scheduling mechanisms (persistent vs non-persistent) to isolate scheduling vs access-pattern effects."
      - "Cross-implementation validation: CUDA baseline vs CuTile variants."
    algorithmic_artifacts:
      - "Algorithm 3: Non-Persistent Scheduling pseudocode."
      - "Algorithm 4: Sawtooth KV Access Pattern (alternating scan) pseudocode."
      - "CuTile implementation variants: fully static vs tile-based local step-2 advancement to realize sawtooth pattern."
    modeling_assumptions:
      - "L2 sector model initially assumes single-batch single-head; batch/head treated as linear scaling factors."
      - "Approximation ignores trailing incomplete-tile effects (explicitly stated)."
      - "Conclusions about scheduling mechanism assume full GPU saturation (SM=48) in experiments."
evidence_index:
  ARCH_BW:
    key_claims:
      - claim: "B200 Blackwell is described as a dual-die configuration unified by NV-HBI, presented to software as a coherent single device with a unified HBM3e memory space."
        support: "Sec. III Blackwell Architecture / III-A (dual-die; NV-HBI; unified memory space)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Chip-specific (B200 datacenter)."
      - claim: "TMEM is reported as a dedicated 256KB on-chip memory per SM for Tensor Core operations, with a lane-column addressing structure (2D array description)."
        support: "Sec. V-A Tensor Memory (TMEM) (TMEM size/structure and purpose)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Provides modeled object for TMEM tier."
      - claim: "TMEM cache-miss end-to-end access latency is measured at ~420 cycles, described as a 58% reduction vs Hopper’s ~1000-cycle global-memory latency."
        support: "Sec. V-A Tensor Memory (TMEM) + referenced Table II (latency characterizatiotext)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Empirical microbenchmark result; compiler/toolchain dependence beyond reported setup is UNVERIFIED."
      - claim: "TMEM bandwidth is reported as 16 TB/s read and 8 TB/s write per SM, operating additively with L1/SMEM bandwidth rather than competing for the same resources."
        support: "Sec. V-A Tensor Memory (TMEM) (bandwidth figures and additive statement)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Empirical characterization; interpret as per-SM on-chip path capability in measured conditions."
      - claim: "Blackwell introduces tcgen05.mma (warp-level) Tensor Core PTX instruction; measured single-instruction latency is ~11 cycles and nearly constant across tile sizes, compared against Hopper wgmma latencies."
        support: "Sec. VI-A Fifth-Generation Tensor Cores + Table V (wgmma vs tcgen05.mma latency)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Useful for machine-model latency parameters in schedule/SMT formulations."
      - claim: "The paper reports that CUDA 13.0 provides preliminary TMEM/CTA support, while FP6 hardware support exists but lacks software tooling."
        support: "Sec. VIII Discussion (Software Ecosystem sentence)."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Toolchain statement; project targets CUDA>13.0, so further validation under CUDA 13.1+ remains needed."
      - claim: "DE characterization uses microbenchmarks over seven compression formats and reports chunk-size-dependent pipeline depth at an 85% efficiency threshold."
        support: "Sec. IV-A2 Decompression Engine Characterization + Table III (pipeline depth characteristics)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Modeling hook: DE throughput as a function of chunk size and concurrency."
      - claim: "LLM inference case study reports throughput/quality tradeoffs across FP16/FP8/FP4 (including perplexity deltas) under a specified batch/sequence configuration."
        support: "Sec. VII-B1 Precision Mode Impact + Table VIII (precision modes and perplexity deltas)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Precision-policy constraints can be treated as QoS constraints in optimization formulations."
      - claim: "Sparse SpMV case study reports consistent speedups attributed to hardware decompression, with stated traffic reduction and small latency overhead."
        support: "Sec. VII-C3 Sparse Operations + Table XIV (SpMV with hardware decompression; traffic/overhead statement)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Enables modeling of DE as a traffic-reduction transform in memory-traffic models."
    explicit_limitations_or_open_questions:
      - "PTX version used/required by the PTX microbenchmarks and tcgen05 instructions is not explicitly stated; compatibility with the project constraint PTX>9.0 is UNVERIFIED."
      - "Software ecosystem integration for TMEM/CTA is described as preliminary/ongoing (CUDA 13.0 mentioned); behavior under CUDA 13.1+ must be re-validated for this project."
      - "FP6 hardware support is stated but software tooling is lacking, limiting practical experimentation and calibration for FP6."
      - "Several performance attributions rely on measured/profiling methodology; portability of numeric results to other Blackwell SKUs (e.g., GB10) is UNVERIFIED."
  OPT_PIPE:
    key_claims:
      - claim: "The joint optimization problem is defined as producing a modulo schedule with minimum initiation interval together with a warp assignment capable of realizing it; solved via a unified constraint system."
        support: "Sec. 4 Joint Optimization Problem (definition of outputs: schedule, II, warp assignment)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Core formal scaffold for a theory-first scheduler for Blackwell."
      - claim: "Twill re-expresses modulo scheduling as SMT constraints over a 3-D boolean schedule array (operation, iteration, time), enforcing uniqueness/consistency/completion/dependence/capacity."
        support: "Sec. 4.1 Modulo Scheduling with Constraints + Figure 4 (constraint categories and op tensor definition)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Directly reusable as a formal constraint layer."
      - claim: "Working-set feasibility is enforced by memory-capacity constraints derived from SSA liveness encoded as boolean live variables, with backward-propagation constraints."
        support: "Sec. 4.2 Memory Aware Constraints + Figure 5 (SSA assumption; live definition; propagation)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Bridges schedule feasibility with on-chip memory/register constraints."
      - claim: "Warp specialization is encoded via warp-assignment constraints including variable-latency isolation, per-warp register limits, cross-warp spill delays, and synchronization-driven concurrency constraints."
        support: "Sec. 4.3 Warp Assignment Constraints + Figure 6 (constraint list) + text on spills/concurrency."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Captures Blackwell-relevant synchronization and communication structure."
      - claim: "Twill handles unsatisfiable constraint systems by a monotone search over initiation interval and schedule length (Algorithm 1)."
        support: "Sec. 5.1 Handling Unsatisfiability + Algorithm 1 (search procedure)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Provides a proof-obligation structure: smallest satisfiable II yields maximal throughput under constraints."
      - claim: "Cost normalization is formulated as an LP problem that preserves ratios of cycle counts while reducing magnitudes to keep LP/SMT tractable."
        support: "Sec. 5.2 Cost Normalization (ratio-preservation rationale and LP framing)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Important for making theory-first solvers usable in practice."
      - claim: "Evaluation reports use of NVIDIA H100 and NVIDIA B200 platforms, and states that all experiments use CUDA 13.0."
        support: "Sec. 6.1 Methodology / Evaluation Platforms."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Toolchain constraint: project targets CUDA>13.0, so results must be re-validated under CUDA 13.1+."
      - claim: "For FMHA forward pass, the paper states that Blackwell requires substantially different SWP/WS strategies than Hopper due to a faster Tensor Core and a larger set of required synchronization operations (Tensor Memory loads/stores)."
        support: "Sec. 6.2.2 Blackwell (stated reasons for strategy difference)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Connects scheduling constraints to Blackwell TMEM and synchronization costs."
      - claim: "Twill’s joint optimization on Blackwell is reported to rediscover the same strategy proposed by FlashAttention-4, combining a different software pipeline with specific warp assignments and cross-warp communication."
        support: "Sec. 6.2.2 Blackwell + Figure 9 (WS strategy visualization)."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Relies on equivalence claim to FA4 strategy; exact equality is asserted in text."
      - claim: "The paper observes configations that render the SMT problem unsatisfiable on Blackwell (e.g., reducing warps, prohibiting cross-warp communication, omitting sub-tiling), illustrating the feasibility boundary."
        support: "Sec. 6.2.2 Blackwell (unsatisfiable modifications list)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Provides negative constraints useful for model falsification tests."
    explicit_limitations_or_open_questions:
      - "Implementation supports only singly-nested loops without additional control flow; hierarchical reduction techniques are suggested as future work."
      - "Tile size is not automatically determined; requires a human or external auto-tuning system, so optimality is conditional on tile-size choice."
      - "End-to-end lowering from Twill IR to efficient code is not automated; authors hand-compile to CUDA C++ and state full automation is out of scope."
      - "Experiments use CUDA 13.0; compatibility and performance under CUDA 13.1+ and PTX>9.0 are UNVERIFIED."
  NV_BLOG_TILE:
    key_claims:
      - claim: "Triton-to-TileIR prerequisites include CUDA 13.1 or higher and NVIDIA Blackwell GPUs; previous architectures are stated to be enabled in upcoming CUDA releases."
        support: "How to use Triton-to-TileIR → Prerequisites (CUDA version; GPU architecture)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Direct toolchain gating fact for this project."
      - claim: "Triton-to-TileIR currently supports only source-bas compilation; prebuilt binaries are not available."
        support: "How to use Triton-to-TileIR section (source-based compilation statement)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Impacts reproducibility/CI setup for model calibration runs."
      - claim: "When the Tile IR backend is active, Triton caches compiled kernels with .tileIR extensions rather than standard .cubin files used by the SIMT backend."
        support: "Verify Tile IR compilation section (cache artifact statement)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Observable artifact for validation/traceability."
      - claim: "The tensor-of-pointer pattern in Triton is reported to have suboptimal performance on the Tile IR backend with CUDA 13.1, described as temporary."
        support: "Limitations → Tensor-of-pointer degradation suboptimal performance."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Performance constraintelevant to memory-access modeling and IR selection."
      - claim: "Suggested mitigations for tensor-of-pointer degradation include falling back to the SIMT backend, awaiting future optimization passes, or rewriting code to adopt the TMA load/store API."
        support: "Limitations → Tensor-of-pointer degradation (mitigation bullet list)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Guides baseline-vs-target modeling choices; treat non-TMA as a baseline."
      - clm: "The blog claims that many tiles have contiguous shapes/strides such that pointer-materialization is unnecessary; layout information can instead be passed to TMA via tensor descriptors for better performance."
        support: "Tensor-of-pointer degradation section + TMA rewrite explanation."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Modeling implication: represent memory accesses via (shape,strides,block_shape) descriptors; 'many' is qualitative."
      - claim: "An example rewrite uses tl.make_tensor_descriptor with (shape, strides, block_shape) and then descriptor load/store calls to express tile transfers."
        support: "TMA load/store API example code block (descriptor creation + load/store)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Concrete API surface for formal objects (tensor descriptor)."
      - claim: "The project is described as early-stage with known constraints including unsupported operations; compatibility is expected to improve with newer CUDA versions."
        support: "Limitations section (unsupported operations + compatibility statement)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Explicit evolution/instability; treat op-coverage as a constraint set that changes with CUDA version."
    explicit_limitations_or_open_questions:
      - "Not all Triton operations are implemented in the Tile IR backend; op coverage is incomplete (unsupported operations section)."
      - "Tensor-of-pointer performance degradation is reported specifically for CUDA 13.1 and is expected to be temporary; the exact optimization timeline is unspecified."
      - "PTX-level behavior/versions are not discussed; relationship to the project constraint PTX>9.0 is UNVERIFIED."
  NV_workloads:
    key_claims:
      - claim: "Mapping is defined as loop transformations (permutation, parallelization, tiling) that affect PE utilization and reuse."
        support: "Sec. II-A Mapping (definition and components)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Provides formal vocabulary for mapping space in Blackwell kernel models."
      - claim: "Mixed-reuse workloads are defined as containing both high- and low-arithmetic-intensity ops with dependencies limiting overlap; transformers are presented as key examples."
        support: "Sec. II-B Mixed-reuse Workloads (definition and transformer example)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Useful for composing Blackwell workload models (LLM inference/training pipelines)."
      - claim: "The Harp taxonomy classifies hierarchical/heterogeneous processors by compute placement across memory hierarchy and by heterogeneity location (intra-node, cross-node, cross-depth, compound)."
        support: "Sec. IV Harp Taxonomy (classification descriptions; Fig. 4 discussion)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Can serve as a meta-model for decomposing Blackwell SM subsystems (tensor cores, special units) as 'sub-accelerators' (INFERENCE)."
      - claim: "The evaluation framework modifies Timeloop and uses taxonomy-driven generation of sub-accelerator architecture files; operations are allocated based on reuse and mapped per-sub-accelerator."
        support: "Sec. VI-A Evaluation Framework: Timeloop for Harp (Fig. 5 description)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Provides a calibration/validation pipeline template for performance/energy modeling."
      - claim: "Results indicate no single architecture configuration dominates across transformer workloads; trends depend on model type and bandwidth partitioning."
        support: "Sec. VII-A Performance + VII-F Summary of Key Trends."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Warns against one-size-fits-all assumptions in Blackwell optimization."
      - claim: "Summary claims: encoder-only BERT favors homogeneous configurations under normal bandwidth; decoder-only GPT/Llama favor heterogeneous configurations due to overlap of high- and low-reuse ops."
        support: "Sec. VII-F Summary of Key Trends (first bullet)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Relevant to Blackwell kernels combining TC compute and variable-latency memory operations (INFERENCE)."
      - claim: "Summary claims: hierarchical+cross-depth accelerators yield lowest energy and highest energy efficiency; energy dominance differs by workload class (DRAM vs register file)."
        support: "Sec. VII-F Summary of Key Trends (energy bullets)."
        blackwell_relevance: "indirect"
        confidence: "medium"
        notes: "Energy modeling context; not Blackwell-specific."
      - claim: "Taxonomy discussion notes that existing-work table does not cover some plausible combinations; taxonomy can classify more complex architectures."
        support: "Sec. IV-B Describing Existing Works (Table I discussion: 'does not cover some combinations')."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Explicit gap: design space incompleteness."
    explicit_limitations_or_open_questions:
      - "Taxonomy examples table covers existing work only and explicitly does not cover some plausible combinations; extrapolation to unseen combinations is a modeling gap."
      - "Performance/energy trends vary with bandwidth partitioning and workload structure; mapping and resource partitioning are critical confounders."
      - "Not Blackwell-specific; any direct instantiation to Blackwell requires additional chip/toolchain measurements (UNVERIFIED)."
  SEED_1:
    key_claims:
      - claim: "Linear layouts are defined as linear maps between labeled vector spaces over F2, enabling representation and composition of tensor layouts via matrix operations over F2."
        support: "Definition 4.1 (Linear Layouts) + Definitions 4.2–4.3 (composition/product via matrix ops)."
        blackwe_relevance: "indirect"
        confidence: "high"
        notes: "Provides core math language for formalizing tile IR / TMA / shared-memory layouts."
      - claim: "The paper proves completeness results: every distributed layout and every memory layout in Triton can be represented as linear layouts."
        support: "Theorem 4.9 (distributed layouts) + Theorem 4.13 (memory layouts)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Supports modeling a broad family of layouts without ad-hoc casework."
      - claim: "Memory layouts are modeled as maps from programmable memory offsets to logical tensor coordinates and explicitly include shared memory and tensor memory among target segments."
        support: "Completeness discussion around memory layouts (memory layout definition context) + Definition 4.14."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Direct mention of 'tensor memory' makes the formalism applicable to Blackwell TMEM as an object (interpretation requires mapping TMEM addressing details)."
      - claim: "Applicability of SIMD primitives to layouts is reduced to an algebraic condition (existence of left division), formalized as Theorem 5.1."
        support: "Theorem 5.1 + Definition 4.4 (left division)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Enables constraint formulation: choose layouts such that required primitives (e.g., vectorized ld/st, ldmatrix) are legal."
      - claim: "Triton’s layout engine is described as propagating 'anchor layouts' forward/backward through use/def chains and inserting/rematerializing conversions; conflicts are resolved with heuristics."
        support: "Section 4.4 Closure Under Triton Operations → Triton’s Layout Engine paragraph."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Heuristic conflict resolution can be treated as a baseline; a theory-first project may replace with optimization (INFERENCE)."
      - c"Theorem 9.3 states that the distributed-layout family (Definition 4.10) is forward/backward closed under Triton shape ops (trans/reshape/join/split/expand_dims/broadcast) and is minimal with that property."
        support: "Theorem 9.3 (Appendix 9.1) + main-text statement in Section 4.4."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Critical for formal semantics of shape ops in tile IR/layout algebra."
      - claim: "Shared-memory bank conflicts are modeled in linear algebra, yielding a wavefront-count criterion/lower bound (Lemma 9.4) and an 'optimal swizzling' algorithm to maximize vectorization while minimizing conflicts."
        support: "Lemma 9.4 + Appendix 9.2 Optimal Swizzling."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Bank model requires Blackwell-specific bank parameters for calibration (UNVERIFIED)."
      - claim: "Limitations explicitly include restriction to power-of-two shapes and inability to express flipping/slicing as linear layouts; an affine-layout extension is suggested, and integration with hardware measurements for autotuning is proposed as future work."
        support: "Limitations paragraph near end (power-of-two; affine layouts; future plan)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Directly informs scope of the formal model and required extensions."
    explicit_limitations_or_open_questions:
      - "Power-of-two shape restriction; non-power-of-two handled via padding/masking but not natively represented."
      - "Flipping and slicing are not expressible in linear layouts; affine layouts proposed as extension."
      - "Bank-conflict model and SIMD-primitive conditions require calibration to Blackwell-specific microarchitectural parameters; those parameters are not provided in this paper (UNVERIFIED)."
      - "Toolchain versions (CUDA/PTX) are not the focus; compatibility with CUDA>13.0 and PTX>9.0 is UNVERIFIED."
  SEED_2:
    key_claims:
      - claim: "The paper proposes integer set relations (ISL) as a unified mathematical framework to model both CuTe layouts (including bit-level swizzles) and Triton linear layouts."
        support: "Introduction contributions paragraph + Background sections 2.1–2.3."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Provides alternative formalization compatible with polyhedral tooling."
      - claim: "CuTe layouts are decomposed into coordinate mapping (integral→natural), index mapping (product with strides), and their composition to form the overall layout mapping."
        support: "Section 2.1 CuTe Layout (coordinate/index/layout mapping explanation) + Section 3 (integer-set relations for these mappings)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Enables formal reasoning about shape/stride/block_shape descriptors used in tile systems."
      - claim: "Hierarchical layouts are supported via nested shapes/strides and define multiple compatible natural coordinate spaces for the same layout mapping."
        support: "Section 2.1.1 Hierarchical Layouts (compatibility definition)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Relevant to multi-level tiling and nested descriptors (INFERENCE)."
      - claim: "CuTe layout operations are grounded in three fundamental operations (composition, inverse, complement) that serve as basis for divisions and products."
        support: "Section 2.1.2 Layout Operations."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Algebra of layout transforms that can be mapped to solver constraints."
      - claim: "CuTe swizzle is modeled as a bijective bit-level mapping parameterized by (bits, base, shift) using XOR/AND and shifts; swizzle mapping can be represented as an integer set relation and is described as an involution."
        support: "Section 2.2 CuTe Swizzle + Section 4.2/4.3 (swizzle mapping and involution statement)."
        blackwell_relevance: "indirect"
        confidence: "medium"
        notes: "Swizzle involution property is stated; exact conditions should be rechecked when instantiating."
      - claim: "Linear layouts are modeled as transformations between binary vector spaces over F2; dimension sizes must be powers of two; basis-vector images define the full mapping via XOR/AND."
        support: "Section 2.3 Linear Layout (binary vector space + basis vector rule + power-of-two note)."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Aligns with SEED_1 formalism; provides an ISL bridge."
      - claim: "Algorithms are provided to reconstruct CuTe layouts from relations when shape or strides are known (e.g., get_layout_strictly_affine), with correctness arguments."
        support: "Algorithms in Section 3 (get_layout_strictly_affine; get_layout_from_strides) + accompanying correctness proofs."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Useful for verification/inference subroutines inside a solver loop."
      - claim: "The paper explicitly states an open problem: inferring both shape and strides from the layout mapping alone is still open."
        support: "Section 3 (layout inference discussion ending with the open problem statement)."
        blackwell_relevance: "indirect"
        confidence: "high"
        notes: "Important limitation for any approach that only observes mapping behavior."
      - claim: "A complexity discussion notes worst-case exponential behavior for relation operations, but argues ISL is practical and widely deployed (e.g., LLVM Polly)."
        support: "Complexity subsection."
        blackwell_relevance: "indirect"
        confidence: "medium"
        notes: "Practicality claim; actual solver/runtime behavior for large layout spaces remains empirical."
    explicit_limitations_or_open_questions:
      - "Open problem: infer both shape and strides from layout mapping alone."
      - "Quasi-affine relations arise for certain hierarchical shapes; reconstruction depends on choosing a compatible shape/stride representation."
      - "Worst-case exponential complexity for relation operations; practical performance depends on ISL heuristics and instance structure."
      - "No Blackwell-specific microarchitectural parameters are provided; linkage to TMEM/TMA specifics is by abstraction (INFERENCE)."
  SEED_3:
    key_claims:
      - claim: "Experiments are conducted on NVIDIA GB10, described as a Grace Blackwell chip with a Blackwell GPU (48 SMs) and ARM v9.2 CPU cores, with unified LPDDR5X memory and stated bandwidth figures."
        support: "Section 2.1 GPU Memory Hierarchy → NVIDIA GB10 paragraph."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Grace-Blackwell context (allowed by scope)."
      - claim: "Cache-counter data shows negligible L1 hit counts and indicates L2 traffic is overwhelmingly driven by the L1 texture path (L1Tex) for the studied streaming Flash Attention workload; persistent vs non-persistentcheduling yields nearly identical L1/L2 behavior when GPU is saturated."
        support: "Section 3.1 Effect of L1 Caching + Tables 1–2 and accompanying interpretation."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Kernel/workload specific; generalization requires validation on other kernels."
      - claim: "The paper introduces an L2 sector-access model for Flash Attention using variables (sequence length, sector size, element size, tile size, head dimension, number  sectors) and provides an approximation formula (ignoring trailing effects)."
        support: "Section 3.2 Modeling L2 Sector Access (variable list + approximation statement)."
        blackwell_relevance: "direct"
        confidence: "medium"
        notes: "Provides a parameterized traffic model component; exact formula details require careful transcription in later stages."
      - claim: "Sawtooth Wavefront Reordering is implemented as an alternating scan of KV tiles (forward/backward) per query-tile parity (Algorithm 4)."
        support: "Section 4.1 Implementation + Algorithm 4."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "A scheduling transformation amenable to formalization as a permutation of traversal order."
      - claim: "CUDA results report that sawtooth ordering reduces L2 non-compulsory misses by ~50% and increases throughput (example: ~1.3 TFLOPS to ~2.4 TFLOPS)."
        support: "Section 4.2 CUDA Results (text; Figures 7–8)."
        blackwell_relevae: "direct"
        confidence: "high"
        notes: "Quantitative but tied to specific kernel/device; treat as calibration datapoint."
      - claim: "CuTile validation reports miss-count reductions (approx. 370M to 120M sectors) and throughput improvements (non-causal: 61→69 TFLOPS, 13%; causal: 41→66 TFLOPS, 60%) for sawtooth variants."
        support: "Section 4.3.1 Results + Figures 9–12 + Summary paragraph."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Direclevant to CUDA Tile / tile-IR-based implementations on Blackwell."
      - claim: "Limitations: effectiveness depends on tile size fitting shared-memory capacity; for large tile size (128), CuTile compiler may split tiles that do not fit in L1Tex, altering the access pattern; left as future work."
        support: "Section 4.3.2 Limitations."
        blackwell_relevance: "direct"
        confidence: "high"
        notes: "Provides explicit constraint on transformation applicability."
      - claim: "Sawtooth ordering is characterized as a machine-independent locality optimization distinct from cache-targeted tiling (conceptual positioning)."
        support: "Related Work / discussion near end (sawtooth vs tiling characterization)."
        blackwell_relevance: "indirect"
        confidence: "medium"
        notes: "Conceptual claim; still needs instantiation to Blackwell cache geometry for quantitative modeling."
    explicit_limitations_or_open_questions:
      - "Tile-size limitation: optimization assumes selected tile size fits shared-memory capacity; compiler transformations (tile splitting) can invalidate the intended access pattern."
      - "L2 sector-access model begins with single-batch single-head and ignores trailing effects; multi-head/batch scaling is treated as linear and needs empirical confirmation."
      - "CUDA/PTX versions are not specified; compliance with CUDA>13.0 and PTX>9.0 is UNVERIFIED."
cross_source_synthesis:
  agreements:
    - "All sources emphasize that modern GPU performance hinges on structured tiling, explicit data movement, and careful scheduling/synchronization (ARCH_BW: TMEM/DE/tcgen05; OPT_PIPE: WS+SWP constraints; NV_BLOG_TILE: descriptors/TMA; SEED_3: CTA scheduling + traversal order)."
    - "Layout as a first-class formal object is repeatedly central: NV_BLOG_TILE’s descriptor parameters (shape/strides/block_shape) align naturally with SEED_1’s linear-layout algebra and SEED_2’s integer-relation modeling."
    - "Solver-ready formulations are viable for key compilisions: OPT_PIPE supplies SMT/LP constraint structure, while SEED_1/SEED_2 supply algebraic representations that can be embedded as constraints or verified post-hoc."
    - "Blackwell introduces memory/synchronization features that change optimal strategies (ARCH_BW TMEM + new PTX instructions; OPT_PIPE notes extra synchronization for tensor-memory loads/stores; SEED_3 shows cache/locality effects dominate for streaming KV)."
  tensions_or_contradictions:
    - "CUDA version baseline differs across sources: OPT_PIPE experiments use CUDA 13.0, ARCH_BW states CUDA 13.0 has preliminary TMEM/CTA support, while NV_BLOG_TILE requires CUDA 13.1+ for Triton-to-TileIR; reconciling results under CUDA>13.0 is required."
    - "Chip/SKU differences inside 'Blackwell': ARCH_BW focuses on datacenter B200, while SEED_3 focuses on Grace-Blackwell GB10; microarchitectural parameters (memory system, cache behavior, TMEM use) may not transfer directly without calibration."
    - "Heuristic vs theory-first tension: SEED_1 describes heuristic conflict resolution in Triton’s layout engine, whereas OPT_PIPE frames a heuristic-free optimal solver approach; integrating both requires clear separation of baseline heuristics vs optimized formulations."
    - "PTX>9.0 toolchain constraint is not explicitly validated by any golden source; PTX version requirements/availability for tcgen05/TMEM/TMA remain UNVERIFIED within this pack."
  inferred_implications_marked_as_inference:
    - "INFERENCE: A unified Blackwell model can treat (i) layo selection as algebra over linear layouts / integer relations (SEED_1/SEED_2), (ii) tile movement as descriptor-driven TMA transfers (NV_BLOG_TILE), and (iii) schedule feasibility/optimality as SMT/ILP constraints (OPT_PIPE), with hardware parameters (latency/bandwidth) calibrated from Blackwell microbenchmarks (ARCH_BW)."
    - "INFERENCE: SEED_3’s L2-sector traffic model and traversal-order transformation can be incorporated as additional constraints/objectives in OPT_PIPE-style scheduling (e.g., penali reuse distance / predicted non-compulsory misses) for Blackwell attention kernels."
    - "INFERENCE: NV_workloads’ reuse-based partitioning and dependency-graph sensitivity can serve as a higher-level prior for selecting which Blackwell kernel stages should be overlapped or specialized across warps/CTAs, while OPT_PIPE provides the feasibility/optimality layer."
    - "INFERENCE: NV_BLOG_TILE’s 'tensor-of-pointer degradation' suggests a formal cost term for pointer-materialization vs descriptor-based  SEED_1/SEED_2 can formalize when a tensor-of-pointers can be eliminated by a descriptor with equivalent mapping."
