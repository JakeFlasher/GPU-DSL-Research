## Ground Truth Glossary

| Term | Hardware_Definition (Official Manuals) | Hard_Constraints (Alignment/Size/Types) | Paper_1_Coverage (Linear Layouts) | Paper_2_Coverage (ISL) | Paper_3_Coverage (Categorical) |
|---|---|---|---|---|---|
| `CuTensorMap` / TMA | **CUDA Driver API:** `CUtensorMap` is an **opaque tensor-map descriptor** created via `cuTensorMapEncodeTiled` / `cuTensorMapEncodeIm2col` to represent a *tiled* or *im2col* memory region for **Tensor Memory Access (TMA)**; the doc explicitly says the tensor map is opaque and “should only be accessed through CUDA APIs and PTX.” ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>**PTX ISA:** tensor copy ops (e.g., `cp.async.bulk.tensor`) take a `[tensorMap, tensorCoords]` operand and refer to the tensor-map definition; PTX also exposes `tensormap.replace` to mutate fields of the **1024-bit tensor-map object**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html))<br><br>**CUDA Programming Guide:** describes the **TMA engine** and that the tensor-map encodes **swizzle mode** for bank-conflict mitigation. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html)) | **Descriptor / bounds:**<br>• `tensorMap` address alignment: **64B**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `tensorRank`: **1..5** (and if `interleave != NONE`, rank must be **≥ 3**). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalDim[i]`: non-zero, **≤ 2^32**; packed-type special cases impose divisibility on `globalDim[0]` (e.g., multiple-of-128 for some packed types). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `globalStrides[i]` (bytes): **multiple of 16**, **< 2^40**, with extra multiple-of-32 requirements in some modes/types. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `boxDim[i]`: non-zero, **≤ 256**; if `interleave == NONE`, `boxDim[0] * elementSize` must be **multiple of 16 bytes**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• `elementStrides[i]`: non-zero, **≤ 8**; **dimension-0 stride is ignored** when `interleave == NONE` because **TMA doesn’t support stride for dimension zero**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>**Address alignment:**<br>• `globalAddress`: **16B aligned**; additional **32B aligned** requirements for `interleave==32B` and for specific packed types. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br><br>**Swizzle modes & alignment cliffs (guide-level):**<br>• Swizzle mode set is **finite**: none / 32B / 64B / 128B. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• Swizzle *validity* requires: **shared-memory inner dimension ≤ swizzle span**, otherwise “instruction is considered invalid”. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• Swizzle *alignment*: “Global memory must be aligned to **128 bytes**”; “shared memory is required to be aligned to **128 bytes**” (in the swizzle considerations section). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• Swizzle granularity fixed at **16 bytes** (“16-byte chunks”). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html)) | **Explicitly targets tensor layouts as hardware-resource mappings** (linear maps over \(\mathbb{F}_2\)), and discusses swizzling/bank conflicts and tensor-core layouts; **does not define** CUDA Driver `CUtensorMap` field/bounds semantics. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Models **CuTe layouts** (shape+strides mapping) and **swizzles** in a unified ISL relation formalism; **no driver/PTX tensor-map encoding constraints** (alignment, rank≤5, stride multiples, etc.) are part of the model. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Categorical reconstruction of **CuTe layout algebra** as used in CUTLASS; **no treatment of tensor-map descriptor bounds/alignment/swizzle legality constraints**. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| TMEM (Blackwell Tensor Memory) | **PTX ISA:** defines **Tensor Memory** as a distinct on-chip memory with **32-bit addresses** composed of lane and column indices; it is **dynamically allocated** via dedicated allocation/management instructions (e.g., `tcgen05.alloc` / `tcgen05.dealloc`) and accessed by Tensorcore 5th-gen instruction families (e.g., `tcgen05.ld`, `tcgen05.shift`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | **Allocation & addressing:**<br>• TMEM address is **32-bit** (lane + column indexing). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Allocation unit is **32 columns**; number of allocated columns must be a **power of 2**; allocating a column allocates **all 128 lanes**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Allocation must be performed by a **single warp in a CTA**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Deallocation must reference a prior allocation; **all TMEM allocated in a kernel must be explicitly deallocated before kernel exit**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br><br>**Collective access rules:**<br>• `tcgen05.ld`: all threads in the warp must specify the **same `taddr`** or behavior is undefined. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Warpgroup lane partitioning: each warp in a warpgroup can access only a **lane subset** (e.g., warp 0: lanes 0–31, … warp 3: lanes 96–127). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• `tcgen05.shift`: “lane … must be aligned to **32**.” ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | Mentions **Tensor Memory on Blackwell** as a “special memory unit” requiring special layouts, but does not model allocation/addressing semantics. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | ISL treatment is at the level of **layout mappings and swizzles** (CuTe + linear layouts); no TMEM allocation/addressing/lane partitioning semantics. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Categorical foundations for CuTe layouts; does not specify TMEM allocation/addressing rules. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `cp.async.bulk.tensor` | **PTX ISA:** `cp.async.bulk.tensor` is a **non-blocking** instruction that initiates an **asynchronous tensor copy** between state spaces; it consumes a **tensor-map object** plus coordinates and can integrate with completion mechanisms (bulk async groups). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)) | **Instruction-level legality constraints (selected):**<br>• Target ISA: requires **`sm_90` or higher** (per PTX target note). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• `tensorCoords` entries are **`.s32`**, and vector length must match `.dim` (1d–5d). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html))<br><br>**Type/shape/alignment cliffs (packed & sub-byte cases):**<br>• For certain packed types (e.g., `.b4x16_p64`, `.b6x16_p32`, `.b6p2x16`): constraints include **Box-Size[0] exact values** and **Tensor-Size[0] multiples**, plus **global address alignment** and **tensor-stride alignment** requirements; also **tensorCoords[0] multiple-of-128** in some cases. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=type))<br>• Additional per-ISA variant restrictions exist (e.g., `sm_103a` constraints for `.b6p2x16`, and `sm_120a` restrictions). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br><br>**Bounding-box legality:** for `.global.shared::cta`, starting coordinates must be **non-negative**, and bounding box must remain within tensor boundaries (per restriction block). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=alternate))<br><br>**Bulk-group semantics:** `cp.async.bulk.commit_group` / `wait_group` define completion management; PTX explicitly states **no memory ordering guarantee** between tensor copies within the same bulk async-group. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | Treats data movement as a **layout conversion/codegen** problem and lowers some conversions to SIMD primitives; does **not** present `cp.async.bulk.tensor`’s type/box/alignment legality constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | ISL model covers mappings/swizzles/relations; **no PTX instruction semantics** for `cp.async.bulk.tensor` completion/legality. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Layout-algebra foundations; **no instruction-level async copy** constraints. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `wgmma` / `mma` (operand layout requirements) | **PTX ISA (`mma.sync`):** warp-level MMA uses `mma.sync.aligned.*` forms with explicit major-order qualifiers (e.g., `.row.col`) and fixed fragment register vector shapes per instruction variant. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br><br>**PTX ISA (`wgmma.mma_async` exists):** PTX memory model explicitly references `wgmma.mma_async` as an asynchronous instruction class. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html))<br><br>**PTX ISA (shared-memory multiplicand descriptor):** matrix multiplicands in shared memory are described via a **64-bit “matrix descriptor”**, which includes encoded **start address**, **leading/stride byte offsets**, and an encoded **swizzle mode** (0 none / 1 128B / 2 64B / 3 32B). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br><br>**PTX ISA (tcgen05 swizzle legality):** for 5th-gen tensorcore datapaths, PTX lists **valid combinations of type-size, major-ness, and swizzling**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | **Descriptor expressiveness limitations:**<br>• Matrix descriptor stores address/offsets via `matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4`, i.e., with **16-byte granularity** (low 4 bits dropped) — meaning arbitrary byte offsets are not representable *(inference from encoding definition; not a separate guarantee statement).* ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Swizzle mode is a **2-bit field** ⇒ only {none, 32B, 64B, 128B}. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• For tcgen05 layouts, supported swizzle depends on **type-size** and **major-ness** and differs by matrix role (A vs B, transpose, etc.). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• `mma.sync.aligned` fragments are fixed-width register vectors determined by instruction shape/type (no “arbitrary layout” freedom at the ISA level). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | Explicitly discusses **mma/wgmma requiring specific layouts**, and develops “mma swizzling” as a linear-layout construct; uses these as primary motivation for layout formalism and codegen. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Models CuTe + Triton linear layouts and swizzles formally, but does not encode PTX fragment packing or matrix-descriptor bitfield constraints. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Uses tiled matmul as motivation for layout algebra; no PTX-level operand descriptor/fragment constraints. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| `mbarrier` / async transaction barriers | **PTX ISA:** mbarrier is an **opaque synchronization object in memory** used to synchronize threads and asynchronous memory operations; PTX enumerates operations (`mbarrier.init`, `.inval`, `.expect_tx`, `.complete_tx`, `.arrive`, `.try_wait`, etc.) and defines phase completion semantics including tx-count. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | **Object shape & lifecycle:**<br>• Type: **`.b64`**, alignment: **8 bytes**, memory space: **`.shared`**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Must be **initialized** before use; using other operations on uninitialized object is UB; doing non-mbarrier ops on initialized object is UB; must be **invalidated** to repurpose storage. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• `count` valid range for init: \([1, 2^{20}-1]\). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Phase completes only when **pending arrivals == 0** *and* **tx-count == 0**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• Cluster-shared vs CTA-shared support differs (some operations not supported on `.shared::cluster` objects). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))<br>• `.expect_tx` support requires **`sm_90`+** (target notes in instruction section). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)) | Does not model mbarrier object lifecycle, alignment, count bounds, or tx-count semantics; layout formalism is orthogonal. ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | No barrier semantics; ISL model is relations over integer sets for layouts/swizzles. ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | No barrier semantics in categorical layout foundations. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |
| Bank Conflicts (shared memory) | **CUDA Programming Guide:** shared memory has **32 banks**; successive **32-bit words map to successive banks**; bank conflicts occur when threads in a warp access different addresses in the same bank, and are serialized; **broadcast** exists when multiple threads access the same location. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html))<br><br>**TMA swizzle context:** TMA swizzle patterns are defined to remap **16-byte chunks** across bank groups to reduce conflicts; tensor map encodes swizzle mode. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html)) | **Baseline bank model (current guide):**<br>• 32 banks, 32-bit word mapping, 32-bit-per-cycle-per-bank bandwidth model, warp-level conflict rules and broadcast exception. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html))<br><br>**TMA swizzle-specific cliffs:**<br>• Swizzle mapping granularity fixed at **16 bytes**. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• Swizzle validity requires **inner dimension ≤ swizzle span**, else invalid. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• Swizzle section requires **128B alignment** for global memory and **128B alignment** for shared memory (as stated in that section). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html)) | Explicitly treats bank conflicts as an optimization target (“optimal swizzling … minimizes … bank conflicts”). ([arxiv.org](https://arxiv.org/html/2505.23819v3)) | Mentions “complex … swizzle patterns” as part of layout complexity, but does not formalize the hardware bank-conflict cost model (bank count/word mapping/broadcast rules). ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | Focuses on layout algebra (e.g., coalesce/compact/composition) rather than the shared-memory bank conflict hardware model. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) |

## Constraint Cliffs (Critical Mismatches)

1. **“Arbitrary layout/stride” vs TMA tensor-map admissibility.**  
   Hardware tensor-maps impose *structural bounds* (rank ≤ 5; `globalStrides` multiple-of-16 and < \(2^{40}\); `boxDim[i] ≤ 256`; `globalAddress` 16B/32B alignment; and even “dimension 0 stride ignored” for `interleave==NONE`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))  
   Theory papers model layouts as general mappings (e.g., CuTe shape+stride mappings; linear maps over \(\mathbb{F}_2\); categorical algebra) without encoding these admissibility constraints. ([arxiv.org](https://arxiv.org/html/2511.10374v1))

2. **Swizzle as “any permutation/linear map” vs swizzle as a *finite*, alignment-gated hardware mode.**  
   TMA swizzle is explicitly enumerated (none/32B/64B/128B) and comes with hard validity requirements (inner dimension ≤ span), plus strong 128B alignment conditions in the swizzle guidance, and a fixed 16B granularity. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html))  
   Papers treat swizzle as an abstract mapping (bitwise XOR/AND constructions in linear layouts; generalized “swizzle patterns” in ISL) without these discrete-mode and alignment cliffs. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

3. **Async tensor copy as “copy” vs `cp.async.bulk.tensor` as a *typed, boxed, aligned* ISA contract.**  
   PTX places type- and ISA-variant-specific constraints on tensor copies (exact Box-Size[0] for certain packed types, strict address/stride alignment, coordinate multiples, limited swizzle availability, bounding box constraints). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=type))  
   None of the seed papers encode these instruction-admissibility conditions as part of their layout formalisms. ([arxiv.org](https://arxiv.org/html/2511.10374v1))

4. **“mma/wgmma needs a special layout” vs operand-layout *encoding limits* (matrix descriptor + swizzle legality tables).**  
   Shared-memory multiplicands are described by a 64-bit matrix descriptor with an explicit swizzle-mode field and address/offset encoding that is quantized via `>> 4` (16B granularity). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))  
   For tcgen05-style datapaths, supported swizzle depends on type-size and major-ness (some combinations are disallowed). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))  
   The papers motivate and model these layouts, but do not expose descriptor-bitfield constraints as first-class objects of the theory. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

5. **TMEM as “a special memory unit” vs TMEM as a dynamically allocated, partitioned, explicitly deallocated resource.**  
   PTX requires TMEM allocation in power-of-two column counts (unit 32 columns), single-warp allocation, explicit deallocation before kernel exit, and enforces warpgroup lane-access partitioning and collective-access rules (e.g., same `taddr` across warp for `tcgen05.ld`). ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async))  
   Linear Layouts mentions TMEM as a special memory unit but does not model these allocation/addressing/lane constraints. ([arxiv.org](https://arxiv.org/html/2505.23819v3))

---
Learn more:
1. [https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
2. [https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)
3. [https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/async-copies.html)
4. [https://arxiv.org/html/2505.23819v3](https://arxiv.org/html/2505.23819v3)
5. [https://arxiv.org/html/2511.10374v1](https://arxiv.org/html/2511.10374v1)
6. [https://arxiv.org/pdf/2601.05972v1](https://arxiv.org/pdf/2601.05972v1)
7. [https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=st+async)
8. [https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=type](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=type)
9. [https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=alternate](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=alternate)
10. [https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)
11. [https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html)
