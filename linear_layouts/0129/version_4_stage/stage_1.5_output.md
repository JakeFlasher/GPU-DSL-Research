## Toolbox

| Bottleneck | Theory_A (why it fits) | Theory_B (why it fits) | Prototype_Mechanism (MLIR/Triton/CuTe) | Key_Metrics | Main_Risk |
|---|---|---|---|---|---|
| **1) Latency hiding / async orchestration** (Hopper+ bulk async + `mbarrier` tx-count) | **Effect/typestate for async transactions**<br>• **Why:** PTX makes async copies + barriers a *protocol* with UB cliffs (init/lifecycle, tx-count phase completion), so “layout correctness” is insufficient. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html))<br>• **Math object:** typestate automaton + **linear capabilities** (tokens for “in‑flight transactions/bytes”).<br>• **Mechanism (IR+algo+prototype):** extend MLIR with SSA **transaction tokens** that are produced by `nvgpu.tma.async.load` / `nvgpu.device_async_copy` and consumed by `nvgpu.mbarrier.*`; add a verifier that proves: init-before-use, `expect_tx` equals sum of completion units, and wait/test_wait happens before reuse. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Hardware hook:** `cp.async.bulk{.tensor}.*.mbarrier::complete_tx::bytes` + `mbarrier.expect_tx` + “phase completes when pending arrivals == 0 **and** tx-count == 0”. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html))<br>• **Metric:** barrier-stall %, “pipeline depth” (outstanding groups), verifier time; **Risk:** conservative tx-unit accounting (e.g., mixed tiles/OOB) inflates waits. | **Axiomatic semantics + bounded counterexample search** (memory-model lens)<br>• **Why:** bulk async groups explicitly have **no ordering guarantees** internally; bugs look correct in SSA but fail by ordering/visibility subtleties. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html))<br>• **Math object:** event structure + relational constraints (axiomatic memory model).<br>• **Mechanism (IR+algo+prototype):** export a reduced “async event graph” from MLIR/Triton lowering, then run bounded checking (Alloy-style) to find counterexamples; use the PTX memory-model formalization pipeline as blueprint. ([research.nvidia.com](https://research.nvidia.com/publication/2019-04_formal-analysis-nvidia-ptx-memory-consistency-model))<br>• **Hardware hook:** PTX scoped synchronization + weak async operations; group non-ordering constraints. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html))<br>• **Metric:** #counterexamples per kernel family + time-to-check; **Risk:** bounded checks can miss large-state deadlocks. | **Hybrid (A+B): “fast typestate + slow model-check”**<br>• **Hybrid:** typestate verifier is the *compiler gate*; bounded checker is the *CI gate* for representative micro-instances (tile sizes, stage counts). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html))<br>• **IR extension:** reuse `nvgpu.mbarrier.*`, `nvgpu.tma.async.*`, `DeviceAsyncTokenType/MBarrierTokenType`; add `pipeline.stage` attrs + tokenized deps. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Algorithm:** (1) schedule synthesis (choose stage distance / group depth), (2) typestate check, (3) optionally emit bounded checker instance.<br>• **Prototype path:** MLIR pass pipeline to NVGPU→NVVM/PTX; Triton backend emits NVGPU ops (or directly emits PTX) and reuses the verifier. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Metric:** bug yield vs compile overhead (ms/kernel) + stall reduction. | • Barrier stall fraction (Nsight/trace-derived)<br>• Bulk-async groups in flight; %time tensor pipe idle<br>• Verifier runtime + false-positive rate<br>• Deadlock/UB catch rate (CI) | Protocol modeling can become the new “constraint cliff” if tx-units and phase rules drift across PTX versions. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html)) |
| **2) Memory bandwidth wall** (reuse + vectorization + overlap dominate) | **Tile calculus / explicit blocking IR** (Triton-style tiling as the *unit of reasoning*)<br>• **Why:** a tile-first IR makes reuse/residency explicit; that’s exactly the lever you need when HBM is the roof. ([research.ibm.com](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations))<br>• **Math object:** tiled iteration spaces + layout transforms (tile→memory mapping).<br>• **Mechanism (IR+algo+prototype):** in Triton (tile IR) or MLIR (`linalg`/`affine` tiling), run a tiler that chooses (i) tile shapes, (ii) shared/TMEM residency, (iii) vector widths; integrate with layout conversion (e.g., linear layouts) so tile boundaries remain legal. ([research.ibm.com](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations))<br>• **Hardware hook:** TMA swizzle/alignment rules constrain which tile shapes can be staged efficiently. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• **Metric:** achieved BW (GB/s), bytes moved per output element, L2 hit rate; **Risk:** dynamic shapes/edge tiles reduce reuse unless masked well. | **Learned cost model + hierarchical search** (Ansor lens)<br>• **Why:** bandwidth depends on discrete choices (tile sizes, unroll, vectorization, pipeline depth) that analytic models approximate poorly—search + cost model wins here. ([usenix.org](https://www.usenix.org/conference/osdi20/presentation/zheng))<br>• **Math object:** combinatorial optimization over a hierarchical schedule space with a learned cost function.<br>• **Mechanism (IR+algo+prototype):** generate candidate tilings/pipelines from MLIR (or Triton) and rank with a learned model; keep “legality predicates” (TMA admissibility, swizzle validity) as hard constraints in the sampler. ([arxiv.org](https://arxiv.org/abs/2006.06762))<br>• **Hardware hook:** use features tied to memory transactions and on-chip reuse (alignment, contiguous bytes, occupancy).<br>• **Metric:** tuning time + achieved BW + code size; **Risk:** training/generalization across GPU families. | **Hybrid (A+B): “analytic tile IR defines space; learned model picks point”**<br>• **Hybrid:** tile calculus defines *what* can be done; learned model decides *which* legal tiling wins per architecture. ([research.ibm.com](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations))<br>• **IR extension:** add `layout.legality` + `layout.cost_features` attrs on tiles (stride, alignment, stage depth). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Algorithm:** (1) generate legal tilings via constraints, (2) rank by cost model, (3) optionally local search refine.<br>• **Prototype path:** Triton: reuse tile IR knobs + backend lowering; MLIR: linalg/affine tiling + NVGPU/AMDGPU lowering. ([arxiv.org](https://arxiv.org/abs/2002.11054))<br>• **Metric:** BW/byte, reuse factor, compile+tune budget. | • Achieved HBM BW / theoretical BW<br>• Bytes loaded per MAC / per output<br>• L2 hit rate / smem(tmem) hit rate proxies<br>• Compile+tuning time; code size | Risk of “optimize the proxy”: cost model predicts BW but misses pipeline hazards (barriers, bank conflicts). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html)) |
| **3) Descriptor legality cliffs** (TMA admissibility + WGMMA descriptor encodability) | **SMT/ILP constraint solving for “legal layout synthesis”**<br>• **Why:** legality is a **binary cliff** (rank/stride/box/alignment); you want a solver to *construct* a legal padding/tiling rather than fail late. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Math object:** integer constraints with congruences (mod/divisibility) + bounded domains.<br>• **Mechanism (IR+algo+prototype):** represent candidate layouts as MLIR attrs; generate constraints for `cuTensorMapEncodeTiled` admissibility (e.g., stride multiple-of-16, rank bounds) and WGMMA descriptor quantization (16B granularity); solve; emit `nvgpu.tma.create.descriptor` + legalized padding. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Hardware hook:** `CUtensorMap` rules + TMA swizzle validity/alignment + matrix-descriptor encoding \( (x \& 0x3FFFF) \gg 4 \). ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Metric:** solve time (ms), %kernels hitting TMA fast path, padding overhead; **Risk:** solver overhead or missing a constraint ⇒ unsound “proof”. | **Refinement (liquid) types for legality proofs** (types-as-constraints lens)<br>• **Why:** many legality facts are *local invariants* (alignment, divisibility, bounds) that should be carried by types, not rediscovered by ad hoc passes. ([arxiv.org](https://arxiv.org/abs/2207.04034))<br>• **Math object:** refinement logic (decidable fragments + SMT discharge).<br>• **Mechanism (IR+algo+prototype):** introduce refined memref/tensor types in MLIR (e.g., `memref<..., align=128, stride_mod=16>`); inference pass propagates refinements through layout ops; verifier rejects illegal `tma.create.descriptor` early. (Flux shows “liquid inference” can make refinements ergonomic.) ([arxiv.org](https://arxiv.org/abs/2207.04034))<br>• **Hardware hook:** alignment/stride/box constraints from CUDA TMA docs become type refinements. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Metric:** compile-time spent proving refinements + #illegal configs rejected with actionable diagnostics; **Risk:** refinement inference complexity (especially across dynamic shapes). | **Hybrid (A+B): “refinements prune; solver constructs”**<br>• **Hybrid:** refinements act as a fast filter + explainable errors; SMT/ILP only runs when choices (padding/tile) must be synthesized. ([arxiv.org](https://arxiv.org/abs/2207.04034))<br>• **IR extension:** (1) refined layout attrs/types, (2) `nvgpu` descriptor ops as the legalization boundary. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Algorithm:** do local proof via refinement propagation; escalate to solver for global “pick padding/boxDim/swizzle” decisions.<br>• **Prototype path:** MLIR: legality verifier + descriptor synthesis pass; Triton: hook into Linear Layouts backend to emit proofs or diagnostics. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Metric:** compile-time vs fallback-rate curve. | • %TMA-admissible kernels (vs fallback)<br>• Solver time / refinement-check time<br>• Padding bytes added; code size impact<br>• #invalid-swizzle rejections caught early | Keeping legality models aligned with evolving PTX/TMA rules (e.g., new swizzle modes / target-specific restrictions). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html)) |
| **4) Bank conflicts + finite swizzle modes** (SMEM/LDS serialization) | **Modular bank algebra + linear swizzle synthesis**<br>• **Why:** bank conflicts are (mostly) a modular-address phenomenon; swizzles are constrained (finite modes), so you want a math model that emits only realizable candidates. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html))<br>• **Math object:** modular arithmetic over bank index + restricted linear/XOR maps (matching what Linear Layouts can express). ([arxiv.org](https://arxiv.org/abs/2505.23819))<br>• **Mechanism (IR+algo+prototype):** represent access pattern as \((lane, i, j) \mapsto addr \mapsto bank\); solve for XOR/padding that makes bank index injective per warp/wave phase; lower to NVIDIA TMA swizzle enums or AMD `ds.swizzle` / padding. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• **Hardware hook:** NVIDIA has enumerated TMA swizzle patterns + 128B align + “inner dimension” validity; AMD MI GPUs have 32 LDS banks and instruction-dependent lane-group conflict rules. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• **Metric:** bank conflict rate + shared/LDS BW; **Risk:** mismatch across ISA variants (AMD phase rules, NVIDIA swizzle atomicity variants). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html)) | **ISL/Presburger relation analysis + profile calibration** (symbolic + empirical lens)<br>• **Why:** you need vendor-parametric analysis; ISL relations already unify CuTe/Triton layout mappings and can be extended to bank-index relations. ([arxiv.org](https://arxiv.org/abs/2511.10374))<br>• **Math object:** Presburger relations + set operations for conflict sets.<br>• **Mechanism (IR+algo+prototype):** encode bank mapping as ISL relation; compute conflict multiplicity (overapprox) symbolically; calibrate with profiling metrics (e.g., ROCm “Bank Conflict Rate”, “Bank Conflicts/Access”). ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/conceptual/local-data-share.html))<br>• **Hardware hook:** ROCm profiler exposes bank conflict metrics; NVIDIA shared memory bank structure defined in CUDA guide. ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/conceptual/local-data-share.html))<br>• **Metric:** correlation between predicted vs measured conflicts; **Risk:** symbolic overapprox too loose without ISA-specific instruction-width modeling. | **Hybrid (A+B): “symbolic predicts; algebra constructs; profiler validates”**<br>• **Hybrid:** ISL gives conflict *detection*; modular algebra gives conflict *repair* (XOR/pad candidates); profiling picks among close options. ([arxiv.org](https://arxiv.org/abs/2511.10374))<br>• **IR extension:** add `bank_model` attr (vendor+ISA+instruction width) + `swizzle_choice` attr (finite) at layout boundary. ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• **Algorithm:** (1) symbolic conflict set, (2) generate candidate swizzles/pads, (3) legality check (finite modes), (4) optional profile-guided selection.<br>• **Prototype path:** MLIR: use NVGPU `TensorMapSwizzleKind` attrs + AMDGPU `amdgpu.swizzle_bitmode`; Triton/CuTe: emit only supported swizzles. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/))<br>• **Metric:** bank-conflict-rate reduction per added padding byte. | • NVIDIA: shared bank conflict indicators + achieved shared BW<br>• AMD: Bank Conflict Rate / Conflicts per Access ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/conceptual/local-data-share.html))<br>• Compile-time of analysis & candidate count | Cross-vendor semantics: “conflict-free” needs ISA-specific lane grouping; easy to overfit to one instruction form. ([rocm.blogs.amd.com](https://rocm.blogs.amd.com/software-tools-optimization/lds-bank-conflict/README.html)) |
| **5) Register pressure / fragment packing → occupancy collapse** | **Min-reg scheduling via CP/ILP** (schedule under register budget)<br>• **Why:** tile+pipeline tends to blow up live ranges; lowering register peak directly improves occupancy and throughput. ([arxiv.org](https://arxiv.org/abs/2303.06855))<br>• **Math object:** DAG scheduling with objective minimize peak live values (NP-hard; CP-SAT workable). ([arxiv.org](https://arxiv.org/abs/2303.06855))<br>• **Mechanism (IR+algo+prototype):** add an MLIR “reg-pressure model” analysis (approx live ranges) and a scheduler that (a) reorders within blocks, (b) chooses unroll factors/stage count; apply before lowering to NVVM/ROCDL. ([arxiv.org](https://arxiv.org/abs/2002.11054))<br>• **Hardware hook:** occupancy is bounded by registers; WGMMA/MFMA fragment choices change live values and pressure. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html))<br>• **Metric:** registers/thread, spills, occupancy, compile-time; **Risk:** scheduling for min-reg can hurt ILP or memory overlap. | **On-chip spill placement (shared/LDS spilling)** (change the spill target, not just count)<br>• **Why:** when spills are unavoidable, spilling to *on-chip* memory can beat default off-chip local memory; RegDem demonstrates this can improve performance by leveraging underutilized shared memory. ([arxiv.org](https://arxiv.org/abs/1907.02894))<br>• **Math object:** resource trade-off optimization (registers ↔ on-chip scratch ↔ bank conflicts).<br>• **Mechanism (IR+algo+prototype):** compiler-managed “spill buffers” in SMEM/LDS, with a heuristic or solver that decides (1) which values to spill, (2) placement to avoid bank conflicts; on AMD, consider VGPR/AGPR pressure and MFMA forms (LLVM is actively adding passes to choose AGPR vs VGPR forms). ([arxiv.org](https://arxiv.org/abs/1907.02894))<br>• **Hardware hook:** shared/LDS bank structure + occupancy constraints. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html))<br>• **Metric:** spill target distribution (global vs shared), bank conflict rate, occupancy; **Risk:** spilling to SMEM creates bank conflicts / reduces SMEM available for tiles. | **Hybrid (A+B): “reduce pressure first; spill smart only if needed”**<br>• **Hybrid:** run min-reg schedule + liveness shortening; if still above occupancy threshold, apply controlled shared/LDS spilling for a small set of values. ([arxiv.org](https://arxiv.org/abs/2303.06855))<br>• **IR extension:** attach per-op “expected reg footprint” metadata (from heuristics / backend feedback); add explicit `spill.alloc` regions in MLIR GPU kernels. ([arxiv.org](https://arxiv.org/abs/2002.11054))<br>• **Algorithm:** (1) schedule/unroll selection under reg budget, (2) spill selection + placement, (3) re-run bank-conflict model for spill traffic.<br>• **Prototype path:** MLIR: custom scheduling + shared allocation; Triton/CuTe: restrict unroll/stage knobs; LLVM: integrate with machine scheduling and address-space casts. ([research.ibm.com](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations))<br>• **Metric:** occupancy vs bank-conflict trade curve. | • Registers/thread & occupancy (per kernel)<br>• Spill count + spill target (local/global/shared)<br>• Code size; compile time<br>• Bank conflict rate after spilling | Shared/LDS spilling is a “second cliff”: can silently trade reg pressure for bank conflicts and lose. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html)) |
| **6) Compilation/search cost** (layout rewrite explosion) | **Equality saturation (e-graphs) with legality-aware analyses**<br>• **Why:** e-graphs compactly represent many equivalent layout expressions; `egg` adds extensibility via e-class analyses—exactly what legality/cost needs. ([popl21.sigplan.org](https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation))<br>• **Math object:** congruence closure over expressions (e-graph) + rewrite system.<br>• **Mechanism (IR+algo+prototype):** MLIR pass: lower layout expressions to an e-graph IR; run rewrite set (permute/reshape/split/merge); use e-class analysis to track legality invariants (alignment, stride mods, TMA admissibility); extract minimal-cost legal plan. ([popl21.sigplan.org](https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation))<br>• **Hardware hook:** legality predicates are from `CUtensorMap` + matrix descriptor bitfields. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Metric:** compile time, e-graph size, extracted code size; **Risk:** saturation blow-up without strong pruning. | **ISL/PRESBURGER canonicalization + memoization** (compute, don’t search)<br>• **Why:** the ISL-relations work gives algorithms for composition/inversion/complement across CuTe + Triton linear layouts; that’s a natural “canonical form + hash-cons” tool to reduce conversion blow-ups. ([arxiv.org](https://arxiv.org/abs/2511.10374))<br>• **Math object:** integer set relations; canonical forms under algebraic operations.<br>• **Mechanism (IR+algo+prototype):** build an `isl-layout` analysis pass that canonicalizes layouts and deduplicates conversions; maintain a memo table keyed by canonical ISL form; emit conversions only when canonical forms differ. ([arxiv.org](https://arxiv.org/abs/2511.10374))<br>• **Hardware hook:** use legality predicates as constraints during canonicalization (e.g., only consider swizzle modes that hardware supports). ([docs.nvidia.cn](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html))<br>• **Metric:** compile-time reduction vs baseline, #unique conversions; **Risk:** ISL worst-case complexity on pathological shapes. ([arxiv.org](https://arxiv.org/abs/2511.10374)) | **Hybrid (A+B): “e-graphs search; ISL merges/prunes”**<br>• **Hybrid:** use ISL canonical forms as an **e-class analysis** (merge equivalent layouts early), then cost-extract the best legal representative. ([popl21.sigplan.org](https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation))<br>• **IR extension:** a dedicated `layout` dialect whose ops round-trip to (a) e-graph terms and (b) ISL relations, plus legality attrs. ([arxiv.org](https://arxiv.org/abs/2511.10374))<br>• **Algorithm:** (1) saturate with bounded rewrites, (2) ISL-based merging + legality pruning, (3) multi-objective extraction (code size + bank conflicts + legality).<br>• **Prototype path:** MLIR: implement as a canonicalization+optimization pipeline feeding NVGPU/AMDGPU lowering; Triton: replace ad hoc layout conversion selection with extraction step. ([arxiv.org](https://arxiv.org/abs/2002.11054))<br>• **Metric:** compile-time vs quality curve (Pareto). | • Compile time (ms/op) & memory usage<br>• #layouts explored vs emitted<br>• Code size (inst count)<br>• “Fast-path hit rate” under legality constraints | Pruning must be legality-aware; otherwise compilation cost explodes or you extract illegal programs. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| **7) Dynamic memory management** (KV cache, fragmentation, paging) | **OS-style paging for KV cache (PagedAttention / vLLM)**<br>• **Why:** KV cache waste/fragmentation limits batching; paging-style block allocation attacks this directly. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **Math object:** page/block allocator + indirection tables (online allocation).<br>• **Mechanism (IR+algo+prototype):** runtime allocates KV blocks; compiler/kernel rewrites attention to gather/scatter through a block table; prototype in Triton or CUDA kernel library. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **Hardware hook:** HBM capacity/fragmentation pressure; kernel must tolerate non-contiguous physical layout.<br>• **Metric:** fragmentation %, max batch size under memory cap, p99 latency; **Risk:** kernel rewrite complexity & portability burden. | **Low-level GPU virtual memory management + demand paging (vAttention)**<br>• **Why:** vAttention keeps KV cache contiguous in virtual memory and uses CUDA VA APIs to map physical memory on demand, aiming to avoid kernel rewrites. ([microsoft.com](https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/))<br>• **Math object:** virtual memory mapping (VA reservation + page mapping) with access-control semantics.<br>• **Mechanism (IR+algo+prototype):** runtime library uses `cuMemCreate`, `cuMemAddressReserve`, `cuMemMap`, `cuMemSetAccess` to manage physical mapping under a stable virtual pointer; compiler keeps “flat pointer” ABI. ([developer.nvidia.com](https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/))<br>• **Hardware hook:** CUDA driver VA subsystem + OS paging behavior. ([developer.nvidia.com](https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/))<br>• **Metric:** allocator CPU time, page-fault rate, throughput/latency stability; **Risk:** driver/OS interaction variance across deployments. | **Hybrid (A+B): “choose paging strategy by platform contract”**<br>• **Hybrid:** prefer vAttention-style VA mapping when supported/stable; fall back to explicit PagedAttention layout when portability or determinism is required. ([microsoft.com](https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/))<br>• **IR extension:** `kv_cache` abstraction with two lowerings: (1) paged physical layout (block table), (2) contiguous VA with dynamic mapping.<br>• **Algorithm:** runtime policy selects allocator + exposes capability flags to compiler; compiler chooses kernel variant (flat vs paged).<br>• **Prototype path:** Triton: two attention kernels; MLIR: runtime callouts + memref address-space annotations; CuTe: pointer indirection adapter. ([arxiv.org](https://arxiv.org/abs/2002.11054))<br>• **Metric:** memory waste vs p99 latency trade-off. | • Fragmentation / memory waste %<br>• Allocation/deallocation overhead (CPU time)<br>• Page-fault rate (if applicable)<br>• p95/p99 latency under load | Portability cliff: behavior depends on CUDA driver VA support and OS paging performance; hard to make deterministic. ([developer.nvidia.com](https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/)) |

---

## Literature Scan (2019–2026)

1. **Lustig et al., “A Formal Analysis of the NVIDIA PTX Memory Consistency Model” (2019)** — formal semantics + Alloy/Coq pipeline you can reuse for async/barrier reasoning. ([research.nvidia.com](https://research.nvidia.com/publication/2019-04_formal-analysis-nvidia-ptx-memory-consistency-model))  
2. **Tillet et al., “Triton: An intermediate language and compiler for tiled neural network computations” (2019)** — tile-first IR + tile-level optimizations; matches the “memory wall first” reality. ([research.ibm.com](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations))  
3. **Lattner et al., “MLIR: A Compiler Infrastructure for the End of Moore’s Law” (2020)** — dialect + progressive lowering model that makes “IR extensions per bottleneck” feasible. ([arxiv.org](https://arxiv.org/abs/2002.11054))  
4. **Zheng et al., “Ansor” (OSDI 2020)** — hierarchical schedule space + evolutionary search + learned cost model; template for bandwidth-first tuning under hard constraints. ([usenix.org](https://www.usenix.org/conference/osdi20/presentation/zheng))  
5. **Willsey et al., “egg: Fast and Extensible Equality Saturation” (POPL 2021)** — e-graphs + e-class analyses are a direct fit for legality-aware layout rewrite search. ([popl21.sigplan.org](https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation))  
6. **Lehmann et al., “Flux: Liquid Types for Rust” (2022)** — modern refinement typing + inference that can inspire “descriptor legality as types” without drowning in annotations. ([arxiv.org](https://arxiv.org/abs/2207.04034))  
7. **Kwon et al., “vLLM / PagedAttention” (2023)** — OS paging analogy applied to KV cache; concrete runtime+kernel co-design target. ([arxiv.org](https://arxiv.org/abs/2309.06180))  
8. **Zhou et al., “Linear Layouts” (2025)** — layout as \(\mathbb{F}_2\) linear algebra, integrated into Triton backend; the most directly codegen-facing seed paper. ([arxiv.org](https://arxiv.org/abs/2505.23819))  
9. **Bhaskaracharya et al., “Modeling Layout Abstractions Using Integer Set Relations” (2025)** — ISL relations unify CuTe + Triton layouts and give algebraic ops (compose/invert) useful for compile-time reduction. ([arxiv.org](https://arxiv.org/abs/2511.10374))  
10. **Prabhu et al., “vAttention” (ASPLOS 2025)** — dynamic KV-cache memory management via CUDA virtual memory APIs; strong reference point for “runtime layer” bottleneck. ([microsoft.com](https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/))

---
Learn more:
1. [https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.9.1/parallel-thread-execution/index.html)
2. [https://mlir.llvm.org/docs/Dialects/NVGPU/](https://mlir.llvm.org/docs/Dialects/NVGPU/)
3. [https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.4.0/parallel-thread-execution/index.html)
4. [https://research.nvidia.com/publication/2019-04\_formal-analysis-nvidia-ptx-memory-consistency-model](https://research.nvidia.com/publication/2019-04_formal-analysis-nvidia-ptx-memory-consistency-model)
5. [https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations](https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations)
6. [https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html](https://docs.nvidia.cn/cuda/cuda-programming-guide/04-special-topics/async-copies.html)
7. [https://www.usenix.org/conference/osdi20/presentation/zheng](https://www.usenix.org/conference/osdi20/presentation/zheng)
8. [https://arxiv.org/abs/2006.06762](https://arxiv.org/abs/2006.06762)
9. [https://docs.nvidia.com/cuda/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
10. [https://arxiv.org/abs/2002.11054](https://arxiv.org/abs/2002.11054)
11. [https://arxiv.org/abs/2207.04034](https://arxiv.org/abs/2207.04034)
12. [https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/writing-cuda-kernels.html)
13. [https://arxiv.org/abs/2505.23819](https://arxiv.org/abs/2505.23819)
14. [https://arxiv.org/abs/2511.10374](https://arxiv.org/abs/2511.10374)
15. [https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/conceptual/local-data-share.html](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/conceptual/local-data-share.html)
16. [https://rocm.blogs.amd.com/software-tools-optimization/lds-bank-conflict/README.html](https://rocm.blogs.amd.com/software-tools-optimization/lds-bank-conflict/README.html)
17. [https://arxiv.org/abs/2303.06855](https://arxiv.org/abs/2303.06855)
18. [https://arxiv.org/abs/1907.02894](https://arxiv.org/abs/1907.02894)
19. [https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation](https://popl21.sigplan.org/details/POPL-2021-research-papers/23/egg-Fast-and-Extensible-Equality-Saturation)
20. [https://arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180)
21. [https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/](https://www.microsoft.com/en-us/research/publication/vattention-dynamic-memory-management-for-serving-llms-without-pagedattention/)
22. [https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/](https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/)