You asked for a forensic hardware-mapping audit of **“PL_proposal.tex / TITAN”**, but the document content you provided is titled:

> **“LegalEGraph: Legality-Aware Equality Saturation for Layout and Asynchronous Schedule Co-Optimization on Modern GPUs”** (IEEEtran).

I will treat the pasted LaTeX as the proposal under review (date of audit: **2026-01-29**). Where your verification targets mention **proxy fences** and **AMD MI300/CDNA3**, I audit whether the proposal *covers them*; when it doesn’t, I flag that omission explicitly.

---

## Claim Verification Matrix

| Proposal_Claim | Hardware_Reality (PTX/CDNA3) | Verdict (Valid/Oversimplified/False) | Citation |
|---|---|---|---|
| TMA tensor-map creation via `cuTensorMapEncodeTiled` requires `tensorRank ≤ 5`. | CUDA Driver API requires `tensorRank` non-zero and ≤ 5; additionally, constraints apply depending on `interleave` (e.g., if interleave is not NONE, `tensorRank` must be ≥ 3). Some documentation variants also state ranks 3–5. | Oversimplified | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| TMA descriptor object (`tensorMap`) must be 64B-aligned. | Driver API: `tensorMap` address must be aligned to 64 bytes. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| TMA `globalAddress` must be 16B-aligned (often 32B in some modes). | Driver API: `globalAddress` must be 16B-aligned; must be 32B-aligned when `interleave` is 32B (and for certain packed types). | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| TMA `globalStrides` must be multiples of 16 and `< 2^40`. | Driver API: strides must be multiples of 16 and `< 2^40`; also must be multiples of 32 under `interleave=32B` (and certain packed types). | Oversimplified | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| TMA `boxDim ≤ 256`. | Driver API: each `boxDim[i]` must be non-zero and ≤ 256. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| TMA `elementStrides ≤ 8`. | Driver API: each `elementStrides[i]` non-zero and ≤ 8. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| “Additional swizzle/interleave coupling and inner-dimension bounds” affect legality. | Driver API encodes swizzle/interleave constraints (e.g., bounding-box inner dimension vs swizzle size), and the CUDA programming guide imposes additional swizzle/inner-dimension validity constraints. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| Proposal accounts for TMA swizzle constraints such as **global alignment 128B**, shared alignment requirements, and “instruction invalid if inner dimension requirements are violated.” | CUDA programming guide (CC 9.x) explicitly states global memory must be aligned to 128B for TMA swizzle patterns; shared-memory alignment and inner-dimension requirements are stated; violating inner-dimension requirements makes the instruction invalid. Note: for `SWIZZLE_NONE`, global alignment shown as 16B. | Oversimplified (needs swizzle-mode conditioning) | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.1.0/cuda-programming-guide/04-special-topics/async-copies.html)) |
| Bulk async-group non-ordering: “no memory ordering guarantee between two `cp.async.bulk.*` operations within the same group.” | PTX explicitly states no memory ordering guarantee between any two `cp{.reduce}.async.bulk.{.prefetch}{.tensor}` operations in the same bulk async-group. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| `cp.async.bulk.commit_group` / `wait_group` exist and are per-thread bulk async-group mechanisms. | PTX defines `cp.async.bulk.commit_group` creating a per-thread bulk async-group and `cp.async.bulk.wait_group` waiting until only N most-recent groups are pending. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| WGMMA descriptor quantization: `matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4` (16B granularity). | PTX “Matrix Descriptor Format” defines exactly this encoding and uses it for start/leading/stride offsets. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| WGMMA descriptor has enumerated swizzle modes and disallows invalid bit patterns. | PTX shared-memory descriptor swizzle field lists valid values and notes invalid encodings (e.g., values 3, 5, 7 invalid). (Separately, another matrix-descriptor format uses a 2-bit field with values 0–3.) | Valid (but descriptor-format-specific) | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| WGMMA descriptors must be warpgroup-uniform (“identical across all warps in a warpgroup”). | PTX states contents of a matrix descriptor must be same across all warps in the warpgroup. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| WGMMA shared-memory multiplicand addresses must be 16B-aligned; non-16B offsets break encodability. | PTX states shared-memory matrix starting addresses must be aligned to 16 bytes; descriptor-encoded start/offsets are 16B-aligned via the encode rule. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| mbarrier phase completion requires both pending arrivals == 0 and tx-count == 0. | PTX “Phase Completion of the mbarrier object”: completion requires pending arrivals reach zero *and* tx-count reach zero. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| expect-tx increments tx-count; complete-tx decrements; mismatches can deadlock. | PTX: expect-tx increases tx-count; complete-tx decrements; phase cannot complete until tx-count reaches 0 ⇒ a missing decrement can prevent phase completion (hang). | Valid (deadlock is a direct consequence) | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| “NVGPU tokenized async/barrier modeling” maps to hardware (`cp.async.bulk`, `mbarrier`, TMA, WGMMA). | MLIR NVGPU dialect explicitly models mbarrier ops (including arrive.expect_tx), TMA descriptor creation + async load/store, and warpgroup MMA that lowers to NVVM/PTX wgmma. | Valid | ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/)) |
| `nvgpu.tma.create.descriptor` calls CUDA Driver `cuTensorMapEncodeTiled`. | NVGPU dialect doc states exactly that. | Valid | ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/)) |
| `nvgpu.mbarrier.arrive.expect_tx` correctly models “expect-tx increases tx-count.” | NVGPU dialect op description matches PTX “expect-tx operation” semantics. | Valid | ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/)) |
| `nvgpu.tma.async.load` uses mbarrier-based completion. | NVGPU doc: tma.async.load uses an mbarrier completion mechanism; PTX tensor bulk copy variants support `.mbarrier::complete_tx::bytes`. | Valid | ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/NVGPU/)) |
| Proposal assumes “schedule legality = SSA tokens + waits/barriers” is sufficient for correctness. | PTX introduces **proxy domains**: `cp{.reduce}.async.bulk` and `wgmma.mma_async` execute in the **async proxy**, and the tensor map is accessed via a **tensormap proxy**; cross-proxy ordering may require `fence.proxy.async` and/or `fence.proxy.tensormap::generic` patterns. Tokens alone do not encode proxy-domain visibility. | False (missing required dimension of legality) | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| (Proxy fences) `fence.proxy.async` is required only in rare cases; completion implies ordering. | PTX: accessing the same memory location across generic and async proxies “needs a cross-proxy fence”; completion of async ops is followed by an implicit generic-async proxy fence *only after completion is observed*. That does not replace all generic→async ordering needs. | Oversimplified | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| WGMMA availability is a Hopper feature gated by target notes (e.g., `sm_90a`). | PTX target notes for `wgmma.mma_async` require `sm_90a`. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)) |
| Tensor map objects are only supported on compute capability 9.0+ devices. | CUDA Driver API: tensor map objects are only supported on devices of compute capability 9.0 or higher. | Valid | ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) |
| AMD portability target: can AMD `s_waitcnt` implement a per-transfer “token wait”? | AMD MI300/CDNA3 ISA defines `S_WAITCNT` as waiting for **counts of outstanding** VM / export / LGKM operations to be at/below thresholds; it is inherently counter-based, not a capability handle to a specific transfer. | False (token abstraction mismatch unless restricted/serialized) | ([amd.com](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf)) |

---

## Hardware Mismatch List (what the proposal misses or blurs)

1. **Missing explicit proxy-domain legality (Generic vs Async vs Tensormap proxies).**  
   The proposal’s legality domains cover: TMA descriptor admissibility, WGMMA descriptor encodability, and mbarrier protocol. What’s absent is the *proxy* dimension, despite PTX making it explicit that:
   - bulk async copy ops run in the **async proxy** and require cross-proxy fences when the same memory is accessed across proxies; and
   - tensor maps are accessed in the **tensormap proxy** (separate from generic), with explicit `fence.proxy.tensormap::generic` patterns available.  
   If LegalEGraph rewrites schedules/layouts across operations that mix proxies (e.g., generic shared-memory stores feeding `wgmma`, or dynamic tensor-map updates), correctness becomes dependent on inserting the right proxy fences—something not modeled in the term language or the legality lattice as written. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma))

2. **Barrier “token granularity” is not free: it implies shared-memory allocation + lifecycle constraints.**  
   PTX defines an `mbarrier` object as a `.b64` object in shared memory with 8-byte alignment. More barriers (to match more “tokens”/pipeline stages) consume shared memory and require initialization; additionally, PTX imposes phase discipline (must observe completion via test/try-wait returning true before advancing phases). The proposal’s token phrasing risks implying “virtual” tokens; hardware requires concrete objects. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma))

3. **TMA legality conflation risk: descriptor admissibility vs instruction-level swizzle validity.**  
   The driver API constraints (rank/stride/boxDim/elementStrides/etc.) are necessary, but the CUDA programming guide adds instruction-level constraints for swizzle patterns (128B alignment, inner-dimension bounds, “instruction invalid” if violated). The proposal mentions these in prose, but they are not concretely spelled out in the analysis invariants list in Table 1 beyond “inner-dimension bounds.” A reviewer should demand that these be first-class hard predicates in `A_TMA`, not “practical constraints.” ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))

4. **AMD portability is not addressed in the proposal, and the abstraction likely does not port without semantic weakening.**  
   Your prompt asks about `s_waitcnt` vs tokens: AMD `S_WAITCNT` is explicitly counter-based. A capability-style “wait on *this* transfer” token is not directly representable unless the compiler constrains itself to a monotonic “wait for all prior ops of class X” discipline (often performance-negative) or introduces additional bookkeeping and serialization. ([amd.com](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf))

---

## Fatal Flaws (abstractions the hardware cannot support efficiently / safely)

1. **Proxy-fence omission is a correctness hole, not a micro-optimization detail.**  
   If LegalEGraph claims “never emits illegal / unsafe behavior” but does not model the proxy system, it can still emit *functionally incorrect* code when schedule/layout rewrites cross proxy boundaries (generic ↔ async; generic ↔ tensormap). PTX is explicit that cross-proxy access to the same location needs a cross-proxy fence, and that tensor maps are accessed via tensormap proxy. Without a proxy-aware typestate/analysis, the “hard legality guarantee” is not defensible. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma))

2. **If you claim AMD portability with token-level waits, that claim is (as stated) not implementable: `S_WAITCNT` is counter-based.**  
   AMD MI300/CDNA3 defines `S_WAITCNT` in terms of thresholds on outstanding operation counts (VM / export / LGKM), not a handle to a particular async event. That is an impedance mismatch with a capability-token abstraction unless you accept coarser synchronization (waiting for “all prior of type”), which can erase the scheduling advantages that tokens are meant to preserve. ([amd.com](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf))

3. **Potential “invalid instruction” generation if TMA swizzle constraints aren’t enforced as hard legality.**  
   The CUDA programming guide states TMA swizzle patterns have strict alignment and inner-dimension constraints and that violating inner-dimension requirements makes the instruction invalid. If `A_TMA` only checks driver-API descriptor creation constraints and does not also enforce the instruction-level swizzle validity conditions, the system can still emit invalid PTX/SASS paths. The proposal gestures at this, but doesn’t pin it down as a non-negotiable predicate in the formalism section. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.1.0/cuda-programming-guide/04-special-topics/async-copies.html))

---

### What I would require in a revision (microarchitecture-reviewer stance)

- Add an explicit **proxy/visibility legality domain** (call it \(A_{\textsc{proxy}}\)) that tracks the last-writer/reader proxy per memory region (at least: generic vs async vs tensormap) and can prove when `fence.proxy.async` / `fence.proxy.tensormap` is required or can be elided. Ground it directly in PTX proxy semantics and fence patterns. ([docs.nvidia.com](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma))  
- Make the **TMA swizzle validity constraints** explicit in the hard legality predicates (not just cited as “practical constraints”). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.1.0/cuda-programming-guide/04-special-topics/async-copies.html))  
- If AMD portability is in scope, redefine the token abstraction as **sequence-number / counter-threshold tokens** (i.e., “wait until counters ≤ captured value”), and be honest that this is not “wait on a specific transfer.” Otherwise, drop the portability claim. ([amd.com](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf))

---
Learn more:
1. [https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/archive/12.4.0/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
2. [https://docs.nvidia.com/cuda/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
3. [https://docs.nvidia.com/cuda/archive/13.1.0/cuda-programming-guide/04-special-topics/async-copies.html](https://docs.nvidia.com/cuda/archive/13.1.0/cuda-programming-guide/04-special-topics/async-copies.html)
4. [https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma)
5. [https://mlir.llvm.org/docs/Dialects/NVGPU/](https://mlir.llvm.org/docs/Dialects/NVGPU/)
6. [https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf](https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf)
