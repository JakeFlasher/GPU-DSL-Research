I’m treating the “top bottlenecks from Stage 1” as the ranked **Elephant‑in‑the‑room diagnosis** list in your `stage1_output.md` (#1–#5), plus the cross‑vendor mismatch item (#6). If you intended a different bottleneck list, paste it and I’ll re-key the table.

## Toolbox

| Bottleneck | Theory_A (why it fits) | Theory_B (why it fits) | Prototype_Mechanism (MLIR/Triton/CuTe) | Key_Metrics | Main_Risk |
|---|---|---|---|---|---|
| **1) Latency hiding / async orchestration** (`cp.async.bulk.tensor` completion via `mbarrier::complete_tx::bytes`) ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)) | • **Math object / Why:** **Session types / typestate protocols** (finite‑state “protocols” + linearity) model “issue→complete→consume” pipelines and can enforce progress/safety properties for async protocols. ([arxiv.org](https://arxiv.org/abs/1908.02940))<br>• **Mechanism (IR+algo+hook):** **IR:** introduce `!gpu.mbarrier<state>` + `gpu.tma.async_copy`/`gpu.mbarrier.wait` ops that consume/produce stateful barrier tokens; lower through MLIR `async.execute/async.await` to keep dependencies explicit. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/AsyncDialect/))<br>• **Risk:** Too‑strict protocol typing can block legal but tricky reorderings (e.g., multi‑consumer patterns) unless you add subtyping/“bounded asynchrony” escapes. ([arxiv.org](https://arxiv.org/abs/1908.02940))<br>• **Metric:** **Barrier correctness + stalls**: (i) static “no unmatched wait/arrive” proofs (compile‑time), (ii) dynamic wait time (stall cycles attributable to barrier waits), (iii) code size impact from explicit tokens. | • **Math object / Why:** **Space‑time scheduling** (polyhedral/ILP or modulo scheduling) treats async copies and compute as nodes with dependences; “time” is a schedule you can optimize. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **Mechanism (IR+algo+hook):** **IR:** represent copies as “DMA‑like” ops and run a loop‑pipelining transform (preload/prologue/steady/epilogue) akin to MLIR’s pipelining of `dma_start/dma_wait`, but targeting TMA+`mbarrier`. ([mlir.llvm.org](https://mlir.llvm.org/docs/Passes/))<br>• **Risk:** Hardware latency/resource models (in‑flight depth, barrier capacity, scoreboard interactions) can be wrong; then the schedule optimizer confidently picks a bad pipeline. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html))<br>• **Metric:** **Overlap quality**: achieved copy/compute overlap (timeline), register pressure delta, and compilation time of scheduling (solver time). | • **Hybrid:** “**Schedule then type‑check**”: use the scheduler to propose a pipelined program; then run a session‑type/typestate verifier to guarantee the `mbarrier` protocol is well‑formed. ([arxiv.org](https://arxiv.org/abs/1908.02940))<br>• **MLIR path:** add a small **GPU‑AsyncPipeline** dialect (or extend GPU dialect) + lowering to MLIR `async` dialect for explicit deps, then to NVVM/PTX. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/AsyncDialect/))<br>• **Triton path:** add a `ttg.async_stage` region construct (steady‑state loop + prologue/epilogue) in the GPU backend; lower to `cp.async.bulk.tensor` + `mbarrier` sequences. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html))<br>• **CuTe path:** encode pipeline stages in CuTe’s epilogue/mainloop policy objects; keep legality checked host‑side (descriptor) and protocol checked compile‑time (C++ types). ([docs.nvidia.com](https://docs.nvidia.com/cutlass/4.3.0/overview.html)) | • Barrier wait stall cycles (NVIDIA) / equivalent “wait” stalls<br>• In‑flight stage count vs achieved overlap<br>• Register pressure / occupancy change<br>• Compile time: scheduler/verification walltime | Modeling mismatch (scheduler) or over‑constraining (typing) prevents peak schedules. |
| **2) Descriptor legality cliffs** (CuTensorMap / TMA descriptor constraints gate fast path) ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) | • **Math object / Why:** **Presburger sets & integer relations (ISL)** naturally encode “alignment/stride multiples/box bounds/swizzle constraints” as constraint systems over layout parameters, matching Seed‑2’s representation. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **Mechanism (IR+algo+hook):** **IR:** attach `layout.isl_relation` attrs to memrefs/tiles; add `gpu.tma.descriptor` op whose verifier is “ISL‑legal ∧ CUDA‑legal”. **Algo:** intersect candidate layout relation with the CuTensorMap legality polyhedron; project to feasible params; pick lexicographic‑min padding. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **Risk:** Seed‑2 explicitly notes worst‑case exponential cost for relation ops (composition/lexmin), so naïve use can dominate compile time. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **Metric:** **Legality hit‑rate** (fraction of candidates that survive), ISL time/memory, and “fast‑path availability” (% kernels that can emit TMA vs fallback). ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | • **Math object / Why:** **Equality saturation (e‑graphs)** is ideal when legality is a *property of an expression* after many rewrites (pad/reshape/swap dims); you want to explore many equivalent layouts without phase ordering. ([arxiv.org](https://arxiv.org/abs/2004.03082))<br>• **Mechanism (IR+algo+hook):** **IR:** use MLIR **`eqsat` dialect** to keep the e‑graph in‑IR; encode layout rewrites; use e‑class analyses to compute `(alignment mod 16/32, stride mod 16/32, boxDim bounds, swizzle enum feasibility)` and a boolean `tma_legal`. ([arxiv.org](https://arxiv.org/abs/2505.09363))<br>• **Risk:** E‑graph growth: without guidance, saturation can explode; use sketch‑guidance / rule budgeting when needed. ([arxiv.org](https://arxiv.org/abs/2111.13040))<br>• **Metric:** Peak e‑graph size, extraction time, number of legal variants found, and rate of “false‑legal” (must still validate against CUDA encode rules). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)) | • **Hybrid:** “**ISL‑filtered e‑graphs**”: use ISL to precompute a *feasible parameter region*; run eqsat/egg only inside that region (or use ISL as an e‑class analysis oracle). ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **MLIR path:** (1) lower layout ops to `eqsat` terms, (2) run saturation with legality analyses, (3) extract best `tma_legal` form, (4) lower to CUDA driver API tensor map encodes + PTX TMA ops. ([arxiv.org](https://arxiv.org/abs/2505.09363))<br>• **Triton path:** integrate legality analysis into `convert_layout` selection so “best math layout” that is descriptor‑illegal is never generated. ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br>• **CuTe path:** add compile‑time `is_tma_legal(Layout, Swizzle, Interleave, BoxDim, ElementStrides)` trait + optional eqsat‑driven search to pick a legal `TiledCopy`. ([prateekshukla1108.github.io](https://prateekshukla1108.github.io/cutlass3/docs/cpp/cute/0z_tma_tensors.html)) | • % candidate layouts rejected by legality<br>• ISL/eqsat compile time + memory peak<br>• “TMA fast‑path” coverage (% kernels emitting TMA)<br>• Code size (number of specialized variants) | Search/solve blow‑ups, or incorrect/over‑approx legality analyses. |
| **3) Bandwidth wall + on‑chip tiering (SMEM vs TMEM) ⇒ data movement dominates** ([developer.nvidia.com](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/)) | • **Math object / Why:** **I/O‑complexity / communication‑avoiding** analysis targets the real limiter when compute scales faster than bandwidth; FlashAttention is the archetype of IO‑aware tiling for exact attention. ([arxiv.org](https://arxiv.org/abs/2205.14135))<br>• **Mechanism (IR+algo+hook):** **IR:** add explicit memory‑space annotations (HBM/L2/SMEM/TMEM) and “reuse contracts” on tiles; **Algo:** choose multi‑level tiling that minimizes HBM traffic subject to SMEM/TMEM capacity; **Hook:** use Blackwell TMEM as “SRAM‑like” tier (256 KB/SM) and prefer keeping intermediates there. ([developer.nvidia.com](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/))<br>• **Risk:** Cost models may ignore instruction shape constraints (tcgen05 shapes) and lane restrictions; then “IO‑optimal” tiles are not executable. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html))<br>• **Metric:** Achieved HBM bandwidth, bytes moved per output (or per FLOP), TMEM residency/utilization proxy, and conversion traffic (extra SMEM moves). ([developer.nvidia.com](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/)) | • **Math object / Why:** **Refinement/ownership types** for *placement + collective access* match TMEM’s semantics: (i) warpgroup‑partitioned lane access, (ii) `tcgen05.st` requires warp‑uniform `taddr` or UB. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html))<br>• **Mechanism (IR+algo+hook):** **IR:** extend layout types with `{space=TMEM, ownerWarp∈{0..3}, laneRange}`; verifier enforces lane ownership + “collective address invariants”. Seed‑3’s **tractable layouts + non‑degeneracy** gives you a manageable, canonical layout subset to attach these refinements to. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1))<br>• **Risk:** Type system complexity + engineering cost; may require explicit annotations when inference is hard (e.g., mixed warpgroup patterns). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html))<br>• **Metric:** (i) number of prevented UB patterns (static rejects), (ii) number of forced fallbacks to SMEM, (iii) code size increase from specialized tcgen05 shapes. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)) | • **Hybrid:** “**IO‑aware tiling with typed placement**”: tiling decides *what* lives in TMEM; the type/refinement system guarantees *who can touch it* + *which tcgen05 shapes are legal*. ([arxiv.org](https://arxiv.org/abs/2205.14135))<br>• **MLIR path:** add a `gpu.tmem` memory space + `gpu.tcgen05.{ld,st}` ops with verifier rules (`taddr` uniform, lane partition). Lower from Linalg/Vector to these ops under a “tmem‑eligible” pattern set. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html))<br>• **Triton path:** extend layout IR (Seed‑1 style) with a `placement` dimension (reg/smem/tmem) and lane ownership constraints; cost model prioritizes reduced HBM moves. ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br>• **CuTe path:** add a “TMEM‑safe” layout subcategory: only construct layouts whose strides/shapes map to legal tcgen05 access patterns and respect non‑degeneracy. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) | • Achieved bandwidth / bytes moved<br>• TMEM/SMEM traffic split (proxy via instruction counts)<br>• Conversion count (layout converts, extra staging ops)<br>• UB prevented (static rejects) | Mis-specified cost model or too‑rigid typing blocks high‑performance (but legal) kernels. |
| **4) Compilation/search cost** (exact ISL/relations can be exponential; layout search explodes) ([arxiv.org](https://arxiv.org/html/2511.10374v1)) | • **Math object / Why:** **Abstract interpretation over congruences + intervals** (a fast lattice domain) approximates the key legality facts: alignment modulo 16/32, stride divisibility, boxDim bounds, power‑of‑two requirements. This is explicitly motivated by Seed‑1 pow‑2 limits and Seed‑2 complexity warnings. ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br>• **Mechanism (IR+algo+hook):** **IR:** attach `layout.summary` attrs (alignment class, stride mods, rank bounds, “maybe_tma_legal”) to memrefs/tiles; **Algo:** forward dataflow + meet/join to quickly prove “definitely illegal” before invoking ISL/eqsat/SMT. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))<br>• **Risk:** Over‑approximation can (a) falsely reject good layouts (false negatives) or (b) allow too many (false positives), pushing work downstream. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **Metric:** Compile wall‑time reduction, number of expensive solver calls avoided, and “false prune rate” measured against an exhaustive baseline on small shapes. | • **Math object / Why:** **Hierarchical search + learned cost models** (Ansor) is a proven strategy to explore huge schedule/layout spaces without enumerating everything; aligns with your need to manage “temporal + spatial” knobs jointly. ([arxiv.org](https://arxiv.org/abs/2006.06762))<br>• **Mechanism (IR+algo+hook):** **IR:** define a parametric schedule/layout space (tile sizes, swizzle, pipeline depth, memory tier choices); **Algo:** sample + evolutionary search + learned cost model; **Hook:** incorporate non‑speed metrics like barrier stalls / bandwidth into the model’s loss. ([arxiv.org](https://arxiv.org/abs/2006.06762))<br>• **Risk:** Needs measurement and good feature engineering; may not generalize across architectures or kernel families. ([arxiv.org](https://arxiv.org/abs/2006.06762))<br>• **Metric:** Tuning time (end‑to‑end), number of compiled candidates, model prediction error, and reproducibility/variance across runs. ([arxiv.org](https://arxiv.org/abs/2006.06762)) | • **Hybrid:** “**Static prune → learned search**”: use abstract interpretation to shrink the space; then run Ansor‑like search only within “maybe‑legal” regions, caching per (arch, dtype, shape, legality summary). ([arxiv.org](https://arxiv.org/abs/2006.06762))<br>• **MLIR path:** implement as a Transform‑dialect‑driven pipeline: transform scripts generate variants; a driver benchmarks; results feed back into selection. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/Transform/))<br>• **Triton path:** extend the autotuner to treat **pipeline depth + descriptor params** as first‑class knobs; cache legality summaries so you don’t re‑solve. ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br>• **CuTe path:** integrate “legal‑by‑construction” templates + small bounded search; avoid calling heavy relation engines unless necessary. ([docs.nvidia.com](https://docs.nvidia.com/cutlass/4.3.0/overview.html)) | • Compile time (p50/p95), peak memory<br>• # candidates compiled/benchmarked<br>• Cache hit rate across runs<br>• Model error vs measured metrics | Either missing good configs (over‑prune) or paying too much tuning cost (under‑prune). |
| **5) Dynamic memory management (KV cache / fragmentation)** ([arxiv.org](https://arxiv.org/abs/2309.06180)) | • **Math object / Why:** **Paging / virtual memory model**: KV cache becomes a page‑table mapping from logical sequence positions to physical blocks, minimizing fragmentation; this is exactly PagedAttention/vLLM’s approach. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **Mechanism (IR+algo+hook):** **IR/runtime:** add a `kv.page_table` ABI + paged KV tensor abstraction; kernels do page‑table address translation + coalesced gathers; compiler ensures page‑aligned block layouts. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **Risk:** Indirection overhead + more irregular memory access; requires purpose‑built attention kernels (layout‑aware, page‑aware). ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **Metric:** Memory waste/fragmentation %, max batch size at fixed memory, alloc/free overhead, and tail latency under churn. ([arxiv.org](https://arxiv.org/abs/2309.06180)) | • **Math object / Why:** **Stream‑ordered regions / memory pools**: allocations are ordered by stream (a temporal logic), enabling fast alloc/free and pooling; CUDA explicitly supports this via `cudaMallocAsync` and memory pools. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/stream-ordered-memory-allocation.html))<br>• **Mechanism (IR+algo+hook):** **IR/runtime:** expose `gpu.alloc_async`/`gpu.free_async` tied to stream tokens; map to `cudaMallocAsync`/`cudaFreeAsync` using a configurable `cudaMemPool_t`. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/stream-ordered-memory-allocation.html))<br>• **Risk:** Cross‑stream use hazards: CUDA docs note undefined behavior if consumers access allocations without ordering guarantees (events/sync needed). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-programming-guide/index.html))<br>• **Metric:** Allocation latency, host synchronization count avoided, pool reuse rate, and peak reserved bytes vs active bytes. ([docs.nvidia.com](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/stream-ordered-memory-allocation.html)) | • **Hybrid:** “**Paged KV on top of stream‑ordered pools**”: pages are the unit of allocation from the mempool; paging gives near‑zero waste; stream ordering keeps alloc/free cheap. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **MLIR path:** model page table + pooled allocation in the runtime dialect; add verifier that enforces stream‑ordering edges when a pointer crosses streams. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-programming-guide/index.html))<br>• **Triton path:** generate paged attention kernels (explicit page table loads); autotune page/block sizes for coalescing. ([arxiv.org](https://arxiv.org/abs/2309.06180))<br>• **CuTe path:** provide page‑block layouts as CuTe tensors with explicit stride/alignment so descriptor legality + coalescing are preserved. ([docs.nvidia.com](https://docs.nvidia.com/cutlass/4.3.0/overview.html)) | • Memory waste % / fragmentation<br>• Max concurrency at fixed HBM<br>• Alloc/free overhead (GPU+CPU time)<br>• Tail latency (p95/p99) under churn | Complexity explosion at the runtime/ABI boundary; easy to introduce ordering bugs across streams. |
| **6) Cross‑vendor semantic mismatch** (NVIDIA TMA/TMEM vs AMD LDS/MFMA constraints differ) ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html)) | • **Math object / Why:** **Parametric semantics via compiler “interfaces”**: treat each GPU family as an instance of a semantic/cost “algebra” plugged into the same layout+pipeline optimizer. (This matches MLIR’s dialect/interface philosophy and multi‑backend lowering.) ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/Transform/))<br>• **Mechanism (IR+algo+hook):** **IR:** add a `layout.target_model` interface with methods: `bank_map(op,width)`, `operand_distribution(op)`, `legal_shapes(op)`, `legal_descriptor(layout)`; **Algo:** run the same passes but parameterized by the model; **Hook:** MLIR already provides target-specific conversion patterns (e.g., GPU→ROCDL). ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/Transform/))<br>• **Risk:** Keeping models faithful across rapidly changing GPU ISAs (Blackwell SM100 vs Hopper SM90 vs CDNA3) is ongoing work. ([developer.nvidia.com](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/))<br>• **Metric:** Portability coverage (# kernels that compile+run on both), compile-time per target, and “semantic divergence bugs” found by differential testing. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/Transform/)) | • **Math object / Why:** **Instruction-granularity constraints + bank-conflict cost functions** are first-order: AMD LDS conflicts are phase-based for `ds_*_b128`, and MFMA operand distribution + VGPR/AGPR limits matter; NVIDIA has TMEM lane restrictions + tcgen05 collective address rules. ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html))<br>• **Mechanism (IR+algo+hook):** **IR:** annotate loads/stores with “access granularity” and “distribution group” (warp/warpgroup/wavefront). **Algo:** per-target simulation/analysis: (AMD) phase-group bank conflict estimator; (NVIDIA) lane-ownership + tcgen05 shape legality checks. ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html))<br>• **Risk:** Cost models can become too detailed (SASS-level) and expensive; you need a tiered approach (cheap approximations, expensive validation). ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html))<br>• **Metric:** Bank-conflict rate (per phase on AMD), register pressure (VGPR/AGPR usage), and frequency of legality fallbacks (e.g., SMEM instead of TMEM/TMA). ([rocm.docs.amd.com](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html)) | • **Hybrid:** Target interface (A) supplies the per-instruction constraint/cost analyzers (B), then you can reuse search engines (ISL/eqsat/autotune) across vendors. ([arxiv.org](https://arxiv.org/html/2511.10374v1))<br>• **MLIR path:** implement `TargetModel` as a dialect interface + Transform dialect scripts selecting NVVM vs ROCDL lowering pipelines; share mid-level layout IR. ([mlir.llvm.org](https://mlir.llvm.org/docs/Dialects/Transform/))<br>• **Triton path:** make the layout IR parametric over target “bank + distribution” parameters; keep Seed‑1’s bit‑level layout machinery but add target-specific legality/cost predicates. ([arxiv.org](https://arxiv.org/html/2505.23819v3))<br>• **CuTe path:** define subcategories like “MFMA-feasible” and “tcgen05-feasible” layouts; reuse categorical normalization (Seed‑3) inside each subcategory for tractable compile-time reasoning. ([arxiv.org](https://arxiv.org/pdf/2601.05972v1)) | • Bank-conflict metrics (NVIDIA + AMD)<br>• Register pressure (VGPR/AGPR) / occupancy<br>• Compile-time per target backend<br>• Differential-testing failure rate | Model drift + engineering overhead; without discipline you build two compilers accidentally. |

---

## Literature Scan (2019–2026)

1. **Zhou et al. (ASPLOS 2026), “Linear Layouts”** — bit‑level layout algebra over $$\mathbb{F}_2$$ + closure results; useful as the *spatial* core IR we extend with legality/time. ([arxiv.org](https://arxiv.org/html/2505.23819v3))  
2. **Bhaskaracharya et al. (2025), ISL integer‑set relations for layouts** — unified relation model for CuTe + Triton layouts; explicitly notes worst‑case exponential ops (guides compile‑time strategy). ([arxiv.org](https://arxiv.org/html/2511.10374v1))  
3. **Carlisle et al. (Jan 2026), “Categorical Foundations for CuTe Layouts”** — tractable layout subcategory + encoding/iff characterization + non‑degeneracy (good for “legal-by-construction” subsets). ([arxiv.org](https://arxiv.org/pdf/2601.05972v1))  
4. **NVIDIA CUDA Driver API (CUDA 13.0.1, updated Sept 4 2025), Tensor Map Object Management** — authoritative descriptor legality constraints for CuTensorMap/TMA. ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html))  
5. **NVIDIA PTX ISA docs (CUDA 12.x archive)** — defines `cp.async.bulk.tensor` completion via `mbarrier::complete_tx::bytes` and TMEM `tcgen05` access restrictions (your correctness/perf “contracts”). ([docs.nvidia.com](https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html))  
6. **NVIDIA (2025), “Inside NVIDIA Blackwell Ultra…”** — states compute/bandwidth/tiering facts (e.g., 256 KB TMEM per SM, 8 TB/s HBM), motivating IO/placement theories. ([developer.nvidia.com](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/))  
7. **Merckx et al. (May 2025), “eqsat: An Equality Saturation Dialect for Non‑destructive Rewriting”** — makes equality saturation *native in MLIR*, directly enabling legality‑aware rewrite search without external tooling. ([arxiv.org](https://arxiv.org/abs/2505.09363))  
8. **Zheng et al. (OSDI 2020), “Ansor”** — hierarchical schedule search + learned cost model; blueprint for controlling compilation/search cost. ([arxiv.org](https://arxiv.org/abs/2006.06762))  
9. **Dao et al. (2022), “FlashAttention”** — exemplifies IO‑aware, communication‑avoiding design; provides an IO‑complexity framing that can be compiled into tiling/placement. ([arxiv.org](https://arxiv.org/abs/2205.14135))  
10. **Kwon et al. (2023), “PagedAttention / vLLM”** — paging-based KV cache management that targets fragmentation/waste; directly relevant to runtime/ABI mechanisms. ([arxiv.org](https://arxiv.org/abs/2309.06180))

---
Learn more:
1. [https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.3.2/parallel-thread-execution/index.html)
2. [https://arxiv.org/abs/1908.02940](https://arxiv.org/abs/1908.02940)
3. [https://mlir.llvm.org/docs/Dialects/AsyncDialect/](https://mlir.llvm.org/docs/Dialects/AsyncDialect/)
4. [https://arxiv.org/html/2511.10374v1](https://arxiv.org/html/2511.10374v1)
5. [https://mlir.llvm.org/docs/Passes/](https://mlir.llvm.org/docs/Passes/)
6. [https://docs.nvidia.com/cutlass/4.3.0/overview.html](https://docs.nvidia.com/cutlass/4.3.0/overview.html)
7. [https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group\_\_CUDA\_\_TENSOR\_\_MEMORY.html](https://docs.nvidia.com/cuda/archive/13.0.1/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html)
8. [https://arxiv.org/abs/2004.03082](https://arxiv.org/abs/2004.03082)
9. [https://arxiv.org/abs/2505.09363](https://arxiv.org/abs/2505.09363)
10. [https://arxiv.org/abs/2111.13040](https://arxiv.org/abs/2111.13040)
11. [https://arxiv.org/html/2505.23819v3](https://arxiv.org/html/2505.23819v3)
12. [https://prateekshukla1108.github.io/cutlass3/docs/cpp/cute/0z\_tma\_tensors.html](https://prateekshukla1108.github.io/cutlass3/docs/cpp/cute/0z_tma_tensors.html)
13. [https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/)
14. [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
15. [https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html)
16. [https://arxiv.org/pdf/2601.05972v1](https://arxiv.org/pdf/2601.05972v1)
17. [https://arxiv.org/abs/2006.06762](https://arxiv.org/abs/2006.06762)
18. [https://mlir.llvm.org/docs/Dialects/Transform/](https://mlir.llvm.org/docs/Dialects/Transform/)
19. [https://arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180)
20. [https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/stream-ordered-memory-allocation.html](https://docs.nvidia.com/cuda/cuda-programming-guide/04-special-topics/stream-ordered-memory-allocation.html)
21. [https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/archive/11.4.0/cuda-c-programming-guide/index.html)
22. [https://rocm.docs.amd.com/projects/composable\_kernel/en/latest/conceptual/ck\_tile/hardware/lds\_bank\_conflicts.html](https://rocm.docs.amd.com/projects/composable_kernel/en/latest/conceptual/ck_tile/hardware/lds_bank_conflicts.html)
